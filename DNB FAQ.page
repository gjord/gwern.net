---
title: Dual N-Back FAQ
description: A compendium of DNB, WM, IQ information
created: 25 Mar 2009
tags: DNB, psychology, experiments
status: in progress
belief: unlikely
...

N-back is a kind of [mental training](!Wikipedia "Working memory training") intended to expand your [working memory](!Wikipedia) (WM), and hopefully your intelligence (IQ^[By [IQ](!Wikipedia), I mean fluid intelligence, not crystallized intelligence, since it's unlikely that any generic training would teach you Latinate vocabulary terms or middle-school geometry. For those who object to the entire idea, please see Wikipedia or for a balanced overview what IQ can predict and the exceptions, see Sternberg et al's 2001 review, ["The Predictive Value of IQ"](/docs/dnb/2001-sternberg.pdf).]).

The theory originally went that novel^[After a large amount of training, a task may become learned and cease to stress the bottleneck: eg ["Virtually Perfect Time Sharing in Dual-task Performance: Uncorking the Central Cognitive Bottleneck"](https://web.archive.org/web/20130903074923/http://digital.sanctuary.org/~ego/papers/Schumaker01.pdf "Schumacher et al 2001").] cognitive processes tend to overlap and seem to go through [one central bottleneck](http://www.pnas.org/content/early/2011/08/03/1103583108). As it happens, WM predicts and correlates with IQ^[See for example ["Do working memory and susceptibility to interference predict individual differences in fluid intelligence?"](http://www.informaworld.com/smpp/content~db=all~content=a757621823), Borella 2006; WM predicts IQ better than strong focus/attention, with the correlation coming mostly from focus with only a small loading on executive control ([Chuderski & Necka 2012](http://ecfi-group.eu/download/papers/43.pdf "The Contribution of Working Memory to Fluid Reasoning: Capacity, Control, or Both?")).] and may use the same neural networks[^fMRI-IQ], suggesting that WM might *be* IQ^[eg. ["Reasoning=working memory≠attention"](/docs/dnb/2005-buehner.pdf), Buehner & Krummb & Pick 2005; more background is available on pg 10/92 of ["Working memory, fluid intelligence, and science learning"](http://www.stanford.edu/dept/SUSE/SEAL/Reports_Papers/YuanEtal_WorkingMemory.pdf "Yuan et al 2006"). But see the meta-analyses in [Ackerman et al 2005](/docs/dnb/2005-ackerman.pdf "Working Memory and Intelligence: The Same or Different Constructs?") which find that WM≠IQ.]. WM is known to be trainable, and so improving WM would hopefully improve IQ. And N-back is a family of tasks which stress attention and WM.

[^fMRI-IQ]: ["Brain networks for working memory and factors of intelligence assessed in males and females with fMRI and DTI"](http://jtoomim.org/brain-training/Brain%20networks%20for%20working%20memory%20and%20factors%20of%20intelligence%20assessed%20in%20males%20and%20females%20with%20fMRI%20and%20DTI.pdf), Tang 2010; it found that "individual differences in activation during the n-back task were correlated to the general intelligence factor (_g_), as well as to distilled estimates (removing _g_) of speed of reasoning, numerical ability, and spatial ability, but not to memory". PDF available in Group Files.

    A more recent result is the fMRI study [Chein 2011](http://jtoomim.org/brain-training/Domain%20general%20mechanisms%20of%20complex%20memory%20span.pdf), "Domain-general mechanisms of complex working memory span", which abstract says "For both verbal and spatial versions of the task, complex working memory span performance increased the activity in lateral prefrontal, anterior cingulate, and parietal cortices during the Encoding, Maintenance, and Coordination phase of task performance. Meanwhile, overlapping activity in anterior prefrontal and medial temporal lobe regions was associated with both verbal and spatial recall from working memory."

Later research found that performance and improvement on N-back seems to correlate better with IQ rather than classic measures of WM like reciting lists of numbers, raising the question of whether N-back works via increasing WM or by improving self-control or improving manipulation of WM contents (rather than WM's size) or somehow training IQ directly.[^jaeggi2010] Performance on DNB [has complicated correlations](http://www.psy.unibe.ch/unibe/philhuman/psy/apn/content/e5616/e5621/e7504/e7709/files7710/Jaeggietal_Memory2010_ger.pdf "'The concurrent validity of the N-back task as a working memory measure', Jaeggi et al 2010") with performance on other tests of working memory or IQ, so it's not clear what it is tapping into. (And the link between WM and performance on IQ tests has been disputed; high WM as measured by OSPAN does not correlate well with performance on hard Raven's questions[^unsworth] and the validity of single tests of WM training has been questioned[^shipstead].)

[^jaeggi2010]: from [Jaeggi et al 2010](#jaeggi-2010):

     > The findings of Study 1 confirm other findings from the literature (Jaeggi, Buschkuehl, Perrig, & Meier, 2010; Kane, Conway, Miura, & Colflesh, 2007): Consistent with our hypotheses, both n-back task variants were highly correlated, and both were best predicted by _Gf_.
     >
     > In general, matrix reasoning tasks seem to be better predictors for both the single and the dual n-back tasks than a measure of working memory capacity. As the reliability estimates were appropriate for the n-back tasks, the lack of correlation between the n-back tasks and the measure of working memory capacity cannot be attributed to insufficient reliability (Jaeggi, Buschkuehl, Perrig, & Meier, 2010). Rather, it seems that performance for the two tasks relies on different sources of variance, which might result from the different memory processes that are involved in the two tasks: whereas the n-back task relies on passive recognition processes, performance in working memory capacity tasks requires active and strategic recall processes (Kane, Conway, Miura, & Colflesh, 2007).
[^unsworth]: ["Working memory capacity and fluid abilities: Examining the correlation between Operation Span and Raven"](/docs/dnb/2005-unsworth.pdf), Unsworth, _Intelligence_ 2005:

    > However, as shown in Fig. 2, the correlations between solution accuracy for each item and Ospan, although fluctuating widely, does not appear to increase in any systematic manner as difficulty increases. Indeed, the correlation between Ospan and accuracy on the first problem was as high as with problem 24 (i.e., problem 1 r=0.26, problem 24 r=0.26). These results are strikingly similar to those of Salthouse (1993) who showed roughly the same pattern of correlations between solution accuracy and a WM composite. Both sets of results suggest that there is not a clear relationship between item variations in difficulty on Raven and measures of WM.
    >
    > ...Although there seems to be adequate variability for quartile 4, this low correlation is probably due to the fact that not as many subjects attempted these problems. Indeed, 80% of participants attempted the first 27 problems, but only 47% of participants finished the test. Thus, only quartiles 1-3 should be interpreted. With this in mind, the results demonstrate that the correlation between solution accuracy and Ospan does not increase as difficulty increases but instead remains fairly constant across increasing levels of difficulty.
    >
    > ...One reviewer was concerned that only high working memory capacity individuals would finish the test. However, of those participants classified as high working memory (one standard deviation above the mean on Ospan), only 25% of them actually finished the test, whereas 71% of those classified as low working memory (one standard deviation below the mean on Ospan) finished the test. This results in somewhat lower scores for these 76 individuals on the two measures as compared the full sample (i.e. M Ospan=11.12, S.D.=5.90; M Raven=17.50, S.D.=7.59).
[^shipstead]: ["Does working memory training generalize?"](http://web.archive.org/web/20130124210209/http://psychology.gatech.edu/renglelab/publications/2010/shipsteadredickengle.pdf), Shipstead et al 2010; abstract:

    > Recently, attempts have been made to alter the capacity of working memory (WMC) through extensive practice on adaptive working memory tasks that adjust difficulty in response to user performance. We discuss the design criteria required to claim validity as well as generalizability and how recent studies do or do not satisfy those criteria. It is concluded that, as of yet, the results are inconsistent and this is likely driven by inadequate controls and ineffective measurement of the cognitive abilities of interest.

Brain Workshop offers many modes, some far more elaborate than simple Dual N-back; no research has been done on them, so little can be said about what they are good for or what they train or what improvements they may offer; [Jaeggi 2010](#jaeggi-2010) seemed to find Single N-back better than Dual N-back. Some of the more elaborate modes seem to focus heavily on shifting the correct response among various modalities - not just sound, but left/right, eg. - and so stress [context switches](!Wikipedia "Task switching (psychology)"); there are results that task switching can be trained and that it transfers[^switching], but how useful this is and how well the BW modes train this are unknown.

[^switching]: See [Minear & Shah 2008](http://www.minearlab.com/M&CFinal1108.pdf "Training and transfer effects in task switching"):

    > Performance on task switching, a paradigm commonly used to measure executive function, has been shown to improve with practice. However, no study has tested whether these benefits are specific to the tasks learned or are transferable to new situations. We report evidence of transferable improvement in a cued, randomly switching paradigm as measured by mixing cost, but we report no consistent improvement for switch cost. Improvement in mixing costs arises from a relative reduction in time to perform both switch and nonswitch trials that immediately follow switch trials, implicating the ability to recover from unexpected switches as the source of improvement. These results add to a growing number of studies demonstrating generalizable improvement with training on executive processing.

## The Argument

[Working memory is important stuff](!Wikipedia "Working memory#Learning") for learning and also just general intelligence.[^NYT] It's not too hard to see why working memory could be so important. Working memory boils down to 'how much stuff you can think about at the same time'.

[^NYT]: ["Guest Column: Can We Increase Our Intelligence?"](http://judson.blogs.nytimes.com/2009/03/10/guest-column-can-we-increase-our-intelligence/); Sam Wang & Sandra Aamodt; _The New York Times_

    > Differences in working memory capacity account for 50-70% of individual differences in fluid intelligence (abstract reasoning ability) in various meta-analyses, suggesting that it is one of the major building blocks of I.Q. (Ackerman et al; Kane et al; [Süss et al](http://web.archive.org/web/20121128010744/http://eis.bris.ac.uk/~psxko/Suess.et-al.2002.Intelligence.pdf "'Working-memory capacity explains reasoning ability - and a little bit more', Süss et al 2002")) This idea is intriguing because working memory can be improved by training.

    See also the 2012 NYT followup, ["Can You Make Yourself Smarter?"](http://www.nytimes.com/2012/04/22/magazine/can-you-make-yourself-smarter.html?pagewanted=all)

Imagine a poor programmer who has suffered brain damage and has only enough working memory for 1 definition at a time. How could he write anything? To write a correct program, he needs to know simultaneously 2 things - what a variable, say, contains, and what is valid input for a program. But unfortunately, our programmer can know that the variable `foo` contains a string with the input, or he can know that the function `processInput` uses a string, but he can't remember these 2 things simultaneously! He will deadlock forever, unsure either what to do with this `foo`, or unsure what exactly `processInput` was supposed to work on.

More seriously, working memory can be useful since it allows one to grasp more of the structure of something at any one time. Commentators on programming [often](http://marginalrevolution.com/marginalrevolution/2011/06/does-this-reliably-increase-your-fluid-intelligence.html#comment-157452631) write that one of the great challenges of programming (besides the challenge of accepting & dealing with the reality that a computer *really is* just a mindless rule-following machine), is that programming requires one to keep in mind dozens of things and circumstances - any one of which could completely bollix things up. Focus is absolutely essential. One of the characteristics of great programmers is their apparent omniscience. Obsession grants them this ability to *know what they are actually doing*:

> "With programmers, it's especially hard. Productivity depends on being able to juggle a lot of little details in short term memory all at once. Any kind of interruption can cause these details to come crashing down. When you resume work, you can't remember any of the details (like local variable names you were using, or where you were up to in implementing that search algorithm) and you have to keep looking these things up, which slows you down a lot until you get back up to speed." --[Joel Spolsky](!Wikipedia), ["Where do These People Get Their (Unoriginal) Ideas?"](http://www.joelonsoftware.com/articles/fog0000000068.html)

"> Several friends mentioned hackers' ability to concentrate - their ability, as one put it, to 'tune out everything outside their own heads.' I've certainly noticed this. And I've heard several hackers say that after drinking even half a beer they can't program at all. So maybe hacking does require some special ability to focus. Perhaps great hackers can load a large amount of context into their head, so that when they look at a line of code, they see not just that line but the whole program around it. John McPhee wrote that Bill Bradley's success as a basketball player was due partly to his extraordinary peripheral vision. 'Perfect' eyesight means about 47 degrees of vertical peripheral vision. Bill Bradley had 70; he could see the basket when he was looking at the floor. Maybe great hackers have some similar inborn ability. (I cheat by using a very dense language, which shrinks the court.) This could explain the disconnect over cubicles. Maybe the people in charge of facilities, not having any concentration to shatter, have no idea that working in a cubicle feels to a hacker like having one's brain in a blender." --[Paul Graham](!Wikipedia "Paul Graham (computer programmer)"), ["Great Hackers"](http://www.paulgraham.com/gh.html)

It's surprising, but bugs have a close relationship to number of lines of code - no matter whether the language is as low-level as assembler or high-level as Haskell (humorously, [Norris' number](http://www.johndcook.com/blog/2011/11/22/norris-number/)); is this because each line takes up a similar amount of working and short-term memory and there's only so much memory to go around?[^WM-bugs]

[^WM-bugs]: Is this right? I have no idea. But it is a curious collection of studies and an interesting proposed model: [Hatton 1997](/docs/dnb/1997-hatton.pdf "Reexamining the Fault Density-Component Size Connection"):

    > For years I subscribed to such a principle: that modularization, or structural decomposition, is a good design concept and therefore always improves systems. This belief is so widespread as to be almost unchallengeable. It is responsible for the important programming language concept of compilation models-which are either separate, with guaranteed interface consistency (such as C++, Ada, and Modula-2), or independent, whereby a system is built in pieces and glued together later (C and Fortran, for example). It is a very attractive concept with strong roots in the "divide and conquer" principle of traditional engineering. However, this conventional wisdom may be wrong. Only those components that fit best into human short-term memory cache seem to use it effectively, thereby producing the lowest fault densities. Bigger and smaller average component sizes appear to degrade reliability.
    >
    > ...It is easy to get the impression from these case histories that developing software systems with low fault densities is exceedingly difficult. In fact, analysis of the literature reveals graphs such as that dependent faults per KLOC will approach an asymptote as time increases. In reality, only this asymptote makes sense for comparing the reliability of different systems. So, given that the asymptote can never be reached, the faults per KLOC and the rate of change of this value are required to compare such systems effectively. Of course, real systems are subject to continual noncorrective change, so things become rather more complex. No notion of rate of change of faults per KLOC was available for any of the data in this study, although both mature and immature systems were present, with the same behavior observed. This would suggest that the observed defect behavior is present through the life cycle, supporting even further the conjecture that it is a macroscopic property. If only immature systems had been present in the studies, it could have been argued that smaller components may get exercised more. This does not seem to be the case.
    >
    > A further related point, also observed in the NAG library study, is that when component fault densities are plotted as a function of size, the usage of each component must be taken into account. The models discussed in this article are essentially asymptotic, and the fault densities they predict are therefore an envelope to which component fault densities will tend only as they are used sufficiently to begin to flush out faults. An unused component has complexity but no faults, by definition. The literature reports apparently near-zero-defect systems that have turned out on closer inspection to have been unused. shown in Figure 2. This data was compiled from NASA Goddard data by the University of Maryland's Software Engineering Laboratory, as quoted in the December 1991 special edition of Business Week. First of all, in spite of NASA's enormous resources and talent pool, the average was still five to six faults per KLOC. Other studies have reported similar fault densities.4,8 More telling is the observation that in Figure 2, improvement has been achieved mostly by improving the bad processes, not the good ones. This fact suggests that consistency, a process issue, has improved much more than actual fault density, a product issue. The simple conclusion is that the average across many languages and development efforts for "good" software is around six faults per KLOC, and that with our best techniques, we can achieve 0.5-1 fault per KLOC. Perfection will always elude us, of course, but the intractability of achieving systematically better fault densities than have been achieved so far also suggests that some other limitation may be at work.
    >
    > THE PROPOSED MODEL ...Recovery code scrambling is an important factor in my proposed model. The evidence suggests that anything that fits in a short-term or cache memory is easier to understand and less fault-prone; pieces that are too large overflow, involving use of the more error-prone recovery code mechanism used for long-term storage. Thus, if a programmer is working with a component of complexity Ω, and that component fits entirely into the cache or short-term memory, which in turn can be manipulated without recourse to back-up or long-term memory, the incremental increase in bugs or disorder dE due to an incremental increase of complexity of dΩ is simply dE = (1/Ω) dΩ.
    >
    > This resembles the argument leading to Boltzmann's law relating entropy to complexity, where the analogue of equipartition of energy in a physical system is mirrored by the apparently equal distribution of rehearsal activity in the short-term memory. In other words, because no part of the cache is favored and the cache accurately manipulates symbols, the incremental increase in disorder is inversely proportional to the existing complexity, making the ideal case when pieces just fit into cache. It is assumed without loss of generality that both E and Ω are continuously valued variables. What happens when we encounter complexity greater than Ω′ (the complexity which will just fit into the cache)? The increase in disorder will correspond to the complexity in the (now-full) cache contents, plus a contribution proportional to the number of times the cache memory must be reloaded from the long-term memory. In other words, dE = (1/2*Ω)' * (1 + Ω/Ω') * dΩ
    >
    > The factor of 1/2 matches Equation 1 when Ω = Ω′, that is, when the complexity of the program is about to overflow the cache memory. The second term is directly proportional to the cache overflow effect and mimics the scrambling of the recovery codes. Integrating Equations 1 and 2 suggests that E = log Ω for Ω ≤ Ω′ and E = 1/2 * (Ω/Ω' + Ω^2/2*Ω'^2) for Ω > Ω'
    >
    > ...The Ada data and the assembly and macro-assembly data provide strong empirical support for this behavior, with about 200 to 400 lines corresponding to the complexity Ω′ at which cache memory overflows into longterm memory. That such disparate languages can produce approximately the same transition point from logarithmic to quadratic behavior supports the view that Ω is not the underlying algorithmic complexity but the symbolic complexity of the language implementation, given that a line of Ada would be expected to generate five or more lines of assembly. This is directly analogous to the observation that it is fit, rather than the actual information content of the cache that is relevant.9
    >
    > ...To summarize, if a system is decomposed into pieces much smaller than the short-term memory cache, the cache is used inefficiently because the interface of such a component with its neighbors is not "rehearsed" explicitly into the cache in the same way, and the resulting components tend to exhibit higher defect densities. If components exceed the cache size, they are less comprehensible because the recovery codes connecting comprehension with long-term memory break down. Only those components that match the cache size well use it effectively, thereby producing the lowest fault densities.
    >
    > ...Suppose that a particular functionality requires 1,000 "lines" to implement, where a "line" is some measure of complexity. The immediate implication of the earlier discussion is that, to be reliable, we should implement it as five 200-line components (each fitting in cache) rather than as 50 20-line components. The former would lead to perhaps 5 log_10(200) = 25 bugs while the latter would lead to 50 × log_10(20) = 150 bugs. This apparently inescapable but unpleasant conclusion runs completely counter to conventional wisdom. ...The additional unreliability caused by splitting up the system might be due to simple interface inconsistencies. The Basili-Perricone study considered this a possible explanation, as did Moller-Paulish. However, it was not a factor in the Hatton-Hopkins study, since the internally reusable components in the NAG library (largely externally used reusable components) had high interface consistency. Furthermore, it is unlikely to explain the Compton-Withrow data because Ada mandates interface consistency in language implementations. (This may be responsible for the difference in small components in Figure 4.)

### The Silver Bullet

It's not all that obvious, but just about every productivity innovation in computing is about either cutting down on how much a programmer needs to know (eg. [garbage collection](!Wikipedia "Garbage collection (computer science)")), or making it easier for him to shuffle things in and out of his 'short term memory'.  Why are some commentators like [Jeff Atwood](!Wikipedia) so focused[^atwood] on having multiple [monitors](!Wikipedia "Visual display unit")? For that matter, why *are* there real studies showing surprisingly large productivity boosts by simply adding a second monitor?^[See citation roundup at the [Skeptics StackExchange](http://skeptics.stackexchange.com/questions/1700/does-more-monitor-real-estate-really-increase-productivity/1701#1701).] It's not like the person is any different afterwards. And arguably multiple or larger monitors come with damaging overheads[^atwood2].

[^atwood]: Jeff Atwood; ["Multiple Monitors and Productivity"](http://www.codinghorror.com/blog/archives/000012.html), ["The Programmer's Bill of Rights"](http://www.codinghorror.com/blog/archives/000666.html), ["Joining the Prestigious Three Monitor Club"](http://www.codinghorror.com/blog/archives/000740.html), ["Does More Than One Monitor Improve Productivity?"](http://www.codinghorror.com/blog/archives/001076.html) etc.
[^atwood2]: Jeff Atwood, ["The Large Display Paradox"](http://www.codinghorror.com/blog/archives/000928.html)

Or, why does [Steve Yegge](!Wikipedia) think touch-typing is one of the few skills programmers *must* know (along with reading)?[^yegge] Why is Unix guru [Ken Thompson](!Wikipedia)'s one regret not learning typing?[^thompson] Typing hardly seems very important - it's what you say, not how you say it. The compiler doesn't care if you typed the source code in at 30WPM or 120WPM, after all.

> I love being able to type that without looking! It's *empowering*, being able to type almost as fast as you can think. Why would you want it any other way?

[^yegge]: See his blog posts, primarily ["Programming's Dirtiest Little Secret "](http://steve-yegge.blogspot.com/2008/09/programmings-dirtiest-little-secret.html). One dissenting viewpoint is John D. Cook's ["How much does typing speed matter?"](http://www.johndcook.com/blog/2010/12/09/does-typing-speed-matter/), which takes an [Amdahl's law](!Wikipedia) perspective - since typing speeds don't vary by more than an order of magnitude or two or take up much time for the most part, you can't expect the overall productivity boost of faster typing to be *too* big (though it could still be well worth your while).
[^thompson]: Page 457, [_Coders at Work_](http://www.amazon.com/Coders-Work-Reflections-Craft-Programming/dp/1430219483/):

    > `Seibel`: "Is there anything you would have done differently about learning to program? Do you have any regrets about the sort of path you took or do you wish you had done anything earlier?"
    >
    > [`Ken Thompson`](!Wikipedia "Ken Thompson"): "Oh, sure, sure. In high school I wish I'd taken typing. I suffer from poor typing yet today, but who knew. I didn't plan anything or do anything. I have no discipline. I did what I wanted to do next, period, all the time. If I had some foresight or planning or something, there are things, like typing, I would have done when I had the chance."

The thing is, multiple monitors, touch-typing, speed-reading[^speed-reading] - they're all about making the external world part of your mind. What's the real difference between having a [type signature](!Wikipedia) in your short-term memory or prominently displayed in your second monitor? What's the *real* difference between writing a [comment](!Wikipedia "Comment (computer programming)") in your mind or touch-typing it as fast as you create it?

[^speed-reading]: When I was younger, I reasoned that early in life is the best time to learn to read fast since one reaps the greatest gains over the longest possible period (I still agree with my former reasoning) and so did a great deal of reading on [speed-reading](!Wikipedia) and the related academic literature, and spent more than a few hours working with tachistocopic-style software. My ultimate conclusion was that it was a good use of my time as it bumped my WPM up to ~400-500 WPM from the ordinary 300 WPM, but the techniques were not going to give any useful ability beyond that as greater speed becomes an indication one is reading too easy material or one should be using more sophisticated search capabilities. In particular, [tachistoscopes](!Wikipedia) weren't very useful for non-practice reading and were least useful on deep or heavily-hyperlinked content. "Photoreading", however, is simply a scam or very shallow skimming. Unfortunately, I omitted to take notes on specific studies or programs, though, being too young to care about being able to explain & defend my beliefs later - but that is just as well since by now, all the websites would be gone, programs bitrotten, and links broken. Readers will just have to do their own research on the topic if they care (much easier in this age of Wikipedia).

<div style="float:right;margin:0 10px 10px 0">
<img alt="Scumbag brain meme: needs to write 100kloc system / can't keep &gt;7 items in memory" height="210" src="/images/dnb/scumbagbrain.jpg" title="Humorous description of the problem of WM shortage" width="300">
</div>

Just some speed. Just some time. And the more visible that type signature is, the faster you can type out that comment, the larger your 'memory' gets. And the larger your memory is, the more intelligent/productive you can be. (Think of this as the [Extended Mind](!Wikipedia "Extended Mind") thesis as applied to programming!) Great programmers often[^codersatwork][^knuth] talk vaguely about 'keeping a system in your head' or 'having a model', and hate distractions[^programming-distractions], saying they destroy one's [carefully developed thoughts](http://www.paulgraham.com/head.html); I think what they are talking about is trying to store all the relevant details inside their short-term or working memory. Learning programming has a correlation with WM.[^shute] (Once you start looking, you see this everywhere. Games, for example.[^angrybirds]) Or in bug rates - WM has been proposed as the reason why small or large chunks of programs have more proportional errors than medium sized chunks[^Hatton]. It remains to be seen whether [programming tools designed](http://www.cc.gatech.edu/~vector/papers/memoryfailures.pdf "'Programmer Information Needs After Memory Failure', Parnin & Rugaber 2012") with an eye to memory will be helpful, though.

[^knuth]: From [an interview](https://github.com/kragen/knuth-interview-2006) given by [Donald Knuth](!Wikipedia) to Dikran Karagueuzian, the director of CSLI Publications:

    > I couldn't keep up with all my teaching at Stanford though, I'm not on sabbatical but I found that doing software was much, was much harder than writing books and doing research papers. It takes another level of commitment that you have to have so much in your head at the time when you're doing software, that, that I had to take leave of absence from Stanford from my, from my ordinary teaching for several quarters during this period.
[^codersatwork]: From the interview anthology _[Coders at Work](!Wikipedia)_ (2009), pg 114:

    > [`Peter Seibel`](http://www.gigamonkeys.com/): "Do you think that programming is at all biased toward being young?"
    >
    > [`Douglas Crockford`](!Wikipedia): "I used to think so. A few years ago I had [sleep apnea](!Wikipedia), but I didn't know it. I thought I was just getting tired and old, and I got to the point where it was so difficult to concentrate that I couldn't program anymore because I just couldn't keep enough stuff in my head. A lot of programming is you keep stuff in your head until you can get it written down and structured properly. And I just couldn't do it.
    >
    > I had lost that ability and I thought it was just because I was getting older. Fortunately, I got better and it came back and so I'm programming again. I'm doing it well and maybe a little bit better now because I've learned how not to depend so much on my memory. I'm better at documenting my code now than I used to be because I'm less confident that I'll remember next week why I did this. In fact, sometimes I'll be going through my stuff and I'm amazed at stuff that I had written: I don't remember having done it and it's either really either awful or brilliant. I had no idea I was capable of that."

    From pg 154:

    > `Seibel`: "How do you design code?"
    >
    > [`Brendan Eich`](!Wikipedia): "A lot of prototyping. I used to do sort of high-level pseudocode, and then I'd start filling in bottom up. I do less of the high-level pseudocode because I can usually hold it in my head and just do bottom-up until it joins.
    >
    > Often I'm working with existing pieces of code adding some new subsystem or something on the side and I can almost do it bottom-up. When I get in trouble in the middle I do still write pseudo-code and just start working bottom up until I can complete it. I try not to let that take too long because you've got to be able to test it; you've got to be able to see it run and step through it and make sure it's doing what it's supposed to be doing."

    From pg 202, a cogent reminder that 'tis a good wind that blows no ill (and that as William T. Powers wrote somewhere on the [CSGNet ML](http://web.archive.org/web/20120721214622/http://www.perceptualcontroltheory.org/email_list.html), "Some people revel in complexity, and what's worse, they have the brain power to deal with vast systems of arcane equations. This ability can be a handicap because it leads to overlooking simple solutions."):

    > `Seibel`: "Speaking of writing intricate code, I've noticed that people who are too smart, in a certain dimension anyway, make the worst code. Because they can actually fit the whole thing in their head they can write these great reams of spaghetti code."
    >
    > [`Joshua Bloch`](!Wikipedia): "I agree with you that people who are both smart enough to cope with enormous complexity and lack empathy with the rest of us may fall prey to that. They think, 'I can understand this and I can use it, so it has to be good.'"

    From pg 236:

    > [`Joe Armstrong`](!Wikipedia "Joe Armstrong (programming)"): "I read somewhere, that you have to have a good memory to be a reasonable programmer. I believe that to be true."
    >
    > `Seibel`: "Bill Gates once claimed that he could still go to a blackboard and write out big chunks of the code to the BASIC that he written for the Altair, a decade or so after he had originally written it. Do you think you can remember your old code that way?"
    >
    > `Armstrong`: "Yeah. Well, I could reconstruct something. Sometimes I've just completely lost some old code and it doesn't worry me in the slightest."

    From page 246:

    > [`Simon Peyton Jones`](!Wikipedia): "Yeah, that's right. So essentially we wrote out our types by drawing them on large sheets of papers with arrows. That was our type system. That was a pretty large program-in fact it was over ambitious; we never completed it."
    >
    > `Seibel`: "Do you think you learned any lessons from that failure?"
    >
    > `Peyton Jones`: "That was probably when I first became aware that writing a really big program you could end up with problems of scale-you couldn't keep enough of it in your head at the same time. Previously all the things I had written, you could keep the whole thing in your head without any trouble. So it was probably the first time I'd done any serious attempt at long-standing documentation."
    >
    > `Seibel`: "But even that wasn't enough, in this case..."

    From page 440:

    > [[David Deutsch](!Wikipedia):] "The second reason I like Python is that-and maybe this is just the way my brain has changed over the years-I can't keep as much stuff in my head as I used to. It's more important for me to have stuff in front of my face. So the fact that in Smalltalk you effectively cannot put more than one method on the screen at a time drives me nuts. As far as I'm concerned the fact that I edit Python programs with Emacs is an advantage because I can see more than ten lines' worth at a time."
[^programming-distractions]: The best programmers seem to suffer few distractions and the worst had many, although it is hard to infer causality from this striking correlation. From ["The Rise of the New Groupthink"](http://www.nytimes.com/2012/01/15/opinion/sunday/the-rise-of-the-new-groupthink.html?pagewanted=all), Susan Cain, _The New York Times_, drawing on the 1987 book _[Peopleware: Productive Projects and Teams](!Wikipedia)_ or perhaps the related paper ["Why Measure Performance"](http://dwp.bigplanet.com/pdkconsulting/nss-folder/pdfdownloads1/Why_Measure%20_DeMarco3.20.01.pdf):

    > Privacy also makes us productive. In a fascinating study known as the Coding War Games, consultants Tom DeMarco and Timothy Lister compared the work of more than 600 computer programmers at 92 companies. They found that people from the same companies performed at roughly the same level - but that there was an enormous performance gap between organizations. What distinguished programmers at the top-performing companies wasn't greater experience or better pay. It was how much privacy, personal workspace and freedom from interruption they enjoyed. 62% of the best performers said their workspace was sufficiently private compared with only 19% of the worst performers. 76% of the worst programmers but only 38% of the best said that they were often interrupted needlessly.

[^shute]: ["Who is Likely to Acquire Programming Skills?"](/docs/dnb/1991-shute.pdf), Shute 1991; Shute measured WM for students learning [Pascal](!Wikipedia "Pascal (programming language)") and of course found that higher WM correlated with faster learning, but despite using the _g_-loaded [ASVAB](!Wikipedia), unfortunately she apparently did not measure against IQ directly, so possibly it's just IQ correlating with the programming skill:

    > Following instruction, an online battery of criterion tests was administered measuring programming knowledge and skills acquired from the tutor. Results showed that a large amount (68%) of the outcome variance could be predicted by a working-memory factor, specific word problem solving abilities (i.e., problem identification and sequencing of elements) and some learning style measures (i.e., asking for hints and running programs).
[^angrybirds]: In ["Why Angry Birds is so successful and popular: a cognitive teardown of the user experience"](http://www.mauronewmedia.com/blog/why-angry-birds-is-so-successful-a-cognitive-teardown-of-the-user-experience/), [ergonomics](!Wikipedia) writer Charles L. Mauro singles out selective stressing of working memory as key to _[Angry Birds](!Wikipedia)_'s management of the difficulty of its puzzles:

    > It is a well-known fact of cognitive science that human short-term memory (SM), when compared to other attributes of our memory systems, is exceedingly limited....Where things get interesting is the point where poor user interface design impacts the demand placed on SM. For example, a user interface design solution that requires the user to view information on one screen, store it in short-term memory, and then reenter that same information in a data field on another screen seems like a trivial task. Research shows that it is difficult to do accurately, especially if some other form of stimulus flows between the memorization of the data from the first screen and before the user enters the data in the second. This disruptive data flow can be in almost any form, but as a general rule, anything that is engaging, such as conversation, noise, motion, or worst of all, a combination of all three, is likely to totally erase SM. When you encounter this type of data flow before you complete transfer of data using short-term memory, chances are very good that when you go back to retrieve important information from short-term memory, it is gone!
    >
    > ..._Angry Birds_ is a surprisingly smart manager of the player's short-term memory.
    >
    > By simple manipulation of the user interface, _Angry Birds_ designers created significant short-term memory loss, which in turn increases game play complexity but in a way that is not perceived by the player as negative and adds to the addictive nature of the game itself. The subtle, yet powerful concept employed in _Angry Birds_ is to bend short-term memory but not to actually break it. If you do break SM, make sure you give the user a very simple, fast way to accurately reload. There are many examples in the _Angry Birds_ game model of this principle in action....
    >
    > One of the main benefits of playing _Angry Birds_ on the iPad [rather than the smaller iPhone] is the ability to pinch down the window size so you can keep the entire game space (birds & pigs in houses) in full view all the time. Keeping all aspects of the game's interface in full view prevents short-term memory loss and improves the rate at which you acquire skills necessary to move up to a higher game level.
    > Side note: If you want the ultimate _Angry Birds_ experience use a POGO pen on the iPad with the display pinched down to view the entire game space. This gives you finer control, better targeting and rapidly changing game play. The net impact in cognitive terms is a vastly superior skill acquisition profile. However, you will also find that the game is less interesting to play over extended periods. Why does this happen?
[^Hatton]: ["Reexamining the Fault Density-Component Size Connection"](/docs/dnb/1997-hatton.pdf), Les Hatton ([extended excerpts](http://groups.google.com/group/brain-training/browse_thread/thread/8053035600aa5d5d)):

    > For years I subscribed to such a principle: that modularization, or structural decomposition, is a good design concept and therefore always improves systems. This belief is so widespread as to be almost unchallengeable. It is responsible for the important programming language concept of compilation models-which are either separate, with guaranteed interface consistency (such as C++, Ada, and [Modula-2](!Wikipedia)), or independent, whereby a system is built in pieces and glued together later (C and Fortran, for example). It is a very attractive concept with strong roots in the "divide and conquer" principle of traditional engineering. However, this conventional wisdom may be wrong. Only those components that fit best into human short-term memory cache seem to use it effectively, thereby producing the lowest fault densities. Bigger and smaller average component sizes appear to degrade reliability.
    >
    > ...The Ada data and the assembly and macro-assembly data provide strong empirical support for this behavior, with about 200 to 400 lines corresponding to the complexity Ω′ at which cache memory overflows into long-term memory. That such disparate languages can produce approximately the same transition point from logarithmic to quadratic behavior supports the view that Ω is not the underlying algorithmic complexity but the symbolic complexity of the language implementation, given that a line of Ada would be expected to generate five or more lines of assembly. This is directly analogous to the observation that it is fit, rather than the actual information content of the cache that is relevant.^9^
    >
    > ...To summarize, if a system is decomposed into pieces much smaller than the short-term memory cache, the cache is used inefficiently because the interface of such a component with its neighbors is not "rehearsed" explicitly into the cache in the same way, and the resulting components tend to exhibit higher defect densities. If components exceed the cache size, they are less comprehensible because the recovery codes connecting comprehension with long-term memory break down. Only those components that match the cache size well use it effectively, thereby producing the lowest fault densities.
    >
    > ...Suppose that a particular functionality requires 1,000 "lines" to implement, where a "line" is some measure of complexity. The immediate implication of the earlier discussion is that, to be reliable, we should implement it as five 200-line components (each fitting in cache) rather than as 50 20-line components. The former would lead to perhaps $5 \times log_10(200) = 25$ bugs while the latter would lead to $50 \times log_10(20) = 150$ bugs. This apparently inescapable but unpleasant conclusion runs completely counter to conventional wisdom. ...The additional unreliability caused by splitting up the system might be due to simple interface inconsistencies. The Basili-Perricone study considered this a possible explanation, as did Moller-Paulish. However, it was not a factor in the Hatton-Hopkins study, since the internally reusable components in the NAG library (largely externally used reusable components) had high interface consistency. Furthermore, it is unlikely to explain the Compton-Withrow data because Ada mandates interface consistency in language implementations. (This may be responsible for the difference in small components in Figure 4.)

But as great as things like garbage collection & touch-typing & multiple monitors are (I am a fan & user of the foregoing), they are still imperfect substitutes. Wouldn't it be better if one could just improve one's short-term/working memory directly? It might be more effective, and certainly would be more portable!

## Training

Unfortunately, in general, IQ/[_g_](!Wikipedia "General intelligence factor") and memory don't seem to be trainable. Many apparent effects are swamped by exercise or nutrition or by simple practice. And when practice does result in gains on tasks or expensive games^["Walking is free, but Americans spent $13 million on brain-fitness software and games last year [2009]..."; from _[Newsweek](http://www.newsweek.com/2010/06/18/this-is-your-brain-aging.print.html)_], said benefits often do not [_transfer_](!Wikipedia "Transfer of learning"); many popular 'brain games' & exercises fail this criterion or at least have not been shown to transfer^[See for example _Nature_'s coverage of the Cambridge study, ["No gain from brain training: Computerized mental workouts don't boost mental skills, study claims"](http://www.nature.com/news/2010/100420/full/4641111a.html); or _Discover_'s [blog discussion](http://blogs.discovermagazine.com/notrocketscience/2010/04/20/brain-training-games-get-a-d-at-brain-training-tests/).][^newsweekactive][^newsweeksalthouse], even brainy skilled exercises like [music](/docs/dnb/2013-elpus.pdf "'Is It the Music or Is It Selection Bias? A Nationwide Analysis of Music and Non-Music Students' SAT Scores', Elpus 2013")[^Elpus-music] or [chess](http://www.brunel.ac.uk/~hsstffg/preprints/chess_and_education.PDF "'Educational benefits of chess instruction: A critical review', Gobet & Campitelli 2005")^[A specific example: [Schneider et al 1993](http://opus.bibliothek.uni-wuerzburg.de/volltexte/2012/6221/pdf/Schneider_W107.pdf "Chess expertise and memory for chess positions in children and adults") - chess-playing had superior chessboard recall than adults, but adults still had better recall of numbers. Exactly as expected from training with no transfer.] or memory competitions[^Foer]. _[Catch-22](!Wikipedia)_ summed it up:

> ...General Dreedle wants his [pilots] to spend as much time on the skeet-shooting range as the facilities and their flight schedule would allow. [Shooting skeet](!Wikipedia) eight hours a month was excellent training for them. It trained them to shoot skeet.

[^Elpus-music]: Music correlates with increased SAT scores, which has been cited as a justification for teaching students music, but it exhibit a common pattern for claims of far transfer: it appears in simple analyses, disappears in randomized experiments (eg [Mehr et al 2013](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0082007 "Two Randomized Trials Provide No Consistent Evidence for Nonmusical Cognitive Benefits of Brief Preschool Music Enrichment")), and finally a thorough analysis including a wide range of covariates like Elpus 2013 finds the correlation disappears because it was due to some confound like the higher-performing students also being wealthier. The background for music:

    > An entire special issue of the _Journal of Aesthetic Education_ (JAE) in 2000, titled "The Arts and Academic Achievement: What the Evidence Shows", was dedicated to examining the academic performance of arts and non-arts students. In that volume, [Winner and Cooper (2000)](/docs/dnb/2000-winner.pdf "Mute those claims: No evidence (yet) for a causal link between arts study and academic achievement") meta-analyzed some 31 published and unpublished studies, yielding 66 separate effect sizes examining the general research question of whether arts education, broadly defined, positively influenced academic achievement. Results of the meta-analysis showed that arts education was moderately positively associated with higher achievement in math, verbal, and composite math-verbal outcomes. In the same journal issue, [Vaughan and Winner (2000)](/docs/dnb/2000-vaughan.pdf "SAT scores of students who study the arts: What we can and cannot conclude about the association") sought to analyze the link between arts course work and SAT scores specifically. Using data from 12 years of national SAT means reported by the College Board in the annual Profiles of College Bound Seniors report, Vaughan and Winner found that students who self-reported on the SAT's Student Descriptive Questionnaire that they had pursued arts course work outscored students who reported they had not taken any arts course work. Meta-analyses of music students' performance on verbal ([Butzlaff, 2000](/docs/dnb/2000-butzlaff.pdf "Can music be used to teach reading?")) and mathematical (Vaughan, 2000) standardized tests were somewhat inconclusive: Although positive associations were found in the correlational research literature, meta-analyses of results from the few experimental studies located in the literature showed little to no influence of music on verbal or math test scores...In British Columbia, Canada ([Gouzouasis, Guhn, & Kishor, 2007](/docs/dnb/2007-gouzouasis.pdf "The predictive relationship between achievement and participation in music and achievement in core grade 12 academic subjects")), results of an observational study indicated an association between music enrollment and higher subject-area standardized test scores among high school students. The results of a randomized experiment in Montreal, Canada, showed no effects of piano instruction on subject-area standardized tests among elementary school children from low socioeconomic backgrounds ([Costa-Giomi, 2004](/docs/dnb/2004-costa-giomi.pdf "Effects of three years of piano instruction on children's academic achievement, school performance and self-esteem")).

Indeed, the general history of attempts to increase IQ in any children or adults remains essentially what it was when [Arthur Jensen](!Wikipedia) wrote his 1969 paper ["How Much Can We Boost IQ and Scholastic Achievement?"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.138.980&rep=rep1&type=pdf) - a history of failure. The exceptions prove the rule by either applying to narrow groups with specific deficits or work only before birth, like [iodization](Iodine). (See also [Algernon's Law](Drug heuristics): if there were an easy fitness-increasing way to make us smarter, evolution would have already used it.)

[^Foer]: National memory champion [Tatiana Cooley](http://www.nytimes.com/2007/06/28/garden/28post-it.html?pagewanted=2 "While You Were Out, the Post-it Went Home"): "I'm incredibly absent-minded. I live by Post-its." Or the [_Washington Post_](http://www.washingtonpost.com/wp-dyn/content/article/2011/03/04/AR2011030402772.html "Joshua Foer's 'Moonwalking With Einstein', on the nature of memory"), reviewing Joshua Foer's 2011 _[Moonwalking With Einstein](!Wikipedia)_:

    > Foer sets out to meet the legendary "Brainman," who learned Spanish in a single weekend, could instantly tell if any number up to 10,000 was prime, and saw digits in colors and shapes, enabling him to hold long lists of them in memory. The author also tracks down "Rain Man" [Kim Peek](!Wikipedia), the famous savant whose astonishing ability to recite all of Shakespeare's works, reproduce scores from a vast canon of classical music and retain the contents of 9,000 books was immortalized in the [Hollywood movie](!Wikipedia "Rain Man") starring Dustin Hoffman. When Foer is told that the Rain Man had an IQ of merely 87 - that he was actually missing a part of his brain; that memory champions have no more intelligence than you or I; that building a memory is a matter of dedication and training - he decides to try for the U.S. [memory championship](!Wikipedia "Memory sport") himself. Here is where the book veers sharply from science journalism to a memoir of a singular adventure.
[^newsweekactive]: [_Newsweek_](http://www.newsweek.com/2011/01/03/can-you-build-a-better-brain.html):

    > Training your memory, reasoning, or speed of processing improves that skill, found a large government-sponsored study called Active. Unfortunately, there is no transfer: improving processing speed does not improve memory, and improving memory does not improve reasoning. Similarly, doing crossword puzzles will improve your ability to...do crosswords. "The research so far suggests that cognitive training benefits only the task used in training and does not generalize to other tasks," says Columbia's Stern.
[^newsweeksalthouse]:  ["This Is Your Brain. Aging. Science is reshaping what we know about getting older. (The news is better than you think.)"](http://www.newsweek.com/2010/06/18/this-is-your-brain-aging.print.html), _Newsweek_:

    > Doing crossword puzzles would seem to be ideal brain exercise since avid puzzlers do them daily and say it keeps them mentally sharp, especially with vocabulary and memory. But this may be confusing cause and effect. It is mostly people who are good at figuring out "Dole's running mate" who do crosswords regularly; those who aren't, don't. In a recent study, Salt-house and colleagues found "no evidence" that people who do crosswords have "a slower rate of age-related decline in reasoning." As he put it in a 2006 analysis, there is "little scientific evidence that engagement in mentally stimulating activities alters the rate of mental aging," an idea that is "more of an optimistic hope than an empirical reality." (P.S.: Bob Dole's 1996 VP choice was Jack Kemp.)

But hope springs eternal, and there are *possible* exceptions. The one this FAQ focuses on is Dual [N-back](!Wikipedia), and it's a variant on an old working-memory test.

One of the nice things about N-back is that while it may or [may not](#criticism) improve your IQ, it may help you in other ways. WM training helps alcoholics reduce their consumption[^jansen] and increases patience in recovering stimulant addicts (cocaine & methamphetamine)[^bickel]. The self-discipline or willpower of students correlates better with grades than even IQ[^discipline], WM [correlates with grades](/docs/dnb/2005-aronen.pdf "'Working memory, psychiatric symptoms, and academic performance at school', Aronen et al 2005") and lower behavioral problems[^emotions] & WM out-predicts grades 6 years later in 5-year olds & 2 years later in older children[^alloway]. WM training has been shown to help children with ADHD[^klingberg] and also preschoolers without ADHD^[["Training and transfer effects of executive functions in preschool children"](http://www.researchgate.net/publication/23718164_Training_and_transfer_effects_of_executive_functions_in_preschool_children/file/9fcfd50b7c49977d9a.pdf), Thorell et al 2009]; [Lucas 2008](http://www.cdzjesenik.cz/APA_Poster_Lucas_May_2008.pdf) found behavior improvements at a summer camp. Another intervention using a miscellany of 'reasoning' games with young (7-9 years old) poor children found a Forwards Digit Span (but not Backwards) and IQ gains, with no gain to the subjects playing games requiring "rapid visual detection and rapid motor responses"[^mackey2011], but it's worth remembering that IQ scores are unreliable in childhood[^sternbergChildhood] or perhaps, as an adolescent brain imaging study indicates[^adolescentImaging], they simply are much more malleable at that point. (WM training in teenagers doesn't seem much studied but given their issues, may help; see ["Beautiful Brains"](http://ngm.nationalgeographic.com/print/2011/10/teenage-brains/dobbs-text) or ["The Trouble With Teens"](http://discovermagazine.com/2011/mar/24-the-brain-the-trouble-with-teens/article_view?b_start:int=1&-C=).)

[^jansen]: ["Getting a Grip on Drinking Behavior: Training Working Memory to Reduce Alcohol Abuse"](http://www.eetonderzoek.nl/publikaties/Houben%20et%20al%20Psychological%20Science.11.pdf), Houben et al 2011:

    > Alcohol abuse disrupts core executive functions, including working memory (WM)-the ability to maintain and manipulate goal-relevant information. When executive functions like WM are weakened, drinking behavior gets out of control and is guided more strongly by automatic impulses. This study investigated whether training WM restores control over drinking behavior. Forty-eight problem drinkers performed WM training tasks or control tasks during 25 sessions over at least 25 days. Before and after training, we measured WM and drinking behavior. Training WM improved WM and reduced alcohol intake for more than 1 month after the training. Further, the indirect effect of training on alcohol use through improved WM was moderated by participants' levels of automatic impulses: Increased WM reduced alcohol consumption in participants with relatively strong automatic preferences for alcohol. These findings are consistent with the theoretical framework and demonstrate that training WM may be an effective strategy to reduce alcohol use by increasing control over automatic impulses to drink alcohol.
[^bickel]: ["Remember the Future: Working Memory Training Decreases Delay Discounting Among Stimulant Addicts"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3015021/), Bickel et al 2011; WM tasks were digit span, reverse digit span, and a list-of-words-matching task. Decreasing their [discount rate](!Wikipedia) does not actually show any reduced drug abuse or better odds of rehabilitation, but it is hopeful.
[^discipline]: ["Self-Discipline Outdoes IQ in Predicting Academic Performance of Adolescents"](/docs/dnb/2005-duckworth.pdf), Duckworth 2006; abstract:

     > In a longitudinal study of 140 eighth-grade students, self-discipline measured by self-report, parent report, teacher report, and monetary choice questionnaires in the fall predicted final grades, school attendance, standardized achievement-test scores, and selection into a competitive high school program the following spring. In a replication with 164 eighth graders, a behavioral delay-of-gratification task, a questionnaire on study habits, and a group-administered IQ test were added. Self-discipline measured in the fall accounted for more than twice as much variance as IQ in final grades, high school selection, school attendance, hours spent doing homework, hours spent watching television (inversely), and the time of day students began their homework. The effect of self-discipline on final grades held even when controlling for first-marking-period grades, achievement-test scores, and measured IQ. These findings suggest a major reason for students falling short of their intellectual potential: their failure to exercise self-discipline.
[^alloway]: ["Investigating the predictive roles of working memory and IQ in academic attainment"](/docs/dnb/2010-alloway.pdf), Alloway 2010:

    > ...The findings indicate that children's working memory skills at 5 years of age were the best predictor of literacy and numeracy 6 years later. IQ, in contrast, accounted for a smaller portion of unique variance to these learning outcomes. The results demonstrate that working memory is not a proxy for IQ but rather represents a dissociable cognitive skill with unique links to academic attainment. Critically, we find that working memory at the start of formal education is a more powerful predictor of subsequent academic success than IQ....

    Less striking but still relevant is ["Working Memory, but Not IQ, Predicts Subsequent Learning in Children with Learning Difficulties"](/docs/dnb/2009-alloway.pdf), Alloway 2009:

    > The purpose of the present study was to compare the predictive power of working memory and IQ in children identified as having learning difficulties...Children aged between 7 and 11 years were tested at Time 1 on measures of working memory, IQ, and learning. They were then retested 2 years later on the learning measures. The findings indicated that working-memory capacity and domain-specific knowledge at Time 1, but not IQ, were significant predictors of learning at Time 2.
[^emotions]: This is probably not surprising, since even in adults, those with higher WMs are better at controlling their emotions when asked to do so; abstract of ["Working memory capacity and spontaneous emotion regulation: High capacity predicts self-enhancement in response to negative feedback"](/docs/dnb/2010-schmeichel.pdf "Schmeichel & Demaree 2010"):

    > Although previous evidence suggests that working memory capacity (WMC) is important for success at emotion regulation, that evidence may reveal simply that people with higher WMC follow instructions better than those with lower WMC. The present study tested the hypothesis that people with higher WMC more effectively engage in spontaneous emotion regulation following negative feedback, relative to those with lower WMC. Participants were randomly assigned to receive either no feedback or negative feedback about their emotional intelligence. They then completed a disguised measure of self-enhancement and a self-report measure of affect. Experimental condition and WMC interacted such that higher WMC predicted more self-enhancement and less negative affect following negative feedback. This research provides novel insight into the consequences of individual differences in WMC and illustrates that cognitive capacity may facilitate the spontaneous self-regulation of emotion.
[^klingberg]: ["Computerized Training of Working Memory in Children With ADHD - A Randomized, Controlled Trial"](http://www.klingberglab.se/pub/CompTrainWM.pdf), Klingberg et al 2005; abstract:

     > ...For the span-board task, there was a significant treatment effect both post-intervention and at follow-up. In addition, there were significant effects for secondary outcome tasks measuring verbal WM, response inhibition, and complex reasoning. Parent ratings showed significant reduction in symptoms of  inattention and hyperactivity/impulsivity, both post-intervention and at follow-up. Conclusions:This study shows that WM can be improved by training in children with ADHD. This training also improved response inhibition and reasoning and resulted in a reduction of the parent-rated inattentive symptoms of ADHD.

     See also [Green et al 2012](/docs/dnb/2012-green.pdf "Will Working Memory Training Generalize to Improve Off-Task Behavior in Children with Attention-Deficit/Hyperactivity Disorder?").
[^mackey2011]: ["Differential effects of reasoning and speed training in children"](http://ihd.berkeley.edu/Mackey_DevSci_withSuppl_2011-1.pdf) (the list of reasoning games, page 5, does not seem to include any direct analogues to n-back):

    > The goal of this study was to determine whether intensive training can ameliorate cognitive skills in children. Children aged 7 to 9 from low socioeconomic backgrounds participated in one of two cognitive training programs for 60 minutes ⁄ day and 2 days ⁄ week, for a total of 8 weeks. Both training programs consisted of commercially available computerized and non- computerized games. Reasoning training emphasized planning and relational integration; speed training emphasized rapid visual detection and rapid motor responses. Standard assessments of reasoning ability - the Test of Non-Verbal Intelligence (TONI-3) and cognitive speed (Coding B from WISC IV) - were administered to all children before and after training. Neither group was exposed to these standardized tests during training. Children in the reasoning group improved substantially on TONI (Cohen's d = 1.51), exhibiting an average increase of 10 points in Performance IQ, but did not improve on Coding. By contrast, children in the speed group improved substantially on Coding (d = 1.15), but did not improve on TONI. Counter to widespread belief, these results indicate that both fluid reasoning and processing speed are modifiable by training.
[^sternbergChildhood]: See again Sternberg et al's 2001 review, "The Predictive Value of IQ":

    > *Evidence from studies of the natural course of development*: Some get more intelligent, others get less intelligent. The Berkeley Guidance Study (Honzik, Macfarlane, & Allen, 1948) investigated the stability of IQ test performance over 12 years. The authors reported that nearly 60% of the sample changed by 15 IQ points or more from 6 to 18 years of age. A similar result was found in the Fels study (Sontag, Baker, & Nelson, 1958): Nearly two thirds of the children changed more than 15 IQ points from age 3 to age 10. Researchers also investigated the so-called intelligence lability score, which is a child's standard deviation from his or her own grand mean IQ. Bayley (1949), in the Berkeley Growth study, detected very large individual differences in lability across the span of 18 years. Rees and Palmer (1970) combined the data from five large-scale longitudinal studies, selecting those participants who had scores at both age 6 and age 12 or at both age 12 and age 17. They found that about 30% of the selected participants changed by 10 or more IQ points.

    Sternberg et al also discusses the dramatic IQ gains possible during infancy when adoptees moving from a bad environment (Third or Second World orphanages) to good ones (First World homes), but also the discouraging examples of early intervention programs in the USA where initial IQ gains often fade away over the years.
[^adolescentImaging]: See ["Verbal and non-verbal intelligence changes in the teenage brain"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3672949/ "Ramsden et al 2011") (media coverage: ["IQ Isn't Set In Stone, Suggests Study That Finds Big Jumps, Dips In Teens"](http://www.npr.org/blogs/health/2011/10/20/141511314/iq-isnt-set-in-stone-suggests-study-that-finds-big-jumps-dips-in-teens));

    > Neuroimaging allows us to test whether unexpected longitudinal fluctuations in measured IQ are related to brain development. Here we show that verbal and non-verbal IQ can rise or fall in the teenage years, with these changes in performance validated by their close correlation with changes in local brain structure. A combination of structural and functional imaging showed that verbal IQ changed with grey matter in a region that was activated by speech, whereas non-verbal IQ changed with grey matter in a region that was activated by finger movements. By using longitudinal assessments of the same individuals, we obviated the many sources of variation in brain structure that confound cross-sectional studies. This allowed us to dissociate neural markers for the two types of IQ and to show that general verbal and non-verbal abilities are closely linked to the sensorimotor skills involved in learning.

    It's worth noting that substantial changes in the brain continue to take place towards the end of adolescence and early adulthood, and at least some are about reducing one's mental flexibility; from _National Geographic_, ["Beautiful Brains: Moody. Impulsive. Maddening. Why do teenagers act the way they do? Viewed through the eyes of evolution, their most exasperating traits may be the key to success as adults"](http://ngm.nationalgeographic.com/print/2011/10/teenage-brains/dobbs-text):

    > Meanwhile, in times of doubt, take inspiration in one last distinction of the teen brain-a final key to both its clumsiness and its remarkable adaptability. This is the prolonged plasticity of those late-developing frontal areas as they slowly mature. As noted earlier, these areas are the last to lay down the fatty [myelin](!Wikipedia) insulation-the brain's white matter-that speeds transmission. And at first glance this seems like bad news: If we need these areas for the complex task of entering the world, why aren't they running at full speed when the challenges are most daunting?
    >
    > The answer is that speed comes at the price of flexibility. While a myelin coating greatly accelerates an axon's bandwidth, it also inhibits the growth of new branches from the axon. According to Douglas Fields, an NIH neuroscientist who has spent years studying myelin, "This makes the period when a brain area lays down myelin a sort of crucial period of learning-the wiring is getting upgraded, but once that's done, it's harder to change."
    >
    > The window in which experience can best rewire those connections is highly specific to each brain area. Thus the brain's language centers acquire their insulation most heavily in the first 13 years, when a child is learning language. The completed insulation consolidates those gains-but makes further gains, such as second languages, far harder to come by. So it is with the forebrain's myelination during the late teens and early 20s. This delayed completion-a withholding of readiness-heightens flexibility just as we confront and enter the world that we will face as adults.

There are many kinds of WM training. One review worth reading is ["Does working memory training work? The promise and challenges of enhancing cognition by training working memory"](http://www.temple.edu/tunl/publications/documents/Morrison_Chein2011.pdf) (Morrison & Chein 2011); ["Is Working Memory Training Effective?"](/docs/dnb/2012-shipstead.pdf) (Shipstead, Redick, & Engle 2012) discusses the multiple methodological difficulties of designing WM training experiments (at least, they are difficult if you want to show *genuine* improvements which transfer to non-WM skills).

<!-- Chooi 2011

> There are many studies in the literature that suggest positive outcome from working memory training, such as reduced inattentive symptoms in ADHD children (Klingberg et al., 2002; Klingberg et al., 2005), increased memory performance in older adults (Buschkuehl et al., 2008), increased math performance in children with working memory deficits (Holmes et al., 2009), improved short term memory in adolescents with borderline intellectual disability (Van der Molen et al., 2010), reduced cognitive deficits in schizophrenic patients (Wykes et al., 1999), significant reduction in symptoms of cognitive problems in patients with stroke (Westerberg et al., 2007) and improved fatigue symptoms in adults with multiple sclerosis (Vogt et al., 2009). Some of these studies reported no improvement in fluid intelligence (Westerberg et al., 2007; Holmes et al., 2009; Van der Molen et al., 2010) and some reported significant improvement (Klingberg et al., 2002; Klingberg et al., 2005). Anecdotal examples in the current study have been encouraging where some participants in the study claimed that they have improved their focus in general.

- Buschkuehl, M., Jaeggi, S. M., Hutchinson, S., Perrig-Chiello, P., Dapp, C., Muller, M., Breil, F., Hoppeler, H & Perrig, W. J. (2008). Impact of working memory training on memory performance in old-old adults. Psychology and Aging, 23(4), 743-753
- Holmes, J., Gathercole, S. E. & Dunning, D. L. (2009). Adaptive training leads to sustained enhancement of poor working memory in children. Developmental Science, 12, F9-F15
- Van der Molen, M. J., Van Luit, J. E. H., Van der Molen, M. W., Klugkist, I. & Jongmans, M. J. (2010). Effectiveness of a computerized working memory training in adolescents with mild to borderline intellectual disabilities. Journal of Intellectual Disability Research, 54, 433-447
- Wykes, T., Reeder, C., Corner, J., Williams, C. & Everitt, B. (1999). The effects of neurocognitive remediation on executive processing in patients with schizophrenia. Schizophrenia Bulletin, 25(2), 291-307
- Westerberg, H., Jacobaeus, H., Hirvikoski, T., Clevberger, P., Ostensson, M. L., Bartfai, A. & Klingberg, T. (2007). Computerized working memory training after stroke - A pilot study. Brain Injury, 21(1), 21-29
- Klingberg, T., Fernell, E., Olesen, P. J., Johnson, M., Gustafsson, P., Dahlstrom, K., et al. (2005). Computerized training of working memory training in children with ADHD - a randomized, controlled trial. Journal of the American Academy of Child and Adolescent Psychiatry, 44(2), 177-186.
- Klingberg, T., Forssberg, H. & Westerberg, H. (2002). Training of working memory in children with ADHD. Journal of Clinical and Experimental Neuropsychology, 24(6), 781-791.

-->
<!--
Briner et al 2011 http://drum.lib.umd.edu/bitstream/1903/11386/2/COGNITIVE%20TRAINING.pdf

> Research has found cognitive deficits in individuals with schizophrenia, suggesting that successful cognitive training may be a valid way of remediating these deficits and allowing schizophrenics to lead more normal lives (Barch, 2005; Lesh, Niendam, Minzenberg, & Carter, 2011; Nestor, Niznikiewicz, & McCarley, 2010; Ranganath, Minzenberg, & Ragland, 2008). Training may also be used to maintain cognitive abilities into old age and ward off dementia (Greenwood, 2007).

> Faille (2006), in a study investigating the effects of cognitive training on the elderly, found that participants with possible mild cognitive impairment outperformed those who were cognitively intact after training. She also found that individuals with no impairment or mild impairment respond similarly to training, and that men and women respond similarly as well.

> Despite the existence of age-related brain atrophy in adults due to synapse loss, dendritic regression, and white matter degeneration, studies suggest that neuroplasticity can facilitate physical recovery in elderly adults (Greenwood 2007). Greenwood (2007) demonstrated that despite the existence of brain shrinkage and atrophy in participants, neuroimaging showed that strategy-induced plasticity training caused increased activity in regions adjacent to the atrophied areas of the prefrontal cortex.

> Chein et al. (2010) found that verbal and visuo-spatial working memory training transferred to measures of inhibition and reading comprehension, but did not lead to measures of abstract reasoning believed to heavily rely on working memory.

> We utilized a training task similar to the Spanboard task (Olesson, 2004; Klingberg, 2005) in order to target working memory, attentional control and inhibition response; however, our version of the task was made adaptive in order to maximize cognitive engagement and keep participants performing at their cognitive capacity. A battery of pre- and post-tests was selected to assess both baseline performance of participants and gauge changes in performance after training. All confounds listed by Shipstead were considered when designing our methodology, and were controlled for as best as possible.

> Our study found transfer effects in all groups to be limited. Both adaptive working memory training and non-adaptive arithmetic training led to improvement on trained tasks, and near transfer to other measures of the trained constructs. The Working Memory group improved on five measures of working memory: two of which were the training tasks, and three of which were untrained tasks. The Math and Reading Comprehension group improved on four measures of math ability: one of which was the training task, and three of which were untrained. This provides further evidence for near transfer of working memory training found by previous studies (Chein et al., 2010). The Working Memory group improved more on the Block Span and Shape Builder tasks than either the Math and Reading Comprehension group or the Low Contact group. This was expected, as the Math and Reading Comprehension group did not train on these tasks and the Low Contact group did not receive as much training as the Working Memory group. Interestingly, both the Working Memory group and the Math and Reading Comprehension group demonstrated more improvement on the Verbal Learning task than the Low Contact group, as the Low Contact group actually decreased performance on all measures of Verbal Learning. The training tasks, as well as the factors they loaded on, correlated well with the Verbal Learning recall variables and the Verbal Learning task factor. This, in combination with the results, led to the conclusion that training on either visuo-spatial working memory tasks or quantitative reasoning tasks can lead to improvement on the recall portion of the Verbal Learning task.
-->

<!--
[Bomyea & Amir 2011](/docs/dnb/2011-bomyea.pdf "The Effect of an Executive Functioning Training Program on Working Memory Capacity and Intrusive Thoughts")

> A number of studies document a relationship between performance on WMC tasks and inhibitory control. For example, individuals with higher WMC experience less proactive interference from previously learned information during cognitive tasks (e.g., Rosen and Engle 1998). Moreover, individuals with greater WMC are better able to deliberately suppress neutral (Brewin and Beaton 2002) and negative, personally relevant thoughts during thought suppression tasks (Brewin and Smart 2005). However, extant literature is limited by correlational study designs and cannot speak to causal relationships between WMC and intrusive thoughts.

> To manipulate proactive interference, participants completed one of two modified Reading Span tasks (Rspan; Lustig et al. 2001). Similar to the Ospan, Rspan tasks require participants to simultaneously remember items while concurrently performing a secondary task. However unlike Ospan, Rspan involved reading a sentence and verifying its semantic accuracy (e.g., ''Jane walked her car in the park,'' correct answer: no) instead of verifying math accuracy. Participants completed three blocks of training in a single session. Within each block, participants were trained on span sizes of two to six, with three repetitions of each span size presented in random order (45 trials lasting approximately 30 min).

> Results from this study converge with research demonstrating malleability of executive functioning with practice (e.g., Persson and Reuter-Lorenz 2008) and extend this literature by demonstrating that improvements in WMC (rather than absolute levels of WMC) influenced the regulation of intrusive thoughts. Moreover, the present findings support theoretical accounts of a relationship between domain-general executive functioning and the regulation of intrusive cognitions (e.g., Verwoerd et al. 2008). Clinical disorders, and emotional distress in general, are associated with deficits in specific aspects of executive functioning (e.g., poor attention control in anxiety, Eysenck et al. 2007; difficulty removing negative information from working memory in depression, Joormann and Gotlib 2008). A growing body of evidence indicates that computer based interventions can effectively modify etiologically significant cognitive biases (e.g., attention bias) and decrease anxiety symptoms (for a recent meta-analysis see Hakamata et al. 2010). Given the role of inhibitory control in regulating cognitions, this form of executive functioning might be targeted using similar interventions. We are currently evaluating the effects of a 4-week version of the present cognitive training program in a sample of women with PTSD (see author note).
-->

<!-- non-replication for simple WM task

http://dept.wofford.edu/ScienceResearch/fall2009/4.pdf

12 participants in each group, trained for 10 days (140 trials of 12 stimuli with 5 breaks), non n-back one modality test. Experimental maintained small IQ lead over control group in first post test, but increased it somewhat by the second post test.
-->

### N-back

The original N-back test simply asked that you remember a single stream of letters, and signal if any letters were precisely, say, 2 positions apart. 'A S S R' wouldn't merit a signal, but '_A_ S _A_ R' would since there are 'A' characters exactly 2 positions away from each other. The program would give you another letter, you would signal or not, and so on. This is simple enough once you understand it, but is a little hard to explain. It may be best to read the [Brain Workshop tutorial](http://brainworkshop.sourceforge.net/tutorial.html), or [watch a video](http://www.youtube.com/watch?v=C1blFZoJSuQ).

### Dual N-back

In 2003, Susan Jaeggi and her team began [fMRI](!Wikipedia "Functional magnetic resonance imaging") studies using a variant of N-back which tried to increase the burden on each turn - remembering multiple things instead of just 1. The abstract describes the reason why:

> With reference to single tasks, activation in the prefrontal cortex (PFC) commonly increases with incremental memory load, whereas for dual tasks it has been hypothesized previously that activity in the PFC decreases in the face of excessive processing demands, i.e., if the capacity of the working memory's central executive system is exceeded. However, our results show that during both single and dual tasks, prefrontal activation increases continuously as a function of memory load. An increase of prefrontal activation was observed in the dual tasks even though processing demands were excessive in the case of the most difficult condition, as indicated by behavioral accuracy measures. The hypothesis concerning the decrease in prefrontal activation could not be supported and was discussed in terms of motivation factors.[^jaeggi2003]

[^jaeggi2003]: Jaeggi, S. M., Seewer, R., Nirkko, A. C., Eckstein, D., Schroth, G., Groner, R., et al, (2003). ["Does excessive memory load attenuate activation in the prefrontal cortex? Load-dependent processing in single and dual tasks: functional magnetic resonance imaging study"](/docs/dnb/2003-jaeggi.pdf), _Neuroimage_ 19(2) 210-225.

In this version, called "*dual* N-back" (to distinguish it from the classic single N-back), one is still playing a turn-based game. In the [Brain Workshop](http://brainworkshop.sourceforge.net/) version, you are presented with a 3x3 grid in which every turn, a block appears in 1 of the 9 spaces and a letter is spoken aloud. (There are any number of variants: the NATO phonetic alphabet, piano keys, etc. And Brain Workshop has any number of modes, like 'Arithmetic N-back' or 'Quintuple N-back'.)

#### 1-back

In 1-back, the task is to correctly answer whether the *letter* is the same as the previous round, and whether the *position* is the same as the previous round. It can be both, making 4 possible responses (position, sound, position+sound, & neither).

This stresses working memory since you need to keep in mind 4 things simultaneously: the position and letter of the previous turn, and the position and letter of the current turn (so you can compare the current letter with the old letter and the current position with the old position). Then on the next turn you need to immediately forget the old position & letter (which are now useless) and remember the new position and letter. So you are constantly remembering and forgetting and comparing.

#### 2-back

But 1-back is pretty easy. The turns come fast enough that you could easily keep the letters in your [phonological loop](!Wikipedia "Baddeley's model of working memory#Phonological loop") and lighten the load on your working memory. Indeed, after 10 rounds or so of 1-back, I mastered it - I now get 100%, unless I forget for a second that it's 1-back and not 2-back (or I simply lose my concentration completely). Most people find 1-back very easy to learn, although a bit challenging at first since the pressure is constant (games and tests usually have some slack or rest periods).

The next step up is a doozy: 2-back. In 2-back, you do the same thing as 1-back but as the name suggests, you are instead matching against *2* turns ago. So before you would be looking for repeated letters - 'AA' - but now you need to look for separated letters - 'ABA'. And of course, you can't forget so quickly, since you still need to match against something like 'ABABA'.

2-back stresses your working memory even more, as now you are remembering 6 things, not 4: 2 turns ago, the previous turn, and the current turn - all of which have 2 salient features. At 6 items, we're also in the mid-range of estimates for [normal working memory capacity](!Wikipedia "Working memory#Capacity"):

> Working memory is generally considered to have limited capacity. The earliest quantification of the capacity limit associated with short-term memory was the ["magical number seven"](!Wikipedia "The Magical Number Seven, Plus or Minus Two") introduced by Miller (1956). He noticed that the memory span of young adults was around seven elements, called chunks, regardless whether the elements were digits, letters, words, or other units. Later research revealed that span does depend on the category of chunks used (e.g., span is around seven for digits, around six for letters, and around five for words), and even on features of the chunks within a category....Several other factors also affect a person's measured span, and therefore it is difficult to pin down the capacity of short-term or working memory to a number of chunks. Nonetheless, Cowan (2001) has proposed that working memory has a capacity of about four chunks in young adults (and fewer in children and old adults).

And even if there are only a few things to remember, the number of responses you have to choose between go up exponentially with how many 'modes' there are, so Triple N-back has not ⅓ more possible responses than Dual N-back, but more than twice as many: if _m_ is the number of modes, then the number of possible responses is 2^m^-1 (the -1 is there because one can nothing in every mode, but that's boring and requires no choice or thought), so DNB has 3 possible responses[^math], while TNB has 7^[or 2^3^ - 1], Quadruple N-back 15^[or 2^4^ - 1], and Quintuple N-back 31^[or 2^5^ - 1]!

[^math]: 2^2^ = 4; 4-1 = 3. For DNB, the 3 responses are:

     1. audio match
     2. visual match
     3. audio & visual matches

Worse, the temporal gap between elements is deeply confusing. It's particularly bad when there's repetition involved - if the same square is selected twice with the same letter, you might wind up forgetting both!

So 2-back is where the challenge first really manifests. After about 20 games I started to get the hang of it. (It helped to play a few games focusing only on one of the stimuli, like the letters; this helps you get used to the 'reaching back' of 2-back.)

### Personal reflection on results

Have I seen any benefits yet? Not really. Thus far it's like meditation: I haven't seen any specific improvements, but it's been interesting just to explore concentration - I've learned that my ability to focus is much less than I thought it was! It is very sobering to get 30% scores on something as trivial as 1-back and strain to reach D2B, and even more sobering to score 60% and minutes later score 20%. Besides the intrinsic interest of changing one's brain through a simple exercise - meditation is equally interesting for how one's mind refuses to cooperate with the simple work of meditating, and I understand that there are even vivid hallucinations at the higher levels - N-back might function as a kind of mental calisthenics. Few people exercise and stretch because they find the activities intrinsically valuable, but they serve to further some other goal; some people jog because they just enjoy running, but many more jog so they can play soccer better or live longer. I am young, and it's good to explore these sorts of calisthenics while one has a long life ahead of one; then one can reap the most benefits.

<!--
Not long after starting N-back, while still at 2-back in February 2009, I did the iqtest.dk and was scored at 115. This may not be particularly accurate (I was hungry, and haven't taken such a test in at least a decade), but that's what I got. I plan to retest in a few months. Hopefully I will have forgotten enough about the .dk test (the questions don't change, alas) that a second time through won't be too inaccurate.

Note to self: I started taking piracetam on 13 September during a N-back break. I currently peaking at 60% on 5-back as of 14 September. The above IQ test was in the middle of February.
-->

## Terminology

N-back training is sometimes referred to simply as 'N-backing', and participants in such training are called 'N-backers'. Almost everyone uses the Free, featureful & portable program [Brain Workshop](http://brainworkshop.sourceforge.net/), abbreviated "BW" (but see [the software section](#software) for alternatives).

There are many variants of N-back training. A 3-letter acronym ending in 'B' specifies one of the possibilities. For example, 'D2B' and 'D6B' both refer to a dual N-back task, but in the former the depth of recall is 2 turns, while in the latter one must remember back 6 rounds; the 'D', for 'Dual', indicates that each round presents 2 stimuli (usually the position of the square, and a spoken letter).

But one can add further stimuli: spoken letter, position of square, and *color* of square. That would be 'Triple N-back', and so one might speak of how one is doing on 'T4B'.

One can go further. Spoken letter, position, color, and *geometric shape*. This would be 'Quad N-back', so one might discuss one's performance on 'Q3B'. (It's unclear how to compare the various modes, but it seems to be much harder to go from D2B to T3B than to go from D2B to D3B.)

Past QNB, there is Pentuple N-back (PNB) which was added in Brain Workshop 4.7 ([video demonstration](/docs/dnb/2009-argumzio-pentuple-nback.avi)). The 5th modality is added by a second audio channel - that is, now sounds are in stereo.

Other abbreviations are in common use: 'WM' for 'working memory', '_Gf_' for '[fluid intelligence](!Wikipedia)', and '_g_' for the [general intelligence factor](!Wikipedia "General intelligence factor") measured by IQ tests.

# Notes from the author

## N-back in general

To those whose time is limited: you may wish to stop reading here. If you seek to improve your life, and want the greatest 'bang for the buck', you are well-advised to look elsewhere.

Meditation, for example, is easier, faster, and ultra-portable. Typing training will directly improve your facility with a computer, a valuable skill for this modern world. [Spaced repetition]() memorization techniques offer unparalleled advantages to students. [Nootropics](!Wikipedia) are the epitome of ease (just swallow!), and their effects are much more easily assessed - one can even run double-blind experiments on oneself, impossible with dual N-back. Other supplements like [melatonin](Melatonin) can deliver benefits incommensurable with DNB - what is the cognitive value of another number in working memory thanks to DNB compared to a good night's sleep thanks to melatonin? Modest changes to one's diet and environs can fundamentally improve one's well-being. Even basic training in reading, with the crudest [tachistoscope](!Wikipedia) techniques, can pay large dividends if one is below a basic level of reading like 200[WPM](!Wikipedia) & still subvocalizing. And all of these can start paying off immediately.

DNB, on the other hand, requires a minimum of 15 hours before one can expect genuine somatic improvements. The task itself is unproven - the Jaeggi studies are suggestive, not definitive (and there are [contrary results](#criticism)). Programs for DNB training rely essentially on guesswork as they explore the large design-space; there are no data on what features are essential, what sort of presentation optimal, or even how long or when to train for. The task itself is unenjoyable. It can be wearying, difficult & embarrassing. It can be one too many daily tasks, a straw which breaks the camel's back, and a distraction from whatever activity has the greatest [marginal utility](!Wikipedia) for one[^charity] and one ought to be doing instead.

[^charity]: Spreading one's efforts over a variety of activities is not necessarily a good thing, and can be sub-optimal; consider the charity example (["Giving Your All"](http://www.slate.com/id/2034/), Steven E. Landsburg):

    > People constantly ignore my good advice by contributing to the American Heart Association, the American Cancer Society, CARE, and public radio all in the same year--as if they were thinking, "OK, I think I've pretty much wrapped up the problem of heart disease; now let's see what I can do about cancer."

So why then do I persevere with DNB?

I do it because I find it fascinating. Fascinating that WM can be so large a part of IQ; fascinating that it can be increased by an apparently trivial exercise. I'm fascinated that there are measurable gross changes in brain activity & chemistry & composition^[see eg. [McNab](#whats-some-relevant-research) or Westerberg.] - that the effects are not purely 'mental' or placebo. I'm fascinated by how the sequence of positions and letters can at some times appear in my mind with boundless lucidity, yet at other times I grope confused in a mental murk unsure of even what the last position/letter was - even though I can rise from my computer and go about normal activities normally; or with how time can stretch and compress during N-backing[^time]. I'm fascinated by how a single increase in _n_-level can render the task nightmarishly difficult when I just finished _n-1_ at 90 or 100%. I'm fascinated by how saccading, another apparently trivial exercise, can reliably boost my score by 10 or 20%, and how my mind seems to be fagged after just a few rounds but recovers within minutes. I'm equally fascinated by the large literature on WM: what it is, what's it good for, how it can be manipulated, etc.

[^time]: I'm not the only one to notice this. 'y offs et' mentions during a [discussion of TNB](http://groups.google.com/group/brain-training/browse_thread/thread/f70b31bab31fafc4/67a6717ba9b06988) that:

     > It's interesting how doing n-back proves that time is relative and based upon our perception of its passing
     >
     > When I'm doing well, the next instance comes with metronome exactness as expected from a machine. When I'm resetting after a tricky double-back, the next instance always comes way too quickly, as if a second had been removed. The same perception happens on an upped level, and it is so persistent. It's like some time had vanished.
     >
     > For the longest time I thought the program had a bug, being the mere human.

I do not think that DNB is terribly practical - but interesting? Very.

## Reading this FAQ

> `Brian`: "Look, you've got it all wrong! You don't *need* to follow *me*, You don't *need* to follow *anybody*! You've got to think for your selves! You're *all* individuals!"
>
> `The Crowd`: "Yes! We're all individuals!"^[_[Monty Python's Life of Brian](!Wikipedia)_]

This FAQ is almost solely my own work. I've striven to make it fair, to incorporate most of the relevant research, and to not omit things. But inevitably I will have made errors or important omissions. You must read this skeptically.

You must read this skeptically also because the N-back community formed around the [mailing list](http://groups.google.com/group/brain-training) *is* a community. That means it is prone to all the biases and issues of a community. One would expect a community formed around a technique or practice to be made up only of people who find value in it; any material (like this FAQ or included testimonials) is automatically suspect due to biases such as the [commitment or sunk cost bias](!Wikipedia "Escalation of commitment"). Imagine if scientists published only papers which showed new results, and no papers reporting failure to [replicate](!Wikipedia "Reproducibility")! Why would any N-backer hang around who had discovered that DNB was not useful or a fraud? Certainly the fans would not thank him. ([Eliezer Yudkowsky](!Wikipedia) has an excellent essay called ["Evaporative Cooling of Group Beliefs"](http://lesswrong.com/lw/lr/evaporative_cooling_of_group_beliefs/) on this topic; fortunately, the damage caused by a dual n-back would be limited, in comparison to some *other* examples of evaporative cooling like [pro-ana](!Wikipedia) or [mind-control victims](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.9838&rep=rep1&type=pdf "'`Mind control experiences` on the internet: Implications for the psychiatric diagnosis of delusions', Bell et al 2006").)

Finally, you must read skeptically because this is about psychology. Psychology is notoriously for being one of the hardest scientific fields to get solid results in, because everybody is [WEIRD](http://lesswrong.com/lw/17x/beware_of_weird_psychological_samples/) and different. As one of my professors joked, "if you have 2 psychology papers reporting he same result, one of them is wrong"; there are many issues with taking a psychology study at face value (to which I have devoted [an appendix, "Flaws in mainstream science (and psychology)"](#flaws-in-mainstream-science-and-psychology)). It's very tempting to engage in ["Generalizing From One Example"](http://lesswrong.com/lw/dr/generalizing_from_one_example/) but you mustn't. Everybody is different; your positive (or negative) result could be due to a [placebo effect](!Wikipedia), it could be thanks to that recent shift in your sleep schedule for the better[^sleep], or that nap you took[^nap], it could be the exercise you're getting[^exercise], it could be a mild [seasonal depression](!Wikipedia "Seasonal affective disorder") lifting (or setting in), it could be a [calcium](http://lesswrong.com/lw/15w/experiential_pica/) or [zinc](!Wikipedia "Zinc deficiency")[^zinc] or [iodine deficiency](Nootropics#iodine), [hypoglycemia](!Wikipedia)[^blood-sugar], variation [in](http://www.pnas.org/content/108/19/7716.full "'Role of test motivation in intelligence testing' Duckworth et al 2011") [motivation](/docs/dnb/2010-moe.pdf "'Beyond genetics in Mental Rotation Test performance: The power of effort attribution', Moe & Pazzaglia 2010") etc.

[^sleep]: Sleep affects IQ, not just vigilance or energy: ["Adolescent sleep and fluid intelligence performance"](http://jtoomim.org/brain-training/fluid%20intelligence%20and%20sleep.pdf), Johnstone et al 2010; abstract:

    > Fluid intelligence involves novel problem-solving and may be susceptible to poor sleep. This study examined relationships between adolescent sleep, fluid intelligence, and academic achievement. Participants were 217 adolescents (42% male) aged 13 to 18 years (mean age, 14.9 years; SD = 1.0) in grades 9-11. Fluid intelligence was predicted to mediate the relationship between adolescent sleep and academic achievement. Students completed online questionnaires of self-reported sleep, fluid intelligence (Letter Sets and Number Series), and self-reported grades. Total sleep time was not significantly related to fluid intelligence nor academic achievement (both _p_>0.05); however, sleep difficulty (e.g. difficulty initiating sleep, unrefreshing sleep) was related to both (_P_ < 0.05)...

    Further, we can easily [delude ourselves](http://www.nytimes.com/2011/04/17/magazine/mag-17Sleep-t.html) about our own mental states:

    > Still, while it's tempting to believe we can train ourselves to be among the five-hour group - we can't, Dinges says - or that we are naturally those five-hour sleepers, consider a key finding from Van Dongen and Dinges's study: after just a few days, the four- and six-hour group reported that, yes, they were slightly sleepy. But they insisted they had adjusted to their new state. Even 14 days into the study, they said sleepiness was not affecting them. In fact, their performance had tanked. In other words, the sleep-deprived among us are lousy judges of our own sleep needs.
[^blood-sugar]: ["Acute hypoglycemia impairs nonverbal intelligence: importance of avoiding ceiling effects in cognitive function testing."](http://jtoomim.org/brain-training/Acute%20Hypoglycemia%20Impairs%20Nonverbal.pdf). While we're at it, blood sugar seems to be closely linked to attention/self-control/self-discipline (see LW discussions: ["The Physiology of Willpower"](http://lesswrong.com/lw/10x/the_physiology_of_willpower/), ["Willpower: not a limited resource?"](http://lesswrong.com/lw/2y2/willpower_not_a_limited_resource/), ["What would you do if blood glucose theory of willpower was true?"](http://lesswrong.com/lw/1yn/what_would_you_do_if_blood_glucose_theory_of/), [Vladimir/ Golovin](http://lesswrong.com/lw/1fe/antiakrasia_technique_structured_procrastination/196o), and ["Superstimuli and the Collapse of Western Civilization"](http://lesswrong.com/lw/h3/superstimuli_and_the_collapse_of_western/)). For a roundup of all the research, read Baumeister & Tierney's 2011 book [_Willpower_](http://www.amazon.com/Willpower-Rediscovering-Greatest-Human-Strength/dp/0143122231/). Quotes from ["Do You Suffer From Decision Fatigue?"](http://www.nytimes.com/2011/08/21/magazine/do-you-suffer-from-decision-fatigue.html?pagewanted=all), _NYT_, itself quoting from Baumeister & Tierney 2011:

    > Once you're mentally depleted, you become reluctant to make trade-offs, which involve a particularly advanced and taxing form of decision making. In the rest of the animal kingdom, there aren't a lot of protracted negotiations between predators and prey. To compromise is a complex human ability and therefore one of the first to decline when willpower is depleted. You become what researchers call a cognitive miser, hoarding your energy. If you're shopping, you're liable to look at only one dimension, like price: just give me the cheapest. Or you indulge yourself by looking at quality: I want the very best (an especially easy strategy if someone else is paying). Decision fatigue leaves you vulnerable to marketers who know how to time their sales, as Jonathan Levav, the Stanford professor, demonstrated in experiments involving tailored suits and new cars.
    >
    > Most of us in America won't spend a lot of time agonizing over whether we can afford to buy soap, but it can be a depleting choice in rural India. Dean Spears, an economist at Princeton, offered people in 20 villages in Rajasthan in northwestern India the chance to buy a couple of bars of brand-name soap for the equivalent of less than 20 cents. It was a steep discount off the regular price, yet even that sum was a strain for the people in the 10 poorest villages. Whether or not they bought the soap, the act of making the decision left them with less willpower, as measured afterward in a test of how long they could squeeze a hand grip. In the slightly more affluent villages, people's willpower wasn't affected significantly...To establish cause and effect, researchers at Baumeister's lab tried refueling the brain in a series of experiments involving lemonade mixed either with sugar or with a diet sweetener. The sugary lemonade provided a burst of glucose, the effects of which could be observed right away in the lab; the sugarless variety tasted quite similar without providing the same burst of glucose. Again and again, the sugar restored willpower, but the artificial sweetener had no effect. The glucose would at least mitigate the ego depletion and sometimes completely reverse it. The restored willpower improved people's self-control as well as the quality of their decisions: they resisted irrational bias when making choices, and when asked to make financial decisions, they were more likely to choose the better long-term strategy instead of going for a quick payoff. The ego-depletion effect was even demonstrated with dogs in two studies by Holly Miller and Nathan DeWall at the University of Kentucky. After obeying sit and stay commands for 10 minutes, the dogs performed worse on self-control tests and were also more likely to make the dangerous decision to challenge another dog's turf. But a dose of glucose restored their willpower. The results of the experiment were announced in January, during Heatherton's speech accepting the leadership of the Society for Personality and Social Psychology, the world's largest group of social psychologists. In his presidential address at the annual meeting in San Antonio, Heatherton reported that administering glucose completely reversed the brain changes wrought by depletion  -  a finding, he said, that thoroughly surprised him. Heatherton's results did much more than provide additional confirmation that glucose is a vital part of willpower; they helped solve the puzzle over how glucose could work without global changes in the brain's total energy use. Apparently ego depletion causes activity to rise in some parts of the brain and to decline in others. Your brain does not stop working when glucose is low. It stops doing some things and starts doing others. It responds more strongly to immediate rewards and pays less attention to long-term prospects.
    >
    > ...The psychologists gave preprogrammed BlackBerrys to more than 200 people going about their daily routines for a week. The phones went off at random intervals, prompting the people to report whether they were currently experiencing some sort of desire or had recently felt a desire. The painstaking study, led by Wilhelm Hofmann, then at the University of Würzburg, collected more than 10,000 momentary reports from morning until midnight.
    >
    > Desire turned out to be the norm, not the exception. Half the people were feeling some desire when their phones went off  -  to snack, to goof off, to express their true feelings to their bosses  -  and another quarter said they had felt a desire in the past half-hour. Many of these desires were ones that the men and women were trying to resist, and the more willpower people expended, the more likely they became to yield to the next temptation that came along. When faced with a new desire that produced some I-want-to-but-I-really-shouldn't sort of inner conflict, they gave in more readily if they had already fended off earlier temptations, particularly if the new temptation came soon after a previously reported one. The results suggested that people spend between three and four hours a day resisting desire. Put another way, if you tapped four or five people at any random moment of the day, one of them would be using willpower to resist a desire. The most commonly resisted desires in the phone study were the urges to eat and sleep, followed by the urge for leisure, like taking a break from work by doing a puzzle or playing a game instead of writing a memo. Sexual urges were next on the list of most-resisted desires, a little ahead of urges for other kinds of interactions, like checking Facebook. To ward off temptation, people reported using various strategies. The most popular was to look for a distraction or to undertake a new activity, although sometimes they tried suppressing it directly or simply toughing their way through it. Their success was decidedly mixed. They were pretty good at avoiding sleep, sex and the urge to spend money, but not so good at resisting the lure of television or the Web or the general temptation to relax instead of work.
    >
    > ...'Good decision making is not a trait of the person, in the sense that it's always there,' Baumeister says. 'It's a state that fluctuates.' His studies show that people with the best self-control are the ones who structure their lives so as to conserve willpower. They don't schedule endless back-to-back meetings. They avoid temptations like all-you-can-eat buffets, and they establish habits that eliminate the mental effort of making choices. Instead of deciding every morning whether or not to force themselves to exercise, they set up regular appointments to work out with a friend. Instead of counting on willpower to remain robust all day, they conserve it so that it's available for emergencies and important decisions....'Even the wisest people won't make good choices when they're not rested and their glucose is low,' Baumeister points out. That's why the truly wise don't restructure the company at 4 p.m. They don't make major commitments during the cocktail hour. And if a decision must be made late in the day, they know not to do it on an empty stomach. 'The best decision makers,' Baumeister says, 'are the ones who know when not to trust themselves.'
[^exercise]: [Aerobic exercise](!Wikipedia) has [been shown](http://well.blogs.nytimes.com/2009/09/16/what-sort-of-exercise-can-make-you-smarter/) to improve mental fitness. One [small study](http://ukpmc.ac.uk/articles/PMC3088429 "'Exercise and Executive Function in Individuals With Chronic Stroke: A Pilot Study', Kluding et al 2011") with old diabetics found improvement in working memory/executive function caused by an aerobic exercise regimen, and another found [increased brain volume](http://www2.pitt.edu/~bachlab/LabSite/Publications.html/colcombe2006.pdf "'Aerobic Exercise Training Increases Brain Volume in Aging Humans', Colcombe et al 2006") and [increased hippocampal volume & BDNF secretion](http://www.pnas.org/content/108/7/3017.full "'Exercise training increases size of hippocampus and improves memory', Erickson et al 2011") in healthy old people; a [Cochrane Collaboration](!Wikipedia) found benefits in [8 of 11](http://web.archive.org/web/20100110234950/http://www.cochrane.org/reviews/en/ab005381.html "'Physical activity and enhanced fitness to improve cognitive function in older people without known cognitive impairment, Angevaren et al 2008") aerobic interventions in the elderly. And exercise improves [working memory](http://www.setantacollege.com/wp-content/uploads/Journal_db/The%20Effect%20of%20Acute%20Aerobic%20and%20Resistance.pdf "'The effect of acute aerobic and resistance exercise on working memory', Pontifex et al 2009") (or at least [correlated](http://www.pnas.org/content/early/2009/11/25/0905307106) with intelligence & education in twins), and there is some [suggestive evidence](http://well.blogs.nytimes.com/2011/01/19/phys-ed-brains-and-brawn/) that [strength training](!Wikipedia) or [resistance training](!Wikipedia) may help as well. One possible mechanism (in rats, anyway) is [increases in chemical energy storage in the brain](http://jp.physoc.org/content/590/3/607.full.pdf "'Brain glycogen supercompensation following exhaustive exercise', Matsui et al 2012"). For further reading, see the review & reviews cited in ["Exercise and Children's Intelligence, Cognition, and Academic Achievement"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2748863/ "Tomporowski et al 2008") and [Wikipedia](!Wikipedia "Physical exercise#Brain function").
[^zinc]: See for example ["Zinc status and cognitive function of pregnant women in Southern Ethiopia"](/docs/dnb/2009-stoecker.pdf "Stoecker et al 2009") or ["Zinc supplementation improved cognitive performance and taste acuity in Indian adolescent girls"](http://intl.jacn.org/content/28/4/388.full "Tupe & Chiplonkar 2009")
[^nap]: ["NASA Naps: NASA-supported sleep researchers are learning new and surprising things about naps."](http://science.nasa.gov/science-news/science-at-nasa/2005/03jun_naps/), 3 June 2005:

     > "To our amazement, working memory performance benefited from the naps, [but] vigilance and basic alertness did not benefit very much," says Dinges.

# N-back training

## Should I do multiple daily sessions, or just one?

[Most users](http://groups.google.com/group/brain-training/browse_thread/thread/5e989058544ad122) seem to go for one long N-back session, pointing out that exercises one's focus. Others do one session in the morning and one in the evening so they can focus better on each one. There is some scientific support for the idea that evening sessions are better than morning sessions, though; see [Kuriyama 2008](#sleep) on how practice before bedtime was more effective than after waking up.

If you break up sessions into more than 2, you're probably wasting time due to overhead, and may not be getting enough exercise in each session to really strain yourself like you need to.

## Strategies

The simplest mental strategy, and perhaps the most common, is to mentally think of a list, and forget the last one each round, remembering the newest in its place. This begins to break down on higher levels - if one is repeating the list mentally, the repetition can just take too long.

Surcer writes up a list of strategies for different levels in his ["My System, let's share strategies"](http://groups.google.com/group/brain-training/browse_thread/thread/cf11964a87ed658c) thread.

### Are strategies good or bad?

People [frequently](http://groups.google.com/group/brain-training/browse_thread/thread/fd08b93399939fb9) [ask](http://groups.google.com/group/brain-training/browse_thread/thread/d779edc2b922db95) [and](http://groups.google.com/group/brain-training/browse_thread/thread/676676a8e34f56d3) [discuss](http://groups.google.com/group/brain-training/browse_thread/thread/c4eaff3059759531) whether they should use some sort of strategy, and if so, what.

A number of N-backers adopt an 'intuition' strategy. Rather than explicitly rehearsing sequences of letters ('f-up, h-middle; f-up, h-middle; g-down, f-up...'), they simply think very hard and wait for a feeling that they should press 'a' (audio match), or 'l' (location match). Some, like SwedishChef can be quite vociferous [about it](http://groups.google.com/group/brain-training/browse_thread/thread/d779edc2b922db95):

> The challenges are in helping people understand that dual-n-back is NOT about remembering n number of visual and auditory stimuli.  It's about developing a new mental process that intuitively recognizes when it has seen or heard a stimuli n times ago.
>
> Initially, most students of dual n-back want to remember n items as fast as they can so they can conquer the dual-n-back hill.  They use their own already developed techniques to help them remember.  They may try to hold the images in their head mentally and review them every time a new image is added and say the sounds out loud and review the sounds every time a new sound is added.  This is NOT what we want.  We want the brain to learn a new process that intuitively recognizes if an item and sound was shown 3 back or 4 back.  It's sort of like playing a new type of musical instrument.
>
> I've helped some students on the site try to understand this. It's not about how much you can remember, it's about learning a new process.  In theory, this new process translates into a better working memory, which helps you make connections better and faster.

Other N-backers think that intuition can't work, or at least doesn't very well:

> I don't believe that much in the "intuitive" method. I mean, sure, you can intuitively remember you heard the same letter or saw the square at the same position a few times ago, but I fail to see how you can "feel" it was exactly 6 or 7 times ago without some kind of "active" remembering. --[Gaël DEEST](http://groups.google.com/group/brain-training/browse_thread/thread/716bd6f8c10be52e/8c9c126767d0f662)

> I totally agree with Gaël about the intuitive method not holding much water...For me a lot of times the intuitive method can be totally unreliable. You'll be doing 5-back one game and a few games later your failing miserably at 3-back..your score all over the place. Plus, intuitive-wise, it's best to play the same n-back level over and over because then you train your intuition...and that doesn't seem right. --MikeM (same thread)

Few N-backers have systematically tracked intuitive versus strategic playing; [DarkAlrx](http://groups.google.com/group/brain-training/browse_thread/thread/1dff0955c27d25eb) reports on his [blog](http://darkmindexperiment.blogspot.com/2009/04/neurogenesis-experimentdark-side.html) the results of his experiment, and while he considers them positive, others find them inconclusive, or like Pheonexia, even unfavorable for the intuitive approach:

> Looking at your graphs and the overall drop in your performance, I think it's clear that intuitive doesn't work. On your score sheet, the first picture, using the intuitive method over 38 days of TNB training in 44 days your average n-back increased by less than .25. You were performing much better before. With your neurogenesis experiment, your average n-back actually decreased.

Jaeggi herself was [more moderate](http://groups.google.com/group/brain-training/browse_thread/thread/08bc4ee2ccd0df80) in ~2008:

> I would NOT recommend you [train the visual and auditory task separately] if you want to train the dual-task (the one we used in our study). The reason is that the combination of both modalities is an entirely different task than doing both separately! If you do the task separately, I assume you use some "rehearsal strategies", e.g. you repeat the letters or positions for yourself. In the dual-task version however, these strategies might be more difficult to apply (since you have to do 2 things simultaneously...), and that is exactly what we want... We don't want to train strategies, we want to train processes. Processes that then might help you in the performance of other, non-trained tasks (and that is our ultimate goal). So, it is not important to reach a 7- or 8-back... It is important to fully focus your attention on the task as well as possible.
>
> I can assure you, it is a very tough training regimen.... You can't divert your attention even 1 second (I'm sure you have noticed...). But eventually, you will see that you get better at it and maybe you notice that you are better able to concentrate on certain things, to remember things more easily, etc. (hopefully).

(Unfortunately, doubt has been cast on this advice by the apparent effectiveness of *single* n-back in [Jaeggi 2010](#jaeggi-2010). If single (visual/position) n-back is effective in increasing IQ, then maybe training just audio or just visual is actually a good idea.)

> this is a question i am being asked a lot and unfortunately, i don't really know whether i can help with that. i can only tell you what we tell (or rather not tell) our participants and what they tell us. so, first of all, we don't tell people at all what strategy to use - it is up to them. thing is, there are some people that tell us what you describe above, i.e. some of them tell us that it works best if they don't use a strategy at all and just "let the squares/letters flow by". but of course, many participants also use more conscious strategies like rehearsing or grouping items together. but again - we let people chose their strategies themselves! [ref](http://groups.google.com/group/brain-training/browse_thread/thread/955524caaf2e9001)

But it may make no difference. Even if you are engaged in a complex mnemonic-based strategy, you're still working your memory. Strategies may not work; quoting from Jaeggi's 2008 paper:

> By this account, one reason for having obtained transfer between working memory and measures of _Gf_ is that our training procedure may have facilitated the ability to control attention. This ability would come about because the constant updating of memory representations with the presentation of each new stimulus requires the engagement of mechanisms to shift attention. Also, our training task discourages the development of simple task-specific strategies that can proceed in the absence of controlled allocation of attention.

Even if they do, they may not be a good idea; quoting from [Jaeggi 2010](#jaeggi-2010):

> We also proposed that it is important that participants only minimally learn task-specific strategies in order to prevent specific skill acquisition. We think that besides the transfer to matrix reasoning, the improvement in the near transfer measure provides additional evidence that the participants trained on task-underlying processes rather than relying on material-specific strategies.

Hopefully even if a trick lets you jump from 3-back to 5-back, Brain Workshop will just keep escalating the difficulty until you are challenged again. It's not the level you reach, but the work you do.

## And the flashing right/wrong feedback?

A matter of preference, although those in favor of disabling the visual feedback (`SHOW_FEEDBACK = False`) seem to be [slightly](http://groups.google.com/group/brain-training/browse_thread/thread/775fba46e8c163f1) [more](http://groups.google.com/group/brain-training/browse_thread/thread/420f332fcd4317c9) vocal or numerous. Brain Twister [apparently](http://groups.google.com/group/brain-training/browse_thread/thread/a8934cc1e04075f9) doesn't give feedback. [Jaeggi says](http://groups.google.com/group/brain-training/browse_thread/thread/955524caaf2e9001):

> the gaming literature also disagrees on this issue - there are different ways to think about this: whereas feedback after each trial gives you immediate feedback whether you did right or wrong, it can also be distracting as you are constantly monitoring (and evaluating) your performance. we decided that we wanted people to fully and maximally concentrate on the task itself and thus chose the approach to only give feedback at the end of the run. however, we have newer versions of the task for kids in which we give some sort of feedback (points) for each trial. thus - i can't tell you what the optimal way is - i guess there are interindividual differences and preferences as well.

[Jonathan Toomin](http://groups.google.com/group/brain-training/browse_frm/thread/eeefb3ca685f4b7c) writes:

> When I was doing visual psychophysics research, I heard from my labmates that this question has been investigated empirically (at least in the context of visual psychophysics), and that the consensus in the field is that using feedback reduces immediate performance but improves learning rates.  I haven't looked up the research to confirm their opinion, but it sounds plausible to me. I would also expect it to apply to Brain Workshop.  The idea, as I see it, is that feedback reduces performance because, when you get an answer wrong and you know it, your brain goes into an introspective mode to analyze the reason for the error and (hopefully) correct it, but while in this mode your brain will be distracted from the task at hand and will be more likely to miss subsequent trials.

## How can I do better on N-back?

Focus harder. Play more. Sleep well, and eat healthily. Use natural lighting[^lighting]. Space out practice. The less stressed you are, the better you can do.

[^lighting]: See ["Effects of prior light exposure on early evening performance, subjective sleepiness, and hormonal secretion"](http://infoscience.epfl.ch/record/174780/files/M%C3%BCnch_BNE_2012.pdf?version=1) ([coverage](http://www.goodtherapy.org/blog/natural-lighting-increases-productivity-0104112/)), Münch et al 2011:

    > ...For cognitive performance we found a significant interaction between light conditions, mental load (2- or 3-back task) and the order of light administration. On their first evening, subjects performed with similar accuracy after both light conditions, but on their second evening, subjects performed significantly more accurately after the DL in both n-back versions and committed fewer false alarms in the 2-back task compared to the AL group. Lower sleepiness in the evening was significantly correlated with better cognitive performance (p < .05).

### Spacing

[Penner et al 2012](/docs/dnb/2012-penner.pdf "Computerised working memory training in healthy adults: A comparison of two different training schedules")

> This study compared a high intensity working memory training (45 minutes, 4 times per week for 4 weeks) with a distributed training (45 minutes, 2 times per week for 8 weeks) in middle-aged, healthy adults...Our results indicate that the distributed training led to increased performance in all cognitive domains when compared to the high intensity training and the control group without training. The most significant differences revealed by interaction contrasts were found for verbal and visual working memory, verbal short-term memory and mental speed.

This is reminiscent of sleep's involvement in other forms of memory and cognitive change, and [Kuriyama 2008](#sleep).

### Hardcore

Curtis Warren has [noticed](http://groups.google.com/group/brain-training/browse_thread/thread/5e7ac7e6432b0d77/7ec15b99a2a1a2f0) that when he underwent a 4-day routine of practicing more than 4 hours a day, he jumped an entire level on even quad N-back^["With regards to changes in n-back level, I went up about 1 solid level on all the tasks that I trained. That is, I went from 7 to 8 for dual, 6 to 7 for position-sound-color, 6 to 7 for position-sound-shape, and 4 to 5 on quad. I don't use any strategies."]:

> For example, over the past week I have been trying a new training routine. My goal was to increase my intelligence as quickly as possible. To that end, over the past 4 days I've done a total of roughly 360 sessions @ 2 seconds per trial (= ~360 minutes of training). I had to rest on Wednesday, and I'm resting again today (I only plan on doing about 40 trials today). But I intend to finish off the week by doing 100 sessions on Saturday and another 100 on Sunday. Or more, if I can manage it.

But he cautions us that besides being a considerable time investment, it may only work for him:

> The point is, while I can say without a doubt that this schedule has been effective *for me*, it might not be effective *for you*. Are the benefits worth the amount of work needed? Will you even notice an improvement? Is this healthy? These are all factors which depend entirely upon the individual actually *doing* the training.

Raman started DNB training, and in his first 30 days, he "took breaks every 5 days or so, and was doing about 20-30 session each day and n-back wise I made good gains (from 2 to 7 touching 9 on the way)."; he kept a journal [on the mailing list](http://groups.google.com/group/brain-training/browse_thread/thread/5e7ac7e6432b0d77/9f3b791ee0fb1c38) about the experience with daily updates.

Alas, neither Raman nor Warren took an IQ or digit-span test before starting, so they can only report DNB level increases & subjective assessments.

The research does suggest that diminishing returns does not set in with training regimes of 10 or 15 minutes a day; for example, [Nutley 2011](#nutley-2011) trained 4-year-olds in WM exercises, _Gf_ (NVR) exercises, or both:

> ...These analyses took into account that the groups differed in the amount of training received, full dose for NVR or WM groups or half dose for the CB group (Table 3). Even though the pattern is not consistent across all tests (see Figure 2), this is interpreted as confirmation of the linear dose effect that was expected to be seen. Our results suggest that the amount of transfer to non-trained tasks within the trained construct was roughly proportionate to the amount of training on that construct. A similar finding, with transfer proportional to amount of training, was reported by [Jaeggi et al. (2008)](#jaeggi-2008). This has possible implications for the design of future cognitive training paradigms and suggests that the training should be intensive enough to lead to significant transfer and that training more than one construct does not entail any advantages in itself. The training effect presumably reaches asymptote, but where this occurs is for future studies to determine. It is probably important to ensure that participants spend enough time on each task in order to see clinically significant transfer, which may be difficult when increasing the number of tasks being trained. This may be one of the explanations for the lack of transfer seen in the [Owen et al. study (2010)](http://web.me.com/adrian.owen/site/IQ_files/OwenNaturefinal.pdf) (training six tasks in 10 minutes).

## Plateauing, or, am I wasting time if I can't get past 4-back?

Some people start n-backing with great vigor and rapidly ascend levels until suddenly they stop improving and [panic](http://groups.google.com/group/brain-training/browse_frm/thread/9303a95ba669742d), wondering if something is wrong with them. Not at all! Reaching a high level is a good thing, and if one does so in just a few weeks, all the more impressive since most members take much longer than, say, 2 weeks to reach good scores on D4B. In fact, if you look at the reports in the [Group survey](http://groups.google.com/group/brain-training/browse_frm/thread/9303a95ba669742d), most reports are of plateauing at D4B or D5B months in.

The crucial thing about N-back is just that you are stressing your working memory, that's all. The actual level doesn't matter very much, just whether you can barely manage it; it is somewhat like lifting weights, in that regard. From Jaeggi 2008:

> The finding that the transfer to _Gf_ remained even after taking the specific training effect into account seems to be counterintuitive, especially because the specific training effect is also related to training time. The reason for this capacity might be that participants with a very high level of n at the end of the training period may have developed very task specific strategies, which obviously boosts n-back performance, but may prevent transfer because these strategies remain too task-specific (5, 20). The averaged n-back level in the last session is therefore not critical to predicting a gain in _Gf_; rather, it seems that working at the capacity limit promotes transfer to _Gf_.

Mailing list members [report benefits](http://groups.google.com/group/brain-training/browse_thread/thread/eacf724158e04506) even if they have plateaued at 3 or 4-back; see the [benefits section](#benefits).

One commonly reported tactic to break a plateauing is to deliberately advance a level (or increase modalities), and practice hard on that extra difficult task, the idea being that this will spur adaptation and make one capable of the lower level.

## Do breaks undo my work?

Some people have wondered if not n-backing for a day/week/month or other extended period undoes all their hard work, and hence n-backing may not be useful in the long-term.

Multiple group members have pointed to long gaps in their training, sometimes multiple months up to [a year](http://groups.google.com/group/brain-training/tree/browse_frm/thread/80613920c0fbd422/18f2a024cd8f87e8?rnum=31&_done=%2Fgroup%2Fbrain-training%2Fbrowse_frm%2Fthread%2F80613920c0fbd422%2Fb960c61b9e515755%3Ftvc%3D1%26#doc_e87c3f795821ac64), which did not change their scores significantly (immediately after the break, scores may dip a level or a few percentage points in accuracy, but quickly rises to the old level). Some members have ceased n-backing for 2 or 3 years, and found their scores dropped by only [2-4](http://groups.google.com/group/brain-training/browse_frm/thread/a3e8641c0d5ff624) levels - far from 1 or 2-back. ([Pontus Granström](http://groups.google.com/group/brain-training/browse_frm/thread/b4a99de43af6a33d), on the other hand, took a break for several months and fell for a long period from D8B-D9B to D6B-D7B; he speculates it might reflect a lack of motivation.) [huhwhat/Nova](http://groups.google.com/group/brain-training/browse_frm/thread/b4a99de43af6a33d) fell 5 levels from D9B but recovered quickly:

> I've been training with n-back on and off, mostly off, for the past few years. I started about 3 years ago and was able to get up to 9-n back, but on average I would be doing around 6 or 7 n back. Then I took a break for a few years. Now after coming back, even though I have had my fair share of partying, boxing, light drugs, even polyphasic sleep,  on my first few tries I was able to get back up to 5-6, and a week into it I am back at getting up to 9 n back.

This anecdotal evidence is supported by at least one [WM-training letter](http://www.casl.umd.edu/node/1331), Chrabaszcz 2010:

> Figure 1b illustrates the degree to which training transferred to an ostensibly different (and untrained) measure of verbal working memory compared to a no-contact control group. Not only did training significantly increase verbal working memory, but these gains persisted 3 months following the cessation of training!

Similarly, [Dahlin 2008](/docs/dnb/2008-dahlin.pdf "Plasticity of executive functioning in young and older adults: Immediate training gains, transfer, and long-term maintenance") found WM training gains which were durable over more than a year:

> The authors investigated immediate training gains, transfer effects, and 18-month maintenance after 5 weeks of computer-based training in updating of information in working memory in young and older subjects. Trained young and older adults improved significantly more than controls on the criterion task (letter memory), and these gains were maintained 18 months later. Transfer effects were in general limited and restricted to the young participants, who showed transfer to an untrained task that required updating (3-back)...

## I heard 12-back is possible

Some users have reported being able to go all the way up to 12-back; Ashirgo [regularly plays](http://groups.google.com/group/brain-training/browse_thread/thread/373089e3e049707b/5dbde0450a4d44ca) at D13B, but the highest at other modes seems to be T9B and Q6B.

Ashirgo [offers up](http://groups.google.com/group/brain-training/browse_thread/thread/72aa2de2fdf75edf/6c777b2646227c38?q) her 8-point scheme as to how to accomplish such feats:

> 1. 'Be focused at all cost. The fluid intelligence itself is sometimes called "the strength of focus".
> 2. You had better not rehearse the last position/sound . It will eventually decrease your performance! I mean the rehearsal "step by step": it will slow you down and distract. The only rehearsal allowed should be nearly unconscious and "effortless" (you will soon realize its meaning :)
> 3. Both points 1 & 2 thus imply that you must be focused on the most current stimulus as strongly as you can. Nevertheless, you cannot forget about the previous stimuli. How to do that? You should hold the image of them (image, picture, drawing, whatever you like) in your mind. Notice that you still do not rehearse anything that way.
> 4. Consider dividing the stream of data (n) on smaller parts. 6-back will be then two 3-back, for instance.
> 5. Follow square with your eyes as it changes its position.
> 6. Just turn on the Jaeggi mode with all the options to ensure your task is closest to the original version.
> 7. Consider doing more than 20 trials. I am on my way to do no less than 30 today. It may also help.
> 8. You may lower the difficulty by reducing the fall-back and advance levels from >75 and =<90 to 70 and 85 respectively (for instance).'

# What's some relevant research?

Training WM tasks has yielded a literature of mixed results - for every positive, there's a negative, it seems. The following sections of positive and null results illustrate that, as do the papers themselves; from [Nutley 2011](#nutley-2011):

> However, there are some studies using several WM tasks to train that have also shown transfer effects to reasoning tasks ([Klingberg, Fernell, Olesen, Johnson, Gustafsson, Dahlstrçm, Gillberg, Forssberg & Westerberg, 2005](http://www.klingberglab.se/pub/CompTrainWM.pdf); [Klingberg, Forssberg & Westerberg, 2002](http://www.teach-the-brain.org/learn/data/Klingberg.pdf)), while other WM training studies have failed to show such transfer ([Dahlin, Neely, Larsson, Backman & Nyberg, 2008](http://olms.noinc.com/olms/data/resource/8585/Week%205_Transfer%20of%20learning.pdf "Transfer of learning after updating training mediated by the striatum"); [Holmes, Gathercole, Place, Dunning, Hilton & Elliott, 2009](https://web.archive.org/web/20121114220714/http://joniholmes.com/Papers/Holmes%20et%20al.%20ADHD%20training%202010%20Applied%20Cog%20Psych.pdf "Working Memory Deficits can be Overcome: Impacts  Training and Medication on Working Memory in Children with ADHD"); Thorell, Lindqvist, Bergman Nutley, Bohlin & Klingberg, 2009). Thus, it is still unclear under which conditions effects of WM training transfer to Gf.
>
> Other intervention studies have included training of attention or executive functions. Rueda and colleagues trained attention in a sample of 4- and 6-year-olds and found significant gains in intelligence (as measured with the Kaufman Brief Intelligence Test) in the 4-year-olds but only a tendency in the group of 6-year-olds ([Rueda, Rothbart, McCandliss, Saccomanno & Posner, 2005](http://www.sacklerinstitute.org/cornell/people/bruce.mccandliss/publications/publications/Rueda.etal.2005.pdf)). A large training study with 11,430 participants revealed practically no transfer after a 6-week intervention (10 min ⁄ day, 3 days a week) of a broader range of tasks including reasoning and planning or memory, visuo-spatial skills, mathematics and attention ([Owen, Hampshire, Grahn, Stenton, Dajani, Burns, Howard & Ballard, 2010](http://web.me.com/adrian.owen/site/IQ_files/OwenNaturefinal.pdf)). However, this study lacked control in sample selection and compliance. In summary, it is still an open question to what extent Gf can be improved by targeted training.

Working memory training including variants on dual n-back has been shown to physically change/increase the distribution of white matter in the brain^[["Training of Working Memory Impacts Structural Connectivity"](http://www.jneurosci.org/cgi/content/abstract/30/9/3297), Takeuchi 2010 in _[Journal of Neuroscience](!Wikipedia)_.]

Physical changes have been linked to WM training and n-backing. For example, Olesen PJ, Westerberg H, Klingberg T (2004) Increased prefrontal and parietal activity after training of working memory. Nat Neuroscience 7:75-79; about this study, Kuriyama writes:

> "Olesen et al. (2004) presented progressive evidence obtained by functional magnetic resonance imaging that repetitive training improves spatial WM performance [both accuracy and response time (RT)] associated with increased cortical activity in the middle frontal gyrus and the superior and inferior parietal cortices. Such a finding suggests that training-induced improvement in WM performance could be based on neural plasticity, similar to that for other skill-learning characteristics."

Westerberg 2007, ["Changes in cortical activity after training of working memory--a single-subject analysis"](/docs/dnb/2007-westerberg.pdf):

> "...Practice on the WM tasks gradually improved performance and this effect lasted several months. The effect of practice also generalized to improve performance on a non-trained WM task and a reasoning task. After training, WM-related brain activity was significantly increased in the middle and inferior frontal gyrus. The changes in activity were not due to activations of any additional area that was not activated before training. Instead, the changes could best be described by small increases in the extent of the area of activated cortex. The effect of training of WM is thus in several respects similar to the changes in the functional map observed in primate studies of skill learning, although the physiological effect in WM training is located in the prefrontal association cortex."

- ["Training and transfer effects of executive functions in preschool children"](http://www.klingberglab.se/pub/Thorell2008.pdf):

> Executive functions, including working memory and inhibition, are of central importance to much of human behavior. Interventions intended to improve executive functions might therefore serve an important purpose. Previous studies show that working memory can be improved by training, but it is unknown if this also holds for inhibition, and whether it is possible to train executive functions in preschoolers. In the present study, preschool children received computerized training of either visuo-spatial working memory or inhibition for 5 weeks. An active control group played commercially available computer games, and a passive control group took part in only pre- and posttesting. Children trained on working memory improved significantly on trained tasks; they showed training effects on non-trained tests of spatial and verbal working memory, as well as transfer effects to attention. Children trained on inhibition showed a significant improvement over time on two out of three trained task paradigms, but no significant improvements relative to the control groups on tasks measuring working memory or attention. In neither of the two interventions were there effects on non-trained inhibitory tasks. The results suggest that working memory training can have significant effects also among preschool children. The finding that inhibition could not be improved by either one of the two training programs might be due to the particular training program used in the present study or possibly indicate that executive functions differ in how easily they can be improved by training, which in turn might relate to differences in their underlying psychological and neural processes.

- ["Training, maturation, and genetic influences on the development of executive attention"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1253585/), Rueda et al 2005:

> A neural network underlying attentional control involves the anterior cingulate in addition to lateral prefrontal areas. An important development of this network occurs between 3 and 7 years of age. We have examined the efficiency of attentional networks across age and after 5 days of attention training (experimental group) compared with different types of no training (control groups) in 4-year-old and 6-year-old children. Strong improvement in executive attention and intelligence was found from ages 4 to 6 years. Both 4- and 6-year-olds showed more mature performance after the training than did the control groups. This finding applies to behavioral scores of the executive attention network as measured by the attention network test, event-related potentials recorded from the scalp during attention network test performance, and intelligence test scores. We also documented the role of the temperamental factor of effortful control and the DAT1 gene in individual differences in attention. Overall, our data suggest that the executive attention network appears to develop under strong genetic control, but that it is subject to educational interventions during development.

- ["Common and unique components of inhibition and working memory: An fMRI, within-subjects investigation"](http://www.klingberglab.se/pub/McNab2008.pdf)

> Behavioural findings indicate that the core executive functions of inhibition and working memory are closely linked, and neuroimaging studies indicate overlap between their neural correlates. There has not, however, been a comprehensive study, including several inhibition tasks and several working memory tasks, performed by the same subjects. In the present study, 11 healthy adult subjects completed separate blocks of 3 inhibition tasks (a stop task, a go/no-go task and a flanker task), and 2 working memory tasks (one spatial and one verbal). Activation common to all 5 tasks was identified in the right inferior frontal gyrus, and, at a lower threshold, also the right middle frontal gyrus and right parietal regions (BA 40 and BA 7). Left inferior frontal regions of interest (ROIs) showed a significant conjunction between all tasks except the flanker task. The present study could not pinpoint the specific function of each common region, but the parietal region identified here has previously been consistently related to working memory storage and the right inferior frontal gyrus has been associated with inhibition in both lesion and imaging studies. These results support the notion that inhibitory and working memory tasks involve common neural components, which may provide a neural basis for the interrelationship between the two systems.

- Huijbers et al 2009 ["When Learning and Remembering Compete: A Functional MRI Study"](http://dx.doi.org/10.1371/journal.pbio.1000011)

> Recent functional neuroimaging evidence suggests a bottleneck between learning new information and remembering old information. In two behavioral experiments and one functional MRI (fMRI) experiment, we tested the hypothesis that learning and remembering compete when both processes happen within a brief period of time. In the first behavioral experiment, participants intentionally remembered old words displayed in the foreground, while incidentally learning new scenes displayed in the background. In line with a memory competition, we found that remembering old information was associated with impaired learning of new information. We replicated this finding in a subsequent fMRI experiment, which showed that this behavioral effect was coupled with a suppression of learning-related activity in visual and medial temporal areas. Moreover, the fMRI experiment provided evidence that left mid-ventrolateral prefrontal cortex is involved in resolving the memory competition, possibly by facilitating rapid switching between learning and remembering. Critically, a follow-up behavioral experiment in which the background scenes were replaced with a visual target detection task provided indications that the competition between learning and remembering was not merely due to attention. This study not only provides novel insight into our capacity to learn and remember, but also clarifies the neural mechanisms underlying flexible behavior.

(There's also a worthwhile blog article on this one: ["Training The Mind: Transfer Across Tasks Requiring Interference Resolution"](http://scienceblogs.com/developingintelligence/2008/10/03/training-the-mind-transfer-acr/).)

["How distractible are you? The answer may lie in your working memory capacity"](http://scienceblogs.com/cognitivedaily/2009/03/02/how-distractible-are-you-the-a/)

- Jennifer C. McVay, Michael J. Kane (2009). "Conducting the train of thought: Working memory capacity, goal neglect, and mind wandering in an executive-control task". Journal of Experimental Psychology: Learning, Memory, and Cognition, 35 (1), 196-204 DOI: [10.1037/a0014104](http://dx.doi.org/10.1037/a0014104):

> On the basis of the executive-attention theory of working memory capacity (WMC; e.g., M. J. Kane, A. R. A. Conway, D. Z. Hambrick, & R. W. Engle, 2007), the authors tested the relations among WMC, mind wandering, and goal neglect in a sustained attention to response task (SART; a go/no-go task). In 3 SART versions, making conceptual versus perceptual processing demands, subjects periodically indicated their thought content when probed following rare no-go targets. SART processing demands did not affect mind-wandering rates, but mind-wandering rates varied with WMC and predicted goal-neglect errors in the task; furthermore, mind-wandering rates partially mediated the WMC-SART relation, indicating that WMC-related differences in goal neglect were due, in part, to variation in the control of conscious thought.

- ["Working memory capacity and its relation to general intelligence"](http://www.psych.rutgers.edu/~jose/courses/578/Conway_etal_2003.pdf); Andrew R.A. Conway et al; _TRENDS in Cognitive Sciences_ Vol.7 No.12 December 2003

     > Several recent latent variable analyses suggest that (working memory capacity) accounts for at least one-third and perhaps as much as one-half of the variance in (intelligence).What seems to be important about WM span tasks is that they require the active maintenance of information in the face of concurrent processing and interference and therefore recruit an executive attention-control mechanism to combat interference. Furthermore, this ability seems to be mediated by portions of the prefrontal cortex.

## Support

### Jaeggi 2005

["Capacity Limitations in Human Cognition: Behavioural and Biological Contributions"](http://jtoomim.org/brain-training/jaeggi2005-thesis-humancapacity-and-limits.pdf); Jaeggi thesis:

> ...Experiment 6 and 7 finally tackle the issue, whether capacity limitations are trait-like, i.e., fixed, or whether it is be possible to extend these limitations with training and whether generalized effects on other domains can be observed. In the last section, all the findings are integrated and discussed, and further issues remaining to be investigated are pointed out.
>
> ...In this experiment [6], the effects of a 10-day training of an adaptive version of an n-back dual task were studied. The adaptive version should be very directly depending on the actual performance of the participant: Not being too easy, but also not too difficult; always providing a sense of achievement in the participant in order to keep the motivation high. Comparing pre and post measures, effects on the task itself were evaluated, but also effects on other WM measures, and on a measure of fluid intelligence.
>
> ...As stated before, this study [7] was conducted in order to replicate and extend the findings of Experiment 6: I was primarily interested to see whether an asymptotic curve regarding performance would be reached after nearly twice of the training sessions used in Experiment 6, and further, whether generalized and differential effects on various cognitive tasks could be obtained with this training. Therefore, more tasks were included compared to Experiment 6, covering many aspects of WM (i.e., verbal tasks, visuospatial tasks), executive functions, as well as control tasks not used in Experiment 6 in order to investigate whether the WM training has a selective effect on tasks which are related to the concept of WM and executive functions with no effect on these control tasks. With respect to fluid intelligence, a more appropriate task than the APM, i.e., the 'Bochumer Matrizentest' (BOMAT; Hossiep, Turck, & Hasella, 1999) was used, which has the advantage that full parallel-versions are available and that the task was explicitly developed in order not to yield ceiling effects in student samples. The experiment was carried out together with Martin Buschkuehl and Daniela Blaser; the latter writing her Master thesis on the topic.

#### Jaeggi 2008

["Improving fluid intelligence with training on working memory"](/docs/dnb/2008-jaeggi.pdf), Jaeggi et al 2008 (all the data in Jaeggi 2005 was used in this as well); this article was widely covered (eg. [Science Daily](!Wikipedia)'s ["Brain-Training To Improve Memory Boosts Fluid Intelligence"](http://www.sciencedaily.com/releases/2008/05/080505075642.htm) or [_Wired_](!Wikipedia "Wired (magazine)")'s ["Forget _Brain Age_: Researchers Develop Software That Makes You Smarter"](http://www.wired.com/science/discoveries/news/2008/04/smart_software)) and sparked most people's interest in the topic. The abstract:

> Fluid intelligence (_Gf_) refers to the ability to reason and to solve new problems independently of previously acquired knowledge. _Gf_  is critical for a wide variety of cognitive tasks, and it is considered one of the most important factors in learning. Moreover, _Gf_ is closely related to professional and educational success, especially in complex and demanding environments. Although performance on tests of _Gf_ can be improved through direct practice on the tests themselves, there is no evidence that training on any other regimen yields increased _Gf_ in adults. Furthermore, there is a long history of research into cognitive training showing that, although performance on trained tasks can increase dramatically, transfer of this learning to other tasks remains poor. Here, we present evidence for transfer from training on a demanding working memory task to measures of _Gf_. This transfer results even though the trained task is entirely different from the intelligence test itself. Furthermore, we demonstrate that the extent of gain in intelligence critically depends on the amount of training: the more training, the more improvement in _Gf_. That is, the training effect is dosage-dependent. Thus, in contrast to many previous studies, we conclude that it is possible to improve Gf without practicing the testing tasks themselves, opening a wide range of applications.

Brain Workshop includes a special 'Jaeggi mode' which replicates almost exactly the settings described for the "Brain Twister" software used in the study.

No study is definitive, of course, but Jaeggi 2008 is still one of the major studies that must be cited in any DNB discussion. There are some issues - not as many subjects as one would like, and the researchers (quoted in the _Wired_ article) obviously don't know if the WM or _Gf_ gains are durable; more technical issues like the administered _Gf_ IQ tests being speeded and thus reduced in validity have been raised by [Moody](#moody-2009-re-jaeggi-2008).

### Qiu 2009

["Study on Improving Fluid Intelligence through Cognitive Training System Based on Gabor Stimulus"](/docs/dnb/2009-qiu.pdf), 2009 First International Conference on Information Science and Engineering, abstract:

> General fluid intelligence (_Gf_) is a human ability to reason and solve new problems independently of previously acquired knowledge and experience. It is considered one of the most important factors in learning. One of the issues which academic people concentrates on is whether _Gf_ of adults can be improved. According to the Dual N-back working memory theory and the characteristics of visual perceptual learning, this paper put forward cognitive training pattern based on Gabor stimuli. A total of 20 undergraduate students at 24 years old participated in the experiment, with ten training sessions for ten days. Through using Raven's Standard Progressive Matrices as the evaluation method to get and analyze the experimental results, it was proved that training pattern can improve fluid intelligence of adults. This will promote a wide range of applications in the field of adult intellectual education.

Discussion and criticism of this Chinese[^china] paper took place in [2](http://groups.google.com/group/brain-training/browse_frm/thread/7321c36dca3dcfad/3eb3ac6bd64498f0) [threads](http://groups.google.com/group/brain-training/browse_frm/thread/80613920c0fbd422/3e7310cc84628fea); the SPM was administer in 25 minutes, which while not as fast as Jaeggi 2008, is still not the normal length. An additional anomaly is that according to the final graph, the control group's IQ dropped massively in the post-test (driving much of the improvement). As part of my [meta-analysis](DNB meta-analysis), I tried to contact the 4 authors in May, June, July & September 2012; they eventually replied with data.

[^china]: _[New Humanist](!Wikipedia)_, ["Lies, Damn Lies, and Chinese Science: The People's Republic is becoming a technological superpower, but who's checking the facts? Sam Geall seeks out the Chinese science cops"](http://newhumanist.org.uk/2365/lies-damn-lies-and-chinese-science) (see also [_Lancet_](http://news.bbc.co.uk/2/hi/8448731.stm), [_Nature_](/docs/dnb/2010-zhang.pdf "'Chinese journal finds 31% of submissions plagiarized', Zhang 2010"), [_NYT_](http://www.nytimes.com/2010/10/07/world/asia/07fraud.html)):

    > This publish-or-perish culture has led to unrealistic targets at Chinese universities - and as a predictable consequence, [rampant](http://news.bbc.co.uk/2/hi/asia-pacific/4755861.stm) [plagiarism](http://news.bbc.co.uk/2/hi/asia-pacific/8442147.stm). In January, the peer-reviewed international journal _[Acta Crystallographica](!Wikipedia) Section E_ [announced](http://www.nature.com/news/2010/100112/full/463142a.html "Publish or perish in China: The pressure to rack up publications in high-impact journals could encourage misconduct, some say") the retraction of more than 70 papers by Chinese scientists who had falsified data. Three months later, the same publication announced the removal of another 39 articles "as a result of problems with the data sets or incorrect atom assignments", 37 of which were entirely produced in Chinese universities. The New Jersey-based Centenary College closed its affiliated Chinese business school programme in July after a review "revealed evidence of widespread plagiarism, among other issues, at a level that ordinarily would have resulted in students' immediate dismissal from the college." A government study, cited by _Nature_, found that about one-third of over 6,000 scientists surveyed at six top Chinese institutions had practised "plagiarism, falsification or fabrication". But it's not only the emphasis on quantity that damages scientific quality in China. Publication bias - the tendency to privilege the results of studies that show a significant finding, rather than inconclusive results - is notoriously pervasive. One systematic review of acupuncture studies from 1998, published in Controlled Clinical Trials, found that every single clinical trial originating in China was positive - in other words, no trial published in China had found a treatment to be ineffective.

    _[Science News](!Wikipedia)_, ["Traditional Chinese medicine: Big questions: Journal reports of benefits often lack methodological rigor or details"](https://www.sciencenews.org/view/generic/id/330930/title/Traditional_Chinese_medicine_Big_questions):

    > [Their new paper](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0020185) focuses exclusively on reports published since 1999 in Chinese academic journals, roughly half of which were specialty publications. Clinicians authored half of the papers. Almost 85% of the reports focused on herbal remedies - anything from bulk herbs or pills to "decoctions". Most of the remaining reviews assessed the value of acupuncture, although about 1% of the reports dealt with Tuina massage...The papers were reviews, or what are typically referred to in Western journals as meta-analyses...Many of the papers were incomplete, roughly one-third contained statistical errors and others provided data or comparisons that the authors termed misleading. Fewer than half of the surveyed papers described how the data they were presenting had been collected, how those data had been analyzed or how a decision had been made about which studies to compare. The majority of papers also did not assess the risk of bias across studies or offer any information on potential conflict-of-interest factors (such as who funded or otherwise offered support for the research being reviewed)....Overall, "the quality of these reviews is troubling," the Lanzhou researchers conclude in the May 25 _PLoS One_.

    ["Plagiarism Plague Hinders China's Scientific Ambition"](http://www.npr.org/2011/08/03/138937778/plagiarism-plague-hinders-chinas-scientific-ambition), NPR:

    > In 2008, when her scientific publication, the _Journal of Zhejiang University-Science_, became the first in China to use CrossCheck text analysis software to spot plagiarism, Zhang was pleased to be a trailblazer. But when the first set of results came in, she was upset and horrified. "In almost 2 years, we find about 31% of papers with unreasonable copy[ing] and plagiarism," she says, shaking her head. "This is true." For computer science and life science papers, that figure went up to almost 40 percent...Despite the outpouring of Chinese papers, Chinese research isn't that influential globally. Thomson Reuters' Science Watch website notes that China isn't even in the top 20 when measuring the number of times a paper is cited on a national basis. ScienceNet's Zhao says he fears Chinese research is still about quantity rather than quality....However, China's leaders have committed to fighting scientific fraud. And Zhang, the journal editor, says that one year on, plagiarism at her publication has fallen noticeably, to 24% of all submissions.

    ["China's academic scandal: call toll-free hotlines to get your name published"](http://www.wired.co.uk/news/archive/2013-12/02/china-academic-scandal); ["Looks good on paper: A flawed system for judging research is leading to academic fraud"](http://www.economist.com/news/china/21586845-flawed-system-judging-research-leading-academic-fraud-looks-good-paper)

### polar (June 2009)

A group member, polar, conducted a small experiment at his university where he was a student; [his results](http://groups.google.com/group/brain-training/browse_thread/thread/4b0697d5619d873b/7b91f868a3f7f979) seemed to show an [improvement](https://spreadsheets.google.com/ccc?key=rOfijmsJ-hxPbzmbi4dmHVg). As polar would be the first to admit, the attrition in subjects (few to begin with), relatively short time of training and whatnot make the power of his study weak.

### Jaeggi 2010

["The relationship between n-back performance and matrix reasoning - implications for training and transfer"](/docs/dnb/2010-jaeggi.pdf), Jaeggi et al (coded as `Jaeggi2` in meta-analysis); abstract:

> ...In the first study, we demonstrated that dual and single n-back task performances are approximately equally correlated with performance on two different tasks measuring _Gf_, whereas the correlation with a task assessing working memory capacity was smaller. Based on these results, the second study was aimed on testing the hypothesis that training on a single n-back task yields the same improvement in _Gf_ as training on a dual n-back task, but that there should be less transfer to working memory capacity. We trained two groups of students for four weeks with either a single or a dual n-back intervention. We investigated transfer effects on working memory capacity and _Gf_ comparing the two training groups' performance to controls who received no training of any kind. Our results showed that both training groups improved more on Gf than controls, thereby replicating and extending our prior results.

The 2 studies measured _Gf_ using Raven's APM and the BOMAT. In both studies, the tests were administered speeded to 10 or 15 minutes as in Jaeggi 2008. The experimental groups saw average gains of 1 or 2 additional correct answers on the BOMAT and APM. It's worth noting that the Single N-Back was done with a *visual* modality (and the DNB with the standard visual & audio).

Followup work:

- [Schneiders et al 2012](http://www.frontiersin.org/Human_Neuroscience/10.3389/fnhum.2012.00173/full "The Impact of Auditory Working Memory Training on the Fronto-Parietal Working Memory Network") trained audio WM and found no transfer to visual WM tasks; unfortunately, they did not measure any far transfer tasks like RAPM/BOMAT.
- [Beavon 2012](http://ro.ecu.edu.au/cgi/viewcontent.cgi?article=1025&context=spsyc_pres "Improving Memory Through N-back Training") reports _n_=47, experimentals trained on single n-back & controls on "combined verbal tasks Definetime and Who wants to be a millionaire (Millionaire)"; no improvements on "STM span and attention, short term auditory memory span and divided attention, and WM as operationalised through the Woodcock-Johnson III: Tests of cognitive abilities (WJ-III)".

#### Studer-Luethi 2012

The second study's data was reused for a Big Five personality factor analysis in Studer-Luethi, Jaeggi, et al 2012, ["Influence of neuroticism and conscientiousness on working memory training outcome"](/docs/dnb/2012-studerluethi.pdf).[^Studer]

[^Studer]: Abstract:

    > We investigated whether and how individual differences in personality determine cognitive training outcomes. 47 participants were either trained on a single or on a dual _n_-back task for a period of 4 weeks. 52 additional participants did not receive any training and served as a no-contact control group. We assessed neuroticism and conscientiousness as personality traits as well as performance in near and far transfer measures. The results indicated a significant interaction of neuroticism and intervention in terms of training efficacy. Whereas dual _n_-back training was more effective for participants low in neuroticism, single _n_-back training was more effective for participants high in neuroticism. Conscientiousness was associated with high training scores in the single _n_-back and improvement in near transfer measures, but lower far transfer performance, suggesting that subjects scoring high in this trait developed task-specific skills preventing generalizing effects. We conclude by proposing that individual differences in personality should be considered in future cognitive intervention studies to optimize the efficacy of training.

The lack of n-back score correlation with WM score seems in line with an earlier study; ["Working Memory, Attention Control, and the N-Back Task: A Question of Construct Validity"](https://www.princeton.edu/~aconway/pdf/Kane2007nback.pdf "Kane et al 2007"):

> ...Participants also completed a verbal WM span task (operation span task) and a marker test of general fluid intelligence (Gf; Ravens Advanced Progressive Matrices Test; J. C. Raven, J. E. Raven, & J. H. Court, 1998). N-back and WM span correlated weakly, suggesting they do not reflect primarily a single construct; moreover, both accounted for independent variance in Gf. N-back has face validity as a WM task, but it does not demonstrate convergent validity with at least 1 established WM measure.

### Stephenson 2010

["Does training to increase working memory capacity improve fluid intelligence?"](http://jtoomim.org/files/stephenson_2010.pdf):

> The current study was successful in replicating Jaeggi et al.'s (2008) results. However, the current study also observed improvements in scores on the Raven's Advanced Progressive Matrices for participants who completed a variation of the dual n-back task or a short-term memory task training program. Participants' scores improved significantly for only two of the four tests of GJ, which raises the issue of whether the tests measure the construct _Gf_ exclusively, as defined by Cattell (1963), or whether they may be sensitive to other factors. The concern is whether the training is actually improving _Gf_ or if the training is improving attentional control and/or visuospatial skills, which improves performance on specific tests of _Gf_. The findings are discussed in terms of implications for conceptualizing and assessing _Gf_.

136 participants split over 25-28 subjects in experimental groups and the control group. Visual n-back improved more than audio n-back; the control group was a passive control group (they did nothing but served as controls for test-retest effects).

#### Stephenson & Halpern 2013

["Improved matrix reasoning is limited to training on tasks with a visuospatial component"](/docs/dnb/2013-stephenson.pdf), Stephenson & Halpern 2013:

 > Recent studies (e.g., Jaeggi et al., 2008, 2010) have provided evidence that scores on tests of fluid intelligence can be improved by having participants complete a four week training program using the dual n-back task. The dual n-back task is a working memory task that presents auditory and visual stimuli simultaneously. The primary goal of our study was to determine whether a visuospatial component is required in the training program for participants to experience gains in tests of fluid intelligence. We had participants complete variations of the dual n-back task or a short-term memory task as training. Participants were assessed with four tests of fluid intelligence and four cognitive tests. We were successful in corroborating Jaeggi et al.'s results, however, improvements in scores were observed on only two out of four tests of fluid intelligence for participants who completed the dual n-back task, the visual n-back task, or a short-term memory task training program. Our results raise the issue of whether the tests measure the construct of fluid intelligence exclusively, or whether they may be sensitive to other factors. The findings are discussed in terms of implications for conceptualizing and assessing fluid intelligence...The data in the current paper was part of Clayton Stephenson's doctoral dissertation.

### Jaeggi 2011

Jaeggi, Buschkuehl, Jonides & Shah 2011 ["Short- and long-term benefits of cognitive training"](http://www.pnas.org/content/108/25/10081.full) (coded as `Jaeggi3` in the meta-analysis); the abstract:

> We trained elementary and middle school children by means of a videogame-like working memory task. We found that only children who considerably improved on the training task showed a performance increase on untrained fluid intelligence tasks. This improvement was larger than the improvement of a control group who trained on a knowledge-based task that did not engage working memory; further, this differential pattern remained intact even after a 3-mo hiatus from training. We conclude that cognitive training can be effective and long-lasting, but that there are limiting factors that must be considered to evaluate the effects of this training, one of which is individual differences in training performance. We propose that future research should not investigate whether cognitive training works, but rather should determine what training regimens and what training conditions result in the best transfer effects, investigate the underlying neural and cognitive mechanisms, and finally, investigate for whom cognitive training is most useful.

It is worth noting that the study used Single N-back (visual). Unlike Jaeggi 2008, "despite the experimental group's clear training effect, we observed no significant group × test session interaction on transfer to the measures of Gf" (so perhaps the training was long enough for subjects to hit their ceilings). The group which did n-back could be split, based on final IQ & n-back scores, into 2 groups; interestingly "Inspection of n-back training performance revealed that there were no group differences in the first 3 wk of training; thus, it seems that group differences emerge more clearly over time [first 3 wk: t(30) < 1; P = ns; last week: t(16) = 3.00; P < 0.01] (Fig. 3)." 3 weeks is ~21 days, or >19 days (the longest period in Jaeggi 2008). It's also worth noting that Jaeggi 2011 seems to avoid Moody's most cogent criticism, the speeding of the IQ tests; from the paper's "Material and Methods" section;

> We assessed matrix reasoning with two different tasks, the Test of Nonverbal Intelligence (TONI) (23) and Raven's Standard Progressive Matrices (SPM) (24). Parallel versions were used for the pre, post-, and follow-up test sessions in counterbalanced order. For the TONI, we used the standard procedure (45 items, five practice items; untimed), whereas for the SPM, we used a shortened version (split into odd and even items; 29 items per version; two practice items; timed to 10 min after completion of the practice items. Note that virtually all of the children completed this task within the given timeframe).

The IQ results were, specifically, the control group averaged 15.33/16.20 (before/after) correct answers on the SPM and 20.87/22.50 on the TONI; the n-back group averaged 15.44/16.94 SPM and 20.41/22.03 TONI. 1.5 more right questions rather than ~1 may not seem like much, but the split groups look quite different - the 'small training gain' n-backing group actually fell on its second SPM and improved by <0.2 questions on the TONI, while the 'large training gain' increased >3 questions on the SPM and TONI. The difference is not so dramatic in the followup 3 months later: the small group is now 17.43/23.43 (SPM/TONI), and the large group 15.67/24.67. Strangely in the followup, the control group has a higher SPM than the large group (but not the small group), and a higher TONI than either group; the control group has higher IQ scores on *both* TONI & SPM in the followup than the aggregate n-back group. (The splitting of groups is also unorthodox[^graehl].)

[^graehl]: Research programmer [Jonathan Graehl](http://jonathan.graehl.org/) [writes on](http://lesswrong.com/lw/68k/nback_news_jaeggi_2011_or_is_there_a/4d34) the [LW discussion](http://lesswrong.com/lw/68k/nback_news_jaeggi_2011_or_is_there_a/?sort=top#comments) of Jaeggi 2011:

    > ...If you separated the "active control" group into high and low improvers post-hoc just like was done for the n-back group, you might see that the active control "high improvers" are even smarter than the n-back "high improvers". We should expect some 8-9 year olds to improve in intelligence or motivation over the course of a month or two, without any intervention. Basically, this result sucks, because of the artificial post-hoc division into high- and low- responders to n-back training, needed to show a strong "effect". I'm not certain that the effect is artificial; I'd have to spend a lot of time doing some kind of sampling to show how well the data is explained by my alternative hypothesis.

UoM produced [a video](http://www.youtube.com/watch?v=-sPOgbz_gq4) with Jonides; Jaeggi 2011 has also been discussed in mainstream media. From the _Wall Street Journal_'s ["Boot Camp for Boosting IQ"](http://online.wsj.com/article/SB10001424052702304432304576371462612272884.html):

> ...when several dozen elementary- and middle-school kids from the Detroit area used this exercise for 15 minutes a day, many showed significant gains on a widely used intelligence test. Most impressive, perhaps, is that these gains persisted for three months, even though the children had stopped training...these schoolchildren showed gains in fluid intelligence roughly equal to five IQ points after one month of training...There are two important caveats to this research. The first is that not every kid showed such dramatic improvements after training. Initial evidence suggests that children who failed to increase their fluid intelligence found the exercise too difficult or boring and thus didn't fully engage with the training.

From _Discover_'s blogs, ["Can intelligence be boosted by a simple task? For some…"](http://blogs.discovermagazine.com/notrocketscience/2011/06/13/can-intelligence-be-boosted-by-a-simple-task-for-some/), come additional details:

> She [Jaeggi] recruited 62 children, aged between seven and ten. While half of them simply learned some basic general knowledge questions, the other half trained with a cheerful computerised n-back task. They saw a stream of images where a target object appeared in one of six locations - say, a frog in a lily pond. They had to press a button if the frog was in the same place as it was two images ago, forcing them to store a continuously updated stream of images in their minds. If the children got better at the task, this gap increased so they had to keep more images in their heads. If they struggled, the gap was shortened.
>
> Before and after the training sessions, all the children did two reasoning tests designed to measure their fluid intelligence. At first, the results looked disappointing. On average, the n-back children didn't become any better at these tests than their peers who studied the knowledge questions. But according to Jaeggi, that's because some of them didn't take to the training. When she divided the children according to how much they improved at the n-back task, she saw that those who showed the most progress also improved in fluid intelligence. The others did not. Best of all, these benefits lasted for 3 months after the training. That's a first for this type of study, although Jaeggi herself says that the effect is "not robust." Over this time period, all the children showed improvements in their fluid intelligence, "probably [as] a result of the natural course of development".
>
> ...Philip Ackerman, who studies learning and brain training at the University of Illinois, says, "I am concerned about the small sample, especially after splitting the groups on the basis of their performance improvements." He has a point - the group that showed big improvements in the n-back training only included 18 children....Why did some of the children benefit from the training while others did not? Perhaps they were simply uninterested in the task, no matter how colourfully it was dressed up with storks and vampires. In Jaeggi's earlier study with adults, every volunteer signed up themselves and were "intrinsically motivated to participate and train." By contrast, the kids in this latest study were signed up by their parents and teachers, and some might only have continued because they were told to do so.
>
> It's also possible that the changing difficulty of the game was frustrating for some of the children. Jaeggi says, "The children who did not benefit from the training found the working memory intervention too effortful and difficult, were easily frustrated, and became disengaged. This makes sense when you think of physical training - if you don't try and really run and just walk instead, you won't improve your cardiovascular fitness." Indeed, a recent study on IQ testing which found that [they reflect motivation as well as intelligence](http://blogs.discovermagazine.com/notrocketscience/2011/04/26/iq-scores-reflect-motivation-as-well-as-intelligence/).

### Schweizer et al 2011

["Extending Brain-Training to the Affective Domain: Increasing Cognitive and Affective Executive Control through Emotional Working Memory Training"](http://www.plosone.org/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0024372&representation=PDF):

> This study investigated whether brain-training (working memory [WM] training) improves cognitive functions beyond the training task (transfer effects), especially regarding the control of emotional material since it constitutes much of the information we process daily. Forty-five participants received WM training using either emotional or neutral material, or an undemanding control task. WM training, regardless of training material, led to transfer gains on another WM task and in fluid intelligence. However, only brain-training with emotional material yielded transferable gains to improved control over affective information on an emotional Stroop task. The data support the reality of transferable benefits of demanding WM training and suggest that transferable gains across to affective contexts require training with material congruent to those contexts. These findings constitute preliminary evidence that intensive cognitively demanding brain-training can improve not only our abstract problem-solving capacity, but also ameliorate cognitive control processes (e.g. decision-making) in our daily emotive environments.

Notes:

1. There seems to be an IQ increase of around one question on the RPM (but there's an oddity with the control group which they think they correct for[^affectiveIQ])
2. The RPM does not seem to have been administered speeded[^affectiveSpeeded]
3. The emotional aspect seems to be just replacing the 'neutral' existing stimuli like colors or letters or piano keys with more loaded ones[^affectiveImplementation], nor does this tweak seem to change the DNB/WM/IQ scores of that group[^affectiveDifference]

[^affectiveIQ]: The DNB groups gain ~1 point (question), and the control group falls ~2 points after starting off ~2 points higher. In other words, if the control group had not fallen so much, the DNB groups would at no point have scored higher!

    > Replicating their results, we found a significant gain in Gf scores in the training group over and above gains on the digit span task F(1, 26) = 3.00, P = 0.05, ηp2 = 0.10. In contrast, the control group showed a non-significant decrease in Gf, F<1, and the critical group by time interaction was significant, F(1, 40) = 7.47, P = 0.01, ηp2 = 0.16. As can be seen in Figure 3, there was a trend toward a significant group difference in Gf (RPM scores) at pre-training, p≤0.10. This raises the possibility that the relative gains in Gf in the training versus control groups may be to some extent an artefact of baseline differences. However, the interactive effect of transfer as a function of group remained significant even after more closely matching the training and control groups for pre-training RPM scores (by removing the highest scoring controls) F(1, 30) = 3.66, P = 0.032, ηp2 = 0.10. The adjusted means (standard deviations) for the control and training groups were now 27.20 (1.93), 26.63 (2.60) at pre-training (t(43) = 1.29, P>0.05) and 26.50 (4.50), 27.07 (2.16) at post-training, respectively. Moreover, there was a trend for the gain in Gf to be positively correlated with improvements in n-back performance across training r(29) = 0.36 at P = 0.057, suggesting that such gains were indeed a function of training....Although the Gf transferable gains we found appear to be somewhat related to training gains and the effects remain when we trim the groups to provide a better match for pre-training Gf, it is important to note that some degree of regression to the mean may be influencing the results.
[^affectiveSpeeded]: At least, they seem to administer the whole thing with no mention of such a variation:

    > We assessed Gf with the Raven's Progressive Matrices (RPM; [[35]](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0024372#pone.0024372-Raven1)) - a standard measure in the literature. Each RPM item presented participants with a matrix of visual patterns with one pattern missing. The participant chose how the matrix should be completed by selecting a pattern from a series of alternatives. We used parallel versions of the RPM (even and uneven numbered pages), which we counterbalanced across participants and pre- and post-training. The RPM is scored on a scale from 0-30, with each correct matrix earning participants one point.
[^affectiveImplementation]: From the paper:

     > The figure depicts a block of the emotional version of the dual n-back task (training task) where n = 1. The top row shows the sequence across trials (A, B, C, D, etc.) of visually presented stimuli in a 4×4 grid (the visual stimuli were presented on a standard 1280×1024 pixel computer display). A picture of a face appeared in one of the 16 possible grid positions on each trial. Simultaneously, with the presentation of these visual stimuli on the computer display, participants heard words over headphones (second row in the figure). Participants were required to indicate, by button press, whether the trial was a 'target trial' or not. Targets could be visual or auditory. In the example here, Trial C is a visual target. That is, the face in Trial C is presented in the same location as the face in Trial B (i.e., n = 1 positions back). Note, the faces are of different actors. For visual stimuli participants were asked to ignore the content of the image and solely attend to the location in which the images were presented. In the current example, Trial D was an auditory target trial because 'Evil' is the same word as the word presented in Trial C - n positions back (where n = 1). Each block consisted of 20+n trials.

    (If you look at Figure 1, example stimuli words are 'dead', 'hate', 'evil', 'rape', 'slum', and a picture of a very angry male face.)
[^affectiveDifference]: The difference doesn't seem to change progress on n-back in either group, which is good since if there were differences, that would be troubling eg. if the affective n-back group didn't increase as many levels, that would make any following results more dubious:

    > Performance of the two n-back groups pre- to post- training did not differ significantly on either the neutral F(1, 27) = 1.02, P>0.05 or affective F (1, 27)<1 n-back tasks. Similarly, the control group showed a significantly greater pre- to post-training improvement on the feature match task they trained on, compared with the n-back groups F(1, 42) = 41.09, P<0.001, ηp2 = 0.67.

    And as one would hope, both DNB groups increased their WM scores:

    > As predicted, participants in the training group showed a significant improvement on digit span F(1, 28) = 33.96, _p_<0.001, ηp2 = 0.55. However, this was not true of controls F(1, 15) = 1.89, _p_=0.19, ηp2 = 0.11, and the gain was significantly greater in the training group participants compared to controls F(1,43) = 5.92, _p_=0.02, ηp2 = 0.12.

Their later study ["Training the Emotional Brain: Improving Affective Control through Emotional Working Memory Training"](http://www.researchgate.net/publication/236068356_Training_the_Emotional_Brain_Improving_Affective_Control_through_Emotional_Working_Memory_Training/file/9c960514c67603791d.pdf) did not use any measure of fluid intelligence.

### Kundu et al 2011

["Relating individual differences in short-term memory-derived EEG to cognitive training effects"](http://psych.wisc.edu/postlab/posters/Kundu_CNS2011.pdf) (coded as `Kundu1` in the meta-analysis); 3 controls (Tetris), 3 experimentals (Brain Workshop) for 1000 minutes. RAPM showed a slight increase. Extremely small experimental size, which may form part of the data for [Kundu et al 2012](#kundu-et-al-2012).

### Zhong 2011

["The Effect Of Training Working Memory And Attention On Pupils' Fluid Intelligence"](http://www.doc88.com/p-397166703921.html) ([abstract](http://www.globethesis.com/?t=2155330335456903)), Zhong 2011; [original encrypted file](/docs/dnb/2012-zhong.ebt) (8M), [screenshots of all pages in thesis](/docs/dnb/2011-zhong.tar) (20M); [discussion](https://groups.google.com/d/topic/brain-training/V_msD2vUjy4/discussion)

Appears to have found IQ gains, but no dose-response effect, using a no-contact control group. Difficult to understand: translation assistance from Chinese speakers would be appreciated.

### Jaušovec 2012

["Working memory training: Improving intelligence - Changing brain activity"](http://jtoomim.org/files/Jau%9aovec%20and%20Jau%9aovec%20-%202012%20-%20Working%20memory%20training%20Improving%20intelligence.pdf):

> The main objectives of the study were: to investigate whether training on working memory (WM) could improve fluid intelligence, and to investigate the effects WM training had on neuroelectric (electroencephalography - EEG) and hemodynamic (near-infrared spectroscopy - NIRS) patterns of brain activity. In a parallel group experimental design, respondents of the working memory group after 30 h of training significantly increased performance on all tests of fluid intelligence. By contrast, respondents of the active control group (participating in a 30-h communication training course) showed no improvements in performance. The influence of WM training on patterns of neuroelectric brain activity was most pronounced in the theta and alpha bands. Theta and lower-1 alpha band synchronization was accompanied by increased lower-2 and upper alpha desynchronization. The hemodynamic patterns of brain activity after the training changed from higher right hemispheric activation to a balanced activity of both frontal areas. The neuroelectric as well as hemodynamic patterns of brain activity suggest that the training influenced WM maintenance functions as well as processes directed by the central executive. The changes in upper alpha band desynchronization could further indicate that processes related to long term memory were also influenced.

14 experimental & 15 controls; the testing was a little unusual:

> Respondents solved four test-batteries, for which the procedure was the same during pre- and post-testing. The same test-batteries were used on pre- and post-testing. The digit span subtest (WAIS-R) was administered separately, according to the directions in the test manual (Wechsler, 1981). The other three tests (RAPM, verbal analogies and spatial rotation) were administered while the respondents' EEG and NIRS measures were recorded.
>
> The RAPM was based on a modified version of Raven's progressive matrices (Raven, 1990), a widely used and well established test of fluid intelligence (Sternberg, Ferrari, Clinkenbeard, & Grigorenko, 1996). The correlation between this modified version of RAPM and WAIS-R was r = .56, (p < .05, n = 97). Similar correlations of the order of 0.40-0.75, were also reported for the standard version of RAPM (Court & Raven, 1995). Therefore it can be concluded that the modified application of the RAPM did not significantly alter its metric characteristics. Used were 50 test items - 25 easy (Advanced Progressive Matrices Set I - 12 items and the B Set of the Colored Progressive Matrices), and 25 difficult items (Advanced Progressive Matrices Set II, items 12-36). Participants saw a figural matrix with the lower right entry missing. They had to determine which of the four options fitted into the missing space. The tasks were presented on a computer screen (positioned about 80-100 cm in front of the respondent), at fixed 10 or 14 s interstimulus intervals. They were exposed for 6 s (easy) or 10 s (difficult) following a 2-s interval, when a cross was presented. During this time the participants were instructed to press a button on a response pad (1-4) which indicated their answer.

At 25 hard questions, and <14s a question, that implies the RAPM was administered in <5.8 minutes. They comment:

> To further investigate possible influences of task difficulty on the observed performance gains on the RAPM a GLM for repeated measures test/retest Â easy/difficult-items Â group (WM, AC) was conducted. The analysis showed only a significant interaction effect for the test/retest condition and type of training used in the two groups (F(1, 27) = 4.47; p < .05; partial eta2 = .15). A GLM conducted for the WM group showed only a significant test/retest effect (F(1, 13) = 30.11; p < .05; partial eta2 = .70), but no interaction between the test/retest conditions and the difficulty level (F(1, 13) = 1.79; p = .17 not-significant; partial eta2 = .12). As can be seen in Fig. 4 after WM training an about equal increase in respondents' performance for the easy and difficult test items was observed. On the other hand, no increases in performance, neither for the easy nor for the difficult test items, in respondents of the active control group were observed (F(1, 14) = .47; p = .50 not- significant; partial eta2 = .03).

(Even on the "easy" questions, no group performed better than 76% accuracy.)

### Clouter 2013

["The Effects of Dual n-back Training on the Components of Working Memory and Fluid Intelligence: An Individual Differences Approach"](http://dalspace.library.dal.ca/bitstream/handle/10222/36238/Clouter-Andrew-MSc-PSYO-August-2013.pdf?sequence=1), Clouter 2013:

> A number of recent studies have provided evidence that training working memory can lead to improvements in fluid intelligence, working memory span, and performance on other untrained tasks. However, in addition to a number of mixed results, many of these studies suffer from design limitations. The aim of the present study was to experimentally investigate the effects of a dual n-back working memory training task on a variety of measures of fluid intelligence, reasoning, working memory span, and attentional control. The present study compared a training group with an active control group (a placebo group), using appropriate methods that overcame the limitations of previous studies. The dual n-back training group improved more than the active control group on some, but not all outcome measures. Differential improvement for the training group was observed on fluid intelligence, working memory capacity, and response times on conflict trials in the Stroop task. In addition, individual differences in pre-training fluid intelligence scores and initial performance on the training task explain some of the variance in outcome measure improvements. We discuss these results in the context of previous studies, and suggest that additional work is needed in order to further understand the variables responsible for transfer from training.

<!--
dual n-back
active control
Cattell's Culture Fair Intelligence Test (CFIT)
n=36
"Of these, 18 participants were in the active control group (mean number of training sessions completed = 15.2, sd = 0.55, min = 14, max = 16.3), and 18 in the training group (mean number of training sessions completed = 14.3, sd = 1.61, min = 10, max = 15.1)."
CFIT: mean, 95% CI
control 28.83 (27.23-30.45); mean distance between CI extremes & mean((28.83-27.23) + (30.45-28.83))/2 = 1.61; for a 95% CI, the extreme lies 1.96 SDs away, so the SD = 1.61/1.96 = 0.821
training 30.94 (29.37-32.51); likewise: (((30.94-29.37) + (32.51-30.94))/2)/1.96 = 0.801

2013 Clouter 18 28.83 0.821 18 30.94 0.801 1 605 3 13 0 1
guesswork: standard deviations of raw CFIT scores; total training time; total CFIT administration time; amount paid

--

Participants were granted a choice between course credit and a small stipend, or a combination, for participating in the study.

The training tasks were modified from the free open-source n-back package Brain Workshop (version 4.8.4), available at http://brainworkshop.sourceforge.net. The program was modified so that the program window was 912 x 684 pixels. Participants used their laptop keyboards to respond to audio and location matches throughout the training sessions. The computer recorded all responses, and participants brought their laptop to the post-training session so that the experimenters could obtain the training data.

The pen-and-paper test is composed of four timed subtests. The time allowed for each subtest varies between 2.5 and 4 minutes. Participants stopped working on each subtest immediately when time expired, and participants were not permitted to return to work on previous subtests at any time.

Both the training group and the active control group completed 20 blocks of trials during each training session. All participants were instructed to complete five training sessions each week over three weeks, for 15 total training sessions.

For the training group, each daily session began with an n level of one. Participants were presented with 20 + n2 trials in each of the 20 blocks per training session (e.g., 21 trials for n = 1)  Each trial began with a 2.5-second delay, followed by the simultaneous presentation of a letter spoken aloud through the computer speakers or headphones and a blue square appearing in one of the eight matrix locations. The location stimuli remained for 0.5 seconds, and each trial was followed by a 2.5- second inter-trial interval, during which participants made responses.

[15 training sessions of 20 blocks of ~22 trials; trial = 2.5s + 0.5s + 2.5s = 5.5s; (15*20*22*5.5)/60 = 605 minutes]

Three weeks after the pre-training session, participants returned for the post-training session in which participants were asked to record their expectations of whether they thought that the training would have any impact on their outcome measure performance, and then the same outcome measures were administered following the same procedure. Participants were debriefed and thanked upon completion.

Point estimates and confidence intervals for each effect of interest were obtained by a posteriori bootstrapping (100,000 iterations) from generalised linear mixed models (GLMMs; Pinheiro and Bates, 2000). GLMMs require fewer assumptions than analysis of variance, while providing greater statistical power and the possibility of multiple random effects. In the present analysis "participant" is defined as a random effect, as interparticipant variability is not an effect of interest. GLMMs account for the interparticipant variance, enhancing the power of the models. Prior to the computation of the bootstrapped confidence intervals, the estimates for the variance of the grand mean of each effect and its covariance with the other effects of interest were zeroed. This approach yields confidence intervals for cells in repeated-measures designs that are inferentially useful for within- and between-groups comparisons (cf. Masson and Loftus, 2003).
While the point estimates and confidence intervals provided evidence of changes in performance on the pre-training and post-training sessions, analysis of variance (ANOVA) were used to test for the presence of training group x session interactions. ANOVAs were used to test for training group x session and training group x split group x session interactions in the individual differences analysis. Analysis was coded in R (Version 3.0.0; R Development Core Team, 2013) using the functions in the ez package (https://github.com/mike-lawrence/ez) for analysis and the ggplot2 package (Wickham, 2009, 2012).

Results from significance testing to compare pre-training and post-training performance are provided in Table 4. Of the five CFIT measures, point estimates and 95% confidence intervals suggest improvements in the outcome measures for two of the measures for the active control group, and four measures for the training group. As shown in Table 3 and Figure 12, the training group improved on the CFIT classifications subtest, PE = 1.94, 95% CI = [0.99, 2.90], the CFIT matrices subtest, PE = 0.56, 95% CI = [0.09, 1.02], the CFIT conditions/topology subtest, PE = 1.56, 95% CI = [1.04, 2.07], and the overall CFIT score, PE = 4.33, 95% CI = [2.93, 5.74]. The active control group also improved on the CFIT conditions/topology subtest, PE = 1.00, 95% CI = [0.60, 1.41], and the overall CFIT score, PE = 2.00, 95% CI = [0.25, 3.73] (Table 3).

In the pre-training session, after being instructed on how to use the training program (after the administration of the outcome measures), participants in both groups completed a short questionnaire in order to gauge their expectations of whether the training task would affect their performance on the outcome measures. The same questionnaire was repeated in the post-training session, after the training had been completed, and prior to the administration of the outcome measure tasks. While the inclusion of an active control task might have reduced the likelihood of positive results occurring due to Hawthorne or placebo-like effects, these measures were included in order to measure the expectations and beliefs of whether the training task would improve performance on the training tasks. Participants were asked, "On a scale from 1 to 5, where 1 is "not at all" and 5 is "very much", where 3 is "unsure", do you think that the training will improve your performance on (each task)?"
Table 11 shows the PEs and 95% CIs for the expectations for each task for the active control group and the training group, for the questionnaires given in both the pre-training and post-training sessions. The only difference between the groups was for the symmetry span task, where participants in the training group expected their performance on the task to improve by more than participants in the active control group, F(1, 31) = 16.39, p = 0.003.
In some, but not all, cases, the expectations of participants matched the actual improvements. For the CFIT, neither the training group nor the active control group thought that their scores would improve, since the 95% CI for both groups included 3 (unsure). However, both groups improved their scores on the CFIT significantly. Based on the 95% CIs, both groups expected improved performance on the OSPAN and symmetry span tasks, which turned out to be the case. Contrary to the group difference in expectations, the training group did not improve by more than the active control group on the symmetry span task. Interestingly, both groups did not expect improvement on the MHP, which also turned out to be the case (in the post-training session, the 95% CI for the MHP for the training group included 3, but only just; the training group x session interaction was not significant). Finally, both groups expected improvement on the Stroop task. In reality, both groups improved on Stroop RTs, but only the training group improved on RTs without increasing errors (in the pre-training session, the 95% CI for the Stroop task includes 3 for the training group, but only just; the training group x session interaction was not significant).

As far as we are aware, this is the first study to examine performance on subtests of fluid intelligence. While both Borella, Carretti, Riboldi, and De Beni (2010) and Redick et al. (2012) included the CFIT as an outcome measure in their respective training studies, neither have reported results for the individual subtests. Given that the active control group improved on only the Conditions/Topology subtest, and the training group did as well, the improvement in the overall CFIT score for the active control group is largely driven by improvement on this subtest (in fact, 50% of the improvement on the overall CFIT score for the active control group resulted from improvement on the Conditions/Topology subtest). Therefore, it may be that the n-back task in general, even a non-adaptive version, may train WM processes that are common to performance on the Conditions/Topology subtest. Alternatively, the Conditions/Topology subtest may be more susceptible to test-retest improvements than the other subtests. However, even though we presented results for individual CFIT subtests, we do not argue for the legitimacy of scores on the individual subtests as a measure of fluid intelligence itself, much as we (and others) have argued against the use of matrix-only fluid intelligence tests.

Based on these results, it appears that participants with lower CFIT scores are likely to show improvements on the CFIT whether they undertake the dual n-back training or the active control training. Participants with higher CFIT scores show improvements on the CFIT only if they undertook the dual n-back training. These individual differences may explain some of the divergent results from a number of other training studies, and suggest that initial fluid intelligence scores may be an important variable that affects transfer from training.

[Mean reversion?]

Jaeggi et al. (2011) split the training group based on performance improvements between the first and last training sessions, and reported that the children in their study with the highest improvements in the n-level on the training task showed more transfer to other outcome measures than the children that improved the least on the training task, suggesting that improvement on the training task may be critical in determining whether transfer to untrained tasks occurs. However, Redick et al. (2012) performed a similar analysis on a more varied group of healthy young adults, using the same dual n-back training task, and did not reach the same conclusion. In the present study, we failed to show a relation between improvement in the n-back training task and transfer to untrained tasks. Rather, individual differences in the n-back intercept explained some transfer effects.

[As expected of the post hoc split; failure to replicate is what happens to data dredging.]
-->

### Jaeggi et al 2013

["The role of individual differences in cognitive training and transfer"](/docs/dnb/2013-jaeggi.pdf):

> Working memory (WM) training has recently become a topic of intense interest and controversy. Although several recent studies have reported near- and far-transfer effects as a result of training WM-related skills, others have failed to show far transfer, suggesting that generalization effects are elusive. Also, many of the earlier intervention attempts have been criticized on methodological grounds. The present study resolves some of the methodological limitations of previous studies and also considers individual differences as potential explanations for the differing transfer effects across studies. We recruited intrinsically motivated participants and assessed their need for cognition (NFC; Cacioppo & Petty Journal of Personality and Social Psychology 42:116-131, 1982) and their implicit theories of intelligence (Dweck, 1999) prior to training. We assessed the efficacy of two  interventions by comparing participants' improvements on a battery of fluid intelligence tests against those of an active control group. We observed that transfer to a composite measure of fluid reasoning resulted from both WM interventions. In addition, we uncovered factors that contributed to training success, including motivation, need for cognition, preexisting ability, and implicit theories about intelligence.

This is quite a complex study, with a lot of analysis I don't think I entirely understand. The quick summary is table 2 on pg10: the DNB group fell on APM, rose on BOMAT (neither statistically-significant); the SNB group increased on APM & BOMAT (but only BOMAT was statistically-significant).

Michael J. Kane has written [some critical comments on the results](http://blogs.scientificamerican.com/beautiful-minds/2013/10/09/new-cognitive-training-study-takes-on-the-critics/#comment-357).

<!--
> We randomly assigned participants to one of two WM interventions or to an active control group. The two WM interventions were similar to ones used by us previously (Jaeggi et al., 2008; Jaeggi et al., 2011a; Jaeggi, StuderLuethi, et al., 2010). Both interventions were versions of an adaptive n -back task in which participants were asked to indicate whether a stimulus was the same as the one presented n-items previously. If participants succeeded at a particular level of n , the task was made incrementally more difficult by increasing the size of n. One WM intervention was a single auditory n-back task (i.e., using spoken letters as stimuli); the other was a dual n-back task in which an auditory n-back task was combined with a visuospatial task; that is, spoken letters and spatial locations were presented and had to be processed simultaneously. The control task, which we termed the "knowledge-training task," required participants to answer vocabulary, science, social science, and trivia questions presented in a multiple-choice format (cf. Anguera et al., 2012, Exp. 2, as well as Jaeggi et al., 2011a). This control task was adaptive, in that new items replaced material that was successfully learned. Participants found the control task to be engaging and enjoyable. In that it tapped crystallized knowledge, it served as an effective and plausible training condition that did not engage fluid intelligence or WM. The auditory single n -back task was selected because our previous studies had always included a visual training task, and we chose to assess whether a nonvisuospatial n-back intervention would lead to improvements in visuospatial reasoning tasks.

> Our second goal was to evaluate the effects of motivation on training and transfer. Our previous research with children had provided evidence that motivation may play a substantial role in the effectiveness of training (Jaeggi et al., 2011a). In addition, our research with young adults provided preliminary evidence that motivational factors mediate training outcomes. We compared the training outcomes across several studies with young adults conducted by our research team, and found that transfer effects to measures of Gf were found only when participants either were not paid at all to participate (Jaeggi et al., 2008) or were paid a very modest amount (i.e., $20; Jaeggi, Studer-Luethi, et al., 2010; see also Stephenson & Halpern, 2013). In contrast, in one study that we conducted, participants were paid a substantial fee for participation (i.e., $150; Anguera et al., 2012, Exp. 2), and we found no fartransfer effects on measures of Gf,1 although near transfer did occur to measures of WM (Anguera et al., 2012; see also Kundu, Sutterer, Emrich, & Postle, 2013).2 Three other research groups that used our training paradigm paid participants~$130, $352, or about $800, and interestingly, they did not find transfer on any of their outcome measures (Chooi & Thompson, 2012; Redick et al., 2013; Thompson, Waskom, Garel, Cardenas-Iniguez, Reynolds, Winter and Gabrieli, 2013). The motivational literature has repeatedly demonstrated that extrinsic rewards such as monetary incentives can severely undermine intrinsic motivation (Deci, Koestner, & Ryan, 1999) and, ultimately, performance (Burton, Lydon, D'Alessandro, & Koestner, 2006). Consistent with this notion, the training curves of the paid studies that did not find far transfer are indeed considerably shallower than those of the earlier, successful studies: Whereas the training gains in the paid studies were only between 1.6 and 1.8 n -back levels (Chooi & Thompson, 2012; Redick et al., 2013; Seidler, Bernard, Buschkuehl, Jaeggi, Jonides, & Humfleet, 2010), the gains on our successful studies were 2.3 and 2.6 n-back levels, respectively (Jaeggi et al., 2008; Jaeggi, Studer-Luethi, et al., 2010). Thompson and colleagues claim that their training gains were similar to those we observed in our 2008 study, but note that their participants trained roughly twice as long as our participants had, and consequentially, this comparison is not entirely appropriate (Thompson et al., 2013). On the basis of this observation, for the present study we recruited participants for a four-week "Brain Training Study" without payment. By not paying participants, we expected the participants to be intrinsically motivated, and consequently, we hoped to increase the likelihood of training and transfer.

> Note that Gf data are not reported in Anguera et al. (2012, Exp. 2) because that article was primarily addressing issues of WM and motor function, building on the findings of Experiment 1. However, the Gf data are available as a technical report (Seidler et al., 2010).

[What bullshit. Seidler et al 2010 reports no such thing, and Jaeggi has been made extremely aware of this by me.]

> A group of 175 participants from the University of Michigan and the Ann Arbor community took part in the present study (mean age = 24.12 years, SD = 6.02, range = 18-45; 99 women, 76 men). They volunteered to participate in a study advertised as a "Brain Training Study" and did not receive payment or course credit. They were recruited via flyers and various online resources, such as Facebook. Fifty-four participants (31 %) withdrew from the study after having completed one or two pretest sessions and having trained for no more than three sessions, largely because of time constraints; most of them (N = 36) never trained at all. Forty-three participants (25 %) dropped out at some point during the training period and/or failed to complete the posttest, after having trained for 11.58 sessions on average (SD = 5.45, range = 4-20).3 Note that the dropout rates did not differ among groups [χ 2(2) = 2.62; see Table 1]. The final group of participants, who completed pre- and posttesting and a minimum of 17 training sessions, consisted of 78 individuals (mean age = 25.21 years, SD = 6.46, range = 18-45; 36 women, 42 men). We randomly assigned participants to one of the three groups until we had a sample of 12 participants. All subsequent participants were assigned to a training group, so that the three groups would remain as similar as possible on the following variables: gender, age, and pretest performance on the APM and the CFT, which were assessed in the first pretest session (cf. Jaeggi et al., 2011a). In addition to the participants recruited for the "brain training" study, 34 participants (mean age = 22.79 years, SD = 6.11, range = 18-44; 17 women, 17 men) were recruited via flyers to take part in the baseline measurement sessions only, and they were paid at an hourly rate of $15.

So no one in the training groups were paid anything.

The IQ tests:

1. RAPM
2. Cattell's Culture Fair Test  / CFT
3. BOMAT

> Regardless of condition, each training session lasted approximately 20-30 min...They were instructed to train once a day, five times per week for a total of 20 sessions.

25 * 20 = 500 minutes total

> In contrast to our previous studies (Jaeggi et al., 2008; Jaeggi, Studer-Luethi, et al., 2010), we administered the test without any time restrictions.

RAPM was unspeeded.

RAPM:
DNB group post-scores:
n=25, mean=14.96, SD=2.7

SNB
n=26, mean=15.23, SD=2.44

Active control
n=27, mean=14.74, SD=2.80

> Despite the promise of WM training, the research supporting its effectiveness is not yet conclusive. In particular, the fartransfer effects found in some studies are controversial. First, several studies have reported null effects of training (see, e.g., Craik, Winocur, Palmer, Binns, Edwards, Bridges and Stuss, 2007; Owen, Hampshire, Grahn, Stenton, Dajani, Burns and Ballard, 2010; Zinke, Zeintl, Eschen, Herzog, & Kliegel, 2011), and even studies that have used the same training regimen have sometimes found transfer, and sometimes not (Anguera, Bernard, Jaeggi, Buschkuehl, Benson, Jennett, & Seidler, 2012; Bergman Nutley et al., 2011; Holmes et al., 2009; Jaeggi, Buschkuehl, et al., 2010; Klingberg et al., 2005; Redick, Shipstead, Harrison, Hicks, Fried, Hambrick and Engle, 2013; Thorell et al., 2009). One explanation for these inconsistent results across studies may be individual differences in age, personality or preexisting abilities that limit the effectiveness of training for some individuals (Chein & Morrison, 2010; Shah, Buschkuehl, Jaeggi, & Jonides, 2012; Zinke et al., 2011; Zinke, Zeintl, Rose, Putzmann, Pydde, & Kliegel, 2013; Studer-Luethi, Jaeggi, Buschkuehl, & Perrig, 2012). It is also possible that motivational conditions in a particular study (e.g., the degree to which participants are intrinsically vs. extrinsically motivated to participate) influence the effectiveness of training (Anguera et al., 2012; Jaeggi et al., 2011a). Finally, other experimental conditions-such as training time, experimenter supervision of the training process, group versus single-subject settings, quality of instructions, or feedback-may also have an impact on training outcomes (cf. Basak, Boot, Voss, & Kramer, 2008; Jaeggi et al., 2008; Tomic & Klauer, 1996; Verhaeghen, Marcoen, & Goossens, 1992).
> In addition to inconsistent transfer effects across studies, some of the research that has reported evidence of far transfer as a result of WM training has been criticized for methodological flaws and/or potential for alternative explanations of the transfer effects. For example, some studies have not included an active control group, yielding the possibility that the transfer found in those studies may be attributable to a Hawthorne effect (Mayo, 1933) or to placebo effects more generally. Other studies have included an active control group, but the nature of the control task has been criticized as being less demanding, engaging, or believable as an intervention than the task experienced by the WM group. Studies have also been criticized for the use of just one or very few far-transfer tasks, rather than using multiple tasks to represent a cognitive construct such as fluid intelligence. Furthermore, some studies have not reported improvements on near-transfer tasks, making it difficult to assess what the underlying mechanisms of improvement might be and leaving open the possibility of placebo-type factors (cf. Buschkuehl & Jaeggi, 2010; Morrison & Chein, 2011; Rabipour & Raz, 2012; Shipstead, Redick, & Engle, 2012, for further discussions). Finally, it is still unresolved whether transfer effects last beyond the training period, and if so, for how long. Only a handful of studies have tested the long-term effects of training by retesting both the experimental and control groups some time after training completion (Borella et al., 2010; Buschkuehl et al., 2008; Carretti et al., 2013; Jaeggi et al., 2011a; Klingberg et al., 2005; Van der Molen, Van Luit, Van der Molen, Klugkist, & Jongmans, 2010). Indeed, some evidence for long-term effects could be attributed to training, but other effects, such as transfer effects that are only present at a long-term follow-up but not at the posttest ("sleeper effects"), are difficult to interpret (Holmes et al., 2009; Van der Molen et al., 2010).
-->

### Savage 2013

["Near and Far Transfer of Working Memory Training Related Gains in Healthy Adults"](http://theses.ucalgary.ca/bitstream/11023/1123/6/ucalgary_2013_savage_linette.pdf), Savage 2013

> Enhancing intelligence through working memory training is an attractive concept, particularly for middle-aged adults. However, investigations of working memory training benefits are limited to younger or older adults, and results are inconsistent. This study investigates working memory training in middle age-range adults. Fifty healthy adults, aged 30-60, completed measures of working memory, processing speed, and fluid intelligence before and after a 5-week web-based working memory (experimental) or processing speed (active control) training program. Baseline intelligence and personality were measured as potential individual characteristics associated with change. Improved performance on working memory and processing speed tasks were experienced by both groups; however, only the working memory training group improved in fluid intelligence. Agreeableness emerged as a personality factor associated with working memory training related change. Albeit limited by power, findings suggest that dual n-back working memory training not only enhances working memory but also fluid intelligence in middle-aged healthy adults.

The personality correlations seem to differ with Studer-Luethi.

### Stepankova et al 2013

"The Malleability of Working Memory and Visuospatial Skills: A Randomized Controlled Study in Older Adults", Stepankova et al 2013: <!-- TODO: get my own copy, can't put up Redick's copy -->

> There is accumulating evidence that training on working memory (WM) generalizes to other nontrained domains, and there are reports of transfer effects extending as far as to measures of fluid intelligence. Although there have been several demonstrations of such transfer effects in young adults and children, they have been difficult to demonstrate in older adults. In this study, we investigated the generalizing effects of an adaptive WM intervention on nontrained measures of WM and visuospatial skills. We randomly assigned healthy older adults to train on a verbal n-back task over the course of a month for either 10 or 20 sessions. Their performance change was compared with that of a control group. Our results revealed reliable group effects in nontrained standard clinical measures of WM and visuospatial skills in that both training groups outperformed the control group. We also observed a dose-response effect, that is, a positive relationship between training frequency and the gain in visuospatial skills; this finding was further confirmed by a positive correlation between training improvement and transfer. The improvements in visuospatial skills emerged even though the intervention was restricted to the verbal domain. Our work has important implications in that our data provide further evidence for plasticity of cognitive functions in old age.

<!--
> The foremost goal of the current study was to examine the efficacy of an adaptive computer-based WM intervention in healthy, community-dwelling older adults. To that end, we trained two groups of participants on a single n-back task for either 10 or 20 sessions over the course of 5 weeks. We compared the training groups’ performance in nontrained measures of WM and visuospatial skills before and after training with the performance of a no-contact control group. Four standardized and widely used clinical measures of WM and visuospatial skills were administered and subsequently combined into a composite score for each domain in order to increase measurement quality (Conway et al., 2005; Kane, et al., 2004).

> We used a pretest/posttest randomized controlled trial design with two experimental groups and a control group (CG). All participants were tested twice in an interval of 5 weeks (baseline and posttest). The two experimental groups underwent a computer-based WM intervention between the two assessments, which differed in terms of training frequency, in that participants were asked to train either twice or four times per week distributed over the course of 5 weeks. The low frequency group (Ex10) trained twice a week for a total of 7–12 sessions (mean: 9.42 sessions; SD ϭ 0.96; 200 –250 min total), whereas the high frequency group (Ex20) trained four times a week for a total of 18 –23 sessions (mean: 19.60 sessions; SD ϭ 0.60; 450 –500 min total). The CG did not train and was appointed to control for test–retest effects. The study was approved by the Ethics Committee of Prague Psychiatric Center (ref. no. 122/09), and informed written consent was obtained from all participants.

> Each training session consisted of 20 blocks, each lasting for about 1 min and consisting of 20 + n stimuli. There were six targets per block that were presented at random positions in the sequence. Each session took about 25 min to complete.

250 minutes of training for the 10-session group, 500 for the 20-session group.

> All participants received a nominal payment of 500 CZK after the second assessment (i.e., about 29 USD), regardless of condition or performance.

> The second target construct, visuospatial skills, was operationalized by two tasks—Block Design (BD) and Matrix Reasoning (MR). Both are nonverbal subtests of the WAIS–III (Wechsler, 1997b). Both tests are often used as proxy for Gf (e.g., Bugg et al., 2006; Friedman et al., 2006), and they are reported to have high g loadings (Colom, Jung, & Haier, 2006; Roivainen, 2010). In the MR test, participants are presented with a display of geometric shapes or patterns, with one shape or pattern missing. They are required to either name or point to the correct answer alternative to complete the pattern out of five response options. The test is thought to require visual information processing, abstract reasoning skills, learning ability, and mental flexibility (Groth-Marnat, 2009, p. 156). The BD test requires participants to organize colored blocks to match pictures on cards and employs visual perception of abstract designs, spatial processing, visuomotor coordination, and processing speed (e.g. Groth-Marnat, 2009; Kaufman, 2006, p. 401). Whereas there was no time limit on MR, BD was timed to a maximum of 21 min as defined in the handbook of the WAIS–III (Wechsler, 1997b). The dependent measure was determined according to the handbook of the WAIS-III (Wechsler, 1997a). In the MR, it corresponded to the number of correctly solved items, which was also true for the BD, but in addition, the scoring depended on the amount of time needed to solve each item. Note that since there are no parallel-test versions of our transfer measures, the same task versions were used in the pre- and posttest sessions.

The MR sounds closest to the RAPM I've been preferring. No time limit.

> We used a verbal version of the n-back task as an intervention task, which was based on other n-back interventions used before (cf. Jaeggi et al., 2008, 2010a, 2011). Participants were presented with a sequence of large yellow capital letters presented in the middle of a screen with a blue background. Participants had to indicate whether the currently presented letter was the same as the one presented n positions back in the sequence by pressing the spacebar on the keyboard (no responses were required for nontargets).

So, single n-back (visual modality).

Test score data in table 3, pg7. Post-test:

Experimental, 10 sessions (n=20):
Matrix Reasoning: 20.25 (3.77)
Block Design: 46.2 (8.84)

Experimental, 20 sessions (n=20):
Matrix Reasoning: 21.1 (2.95)
Block Design: 44.9 (8.77)

Passive controls (n=25):
Matrix Reasoning: 17.04 (5.02)
Block Design: 39.84 (9.52)
-->

## Criticism

### Moody 2009 (re: Jaeggi 2008)

[Jaeggi 2008](#jaeggi-2008), you may remember, showed that training on N-back improved working memory, but it also boosted scores on tests of _Gf_. The latter would be a major result - indeed, unique - and is one of the main research results encouraging people to do N-back in a non-research setting. People *want* to believe that N-back is efficacious and particularly that it will do more than boost working memory. So we need to be wary of [confirmation bias](!Wikipedia) (for those of you who read too much fantasy, you'll know this as [Wizard's First Rule](!Wikipedia)).

Fortunately, we can [discuss](http://groups.google.com/group/brain-training/browse_thread/thread/5c7247e00fe9bca9) at length the work of one David E. Moody who has [published](http://dx.doi.org/10.1016/j.intell.2009.04.005) a criticism of how the odd methodology of Jaeggi 2008 undermines this result. He's worth quoting at length, since besides being important to understanding Jaeggi's study, it's an interesting example of how subtle issues can be important in psychology:

> "The subjects were divided into four groups, differing in the number of days of training they received on the task of working memory. The group that received the least training (8 days) was tested on Raven's Advanced Progressive Matrices (Raven, 1990), a widely used and well-established test of fluid intelligence. This group, however, demonstrated negligible improvement between pre- and post-test performance.
>
> The other three groups were not tested using Raven's Matrices, but rather on an alternative test of much more recent origin. The Bochumer Matrices Test (BOMAT) (Hossiep, Turck, & Hasella, 1999) is similar to Raven's in that it consists of visual analogies. In both tests, a series of geometric and other figures is presented in a matrix format and the subject is required to infer a pattern in order to predict the next figure in the series. The authors provide no reason for switching from Raven's to the BOMAT.
>
> The BOMAT differs from Raven's in some important respects, but is similar in one crucial attribute: both tests are progressive in nature, which means that test items are sequentially arranged in order of increasing difficulty. A high score on the test, therefore, is predicated on subjects' ability to solve the more difficult items.
>
> However, this progressive feature of the test was effectively eliminated by the manner in which Jaeggi et al. administered it. The BOMAT is a 29-item test which subjects are supposed to be allowed 45 min to complete. Remarkably, however, Jaeggi et al. reduced the allotted time from 45 min to 10. The effect of this restriction was to make it impossible for subjects to proceed to the more difficult items on the test. The large majority of the subjects-regardless of the number of days of training they received-answered less than 14 test items correctly.\
> By virtue of the manner in which they administered the BOMAT, Jaeggi et al. transformed it from a test of fluid intelligence into a speed test of ability to solve the easier visual analogies.
> The time restriction not only made it impossible for subjects to proceed to the more difficult items, it also limited the opportunity to learn about the test-and so improve performance-in the process of taking it. This factor cannot be neglected because test performance does improve with practice, as demonstrated by the control groups in the Jaeggi study, whose improvement from pre- to post-test was about half that of the experimental groups. The same learning process that occurs from one administration of the test to the next may also operate within a given administration of the test-provided subjects are allowed sufficient time to complete it.
>
> Since the whole weight of their conclusion rests upon the validity of their measure of fluid intelligence, one might assume the authors would present a careful defense of the manner in which they administered the BOMAT. Instead they do not even mention that subjects are normally allowed 45 min to complete the test. Nor do they mention that the test has 29 items, of which most of their subjects completed less than half.
>
> The authors' entire rationale for reducing the allotted time to 10 min is confined to a footnote. That footnote reads as follows:
>
> > Although this procedure differs from the standardized procedure, there is evidence that this timed procedure has little influence on relative standing in these tests, in that the correlation of speeded and non-speeded versions is very high (r = 0.95; ref. 37).
>
> The reference given in the footnote is to a 1988 study (Frearson & Eysenck, 1986) that is not in fact designed to support the conclusion stated by Jaeggi et al. The 1988 study merely contains a footnote of its own, which refers in turn to unpublished research conducted forty years earlier. That research involved Raven's matrices, not the BOMAT, and entailed a reduction in time of at most 50%, not more than 75%, as in the Jaeggi study.
>
> So instead of offering a reasoned defense of their procedure, Jaeggi et al. provide merely a footnote which refers in turn to a footnote in another study. The second footnote describes unpublished results, evidently recalled by memory over a span of 40 years, involving a different test and a much less severe reduction in time.
>
> In this context it bears repeating that the group that was tested on Raven's matrices (with presumably the same time restriction) showed virtually no improvement in test performance, in spite of eight days' training on working memory. Performance gains only appeared for the groups administered the BOMAT. But the BOMAT differs in one important respect from Raven's. Raven's matrices are presented in a 3 × 3 format, whereas the BOMAT consists of a 5 × 3 matrix configuration.
>
> With 15 visual figures to keep track of in each test item instead of 9, the BOMAT puts added emphasis on subjects' ability to hold details of the figures in working memory, especially under the condition of a severe time constraint. Therefore it is not surprising that extensive training on a task of working memory would facilitate performance on the early and easiest BOMAT test items-those that present less of a challenge to fluid intelligence.
>
> This interpretation acquires added plausibility from the nature of one of the two working-memory tasks administered to the experimental groups. The authors maintain that those tasks were "entirely different" from the test of fluid intelligence. One of the tasks merits that description: it was a sequence of letters presented auditorily through headphones.
>
> But the other working-memory task involved recall of the location of a small square in one of several positions in a visual matrix pattern. It represents in simplified form precisely the kind of detail required to solve visual analogies. Rather than being "entirely different" from the test items on the BOMAT, this task seems well-designed to facilitate performance on that test."

Sternberg reviewed Jaeggi 2008

[Email](http://groups.google.com/group/brain-training/browse_thread/thread/8b88bc0b82527bcc) from Jaeggi to Pontus about the timelimit; visual problems [bias RPM against women](http://www.uam.es/personal_pdi/psicologia/fjabad/cv/articulos/paid/DIFRAVEN.pdf) who have a slightly lower average visuospatial performance.

[Nutley 2011](#nutley-2011) discusses why one test may be insufficient when an experimental intervention is done:

> Since the definition of _Gf_ itself stems from [factor analytical methods](!Wikipedia "Factor analysis"), using the shared variance of several tests to define the _Gf_ factor, a similar method should be used to measure gains in _Gf_. Another issue raised by [Sternberg (2008)](http://www.pnas.org/content/105/19/6791.full "Increasing fluid intelligence is possible after all") is that the use of only one single training task makes it difficult to infer if the training effect was due to some specific aspect of the task rather than the general effect of training a construct.

Shipstead, Redick, & Engle 2012 elaborate on how, while matrix-style IQ tests are considered gold standards, they are not perfect measures of IQ such that an increase in performance *must* reflect an increase in underlying intelligence:

> ...far transfer tasks are not perfect measures of ability. In many training studies, Raven's Progressive Matrices (Ravens; Raven, 1990, 1995, 1998) serves as the sole indicator of Gf. This "matrix reasoning" task presents test takers with a series of abstract pictures that are arranged in a grid. One piece of the grid is missing, and the test taker must choose an option (from among several) that completes the sequence. Jensen (1998) estimates that 64% of the variance in Ravens performance can be explained by Gf. Similarly, Figure 3 indicates that in the study of Kane et al. (2004), 58% of the Ravens variance was explained by Gf. It is clear that Ravens is strongly related to Gf. However, 30%-40% of the variance in Ravens is attributable to other influences. Thus, when Ravens (or any other task) serves as the sole indicator of far transfer, performance improvements can be explained without assuming that a general ability has changed. Instead, it can be parsimoniously concluded that training has influenced something that is specific to performing Ravens, but not necessarily applicable to other reasoning contexts (Carroll, 1993; Jensen, 1998; Moody, 2009; Schmiedek et al., 2010; te Nijenhuis, van Vianen, & van der Flier, 2007).
>
> ...Preemption of criticisms such as Moody's (2009) is, however, readily accomplished through demonstration of transfer to several measures of an ability. Unfortunately, the practice of equating posttest improvement on one task with change to cognitive abilities is prevalent within the WM training literature (cf. Jaeggi et al., 2008; Klingberg, 2010). This is partially driven by the time and monetary costs associated with conducting multisession, multiweek studies. Regardless, training studies can greatly improve the persuasiveness of their results by measuring transfer via several tasks that differ in peripheral aspects but converge on an ability of interest (e.g., a verbal, Gf, and spatial task from Figure 3). If a training effect is robust, it should be apparent in all tasks.

### Seidler 2010

["Cognitive Training As An Intervention To Improve Driving Ability In The Older Adult"](/docs/dnb/2010-seidler.pdf), a [technical report](!Wikipedia) by a group which includes Susanne Jaeggi, studied the effect of DNB on the driving ability of younger/older adults. As part of the before/after test battery, a Raven's was administered:

> Type 2 tests included Raven's matrices (Raven et al., 1990), which is a standardized test of fluid intelligence, and the BOMAT and verbal analogies tests of intelligence (Hossiep et al., 1995). We have previously shown that working memory training transfers to performance on this task (Jaeggi et al., 2008), and we included it here for the sake of replication.

They found the [null result](!Wikipedia):

> There were no significant group by test session interactions for the intelligence measures or complex motor tasks for the young adults, although one of the intelligence measures exhibited a trend for transfer effects that scaled with training task gains.
>
> ...Unlike in our previous work (Jaeggi et al., 2008) we did not observe transfer to measures of intelligence. This may have been a by-product of the rather extensive pre and post test battery of assessments that we performed, particularly given that one of the intelligence measures was always performed last in the sequence of tests. Given this, participants may have been too fatigued and / or unmotivated to perform these tests well.

<!--
Young and older adults were assigned to the cognitive training intervention group (young n=29, older adult n =18) and to a knowledge training (vocabulary and trivia) control group (young n = 27, older adult n=18).

Twenty-nine young adults (mean age 21.4 years, 15 women) and 18 older adults (11 have completed training and post testing thus far; mean age 68.4 years, 7 women) were assigned to the dual task working memory training intervention group. Twenty-seven young adults (mean age 20.2 years, 12 women) and 18 older adults (13 have completed training and post testing thus far; mean age 70.4 years, 9 women) were assigned to the knowledge training control group.

paid

Working memory training sessions comprised 20 blocks of 20 + n trials, resulting in approximately 20 - 25 minutes of training per day. Knowledge training sessions were determined by time, that is, participants trained 23 minutes per training session. Participants were asked to engage in one training session each day, five days per week, for five weeks, resulting in a total of 25 training sessions. All participants completed a minimum of 17 training sessions, with several completing up to the full 25.

Type 2 tests included Raven's matrices (Raven et al., 1990), which is a standardized test of fluid intelligence, and the BOMAT and verbal analogies tests of intelligence (Hossiep et al., 1995). We have previously shown that working memory training transfers to performance on this task (Jaeggi et al., 2008), and we included it here for the sake of replication. Participants also performed a motor sequence learning task and a sensorimotor adapation test as part of the type 2 test items. We have previously shown that measures of working memory correlate with the rate of learning both types of tasks (Anguera et al., in press, Bo & Seidler, 2009), and that aging is associated with declines in both types of skill learning (Seidler 2006). The administration of type 2 tests allows us to determine whether working memory training benefits are restricted to measures of working memory and related cognitive tasks, or if they transfer to tasks which purportedly rely on working memory mechanisms as well.
- Raven, JC, Court, JH, & Raven, J (1990). _Coloured progressive matrices_. Oxford, England: Oxford Psychologists Press.

There were no significant group by test session interactions for the intelligence measures or complex motor tasks for the young adults, although one of the intelligence measures exhibited a trend for transfer effects that scaled with training task gains. Again, there is insufficient data to statistically evaluate these transfer effects for older adults at this point in the study.
-->

### Jonasson 2011

["Investigating training and transfer in complex tasks with dual n-back"](http://his.diva-portal.org/smash/get/diva2:424076/FULLTEXT01), bachelor degree thesis:

> No clear consensus exists in the scientific community of what constitutes efficient dual-tasking abilities. Moreover, the training of executive components has been given increased attention in the literature in recent years. Investigating transferability of cognitive training in a complex task setting, thirty subjects practiced for five days on a Name-Tag task (controls) or a Dual N-Back task (experimental), subsequently being tested on two transfer tasks; the Automated Operation Span and a dual task (Trail Making task + Mathematical Addition task). Dual N-Back training previously transferred to unrelated intelligence tests and in this study is assumed to rely primarily on executive attention. Executive attention, functioning to resolve interference and maintaining task-relevant information in working memory, has previously been linked to fluid intelligence and to dual-tasking. However, no transfer effects were revealed. The length of training may have been too short to reveal any such effects. However, the three complex tasks correlated significantly, suggesting common resources, and therefore having potentials as transfer tasks. Notably, subjects with the highest task-specific improvements performed worse on the transfer tasks than subjects improving less, suggesting that task-specific gains do not directly correlate with any transfer effect. At present, if transfer exists in these settings, data implies that five days of training is insufficient for a transfer to occur. Important questions for future research relates to the necessary conditions for transfer to occur, such as the amount of training, neural correlates, attention, and motivation.

Caveats for this study:

1. It did not attempt to measure any form of _Gf_
2. It used 30 total subjects, or 15 in each group
3. Training was over 5-6 days, 16-20 minutes each day (although the DNB subjects did increase their scores), which may not be enough; although Jonasson comments (pg 44-45):

   > Nevertheless, training for five days or less has also led to significant improvements in performance on transfer tasks ([Damos & Wickens, 1980](/docs/dnb/1980-damos.pdf "The Identification and Transfer of Timesharing Skills"); [Kramer et al., 1995](/docs/dnb/1995-kramer.pdf "Training for attentional control in dual task settings: A comparison of young and old adults"); [Rueda et al., 2005](http://www.sacklerinstitute.org/cornell/people/bruce.mccandliss/publications/publications/Rueda.etal.2005.pdf)). However, the study by Kramer et al. (1995) may have transferred a strategy rather than training a specific component, and the study by Rueda et al. (2005) found transfer in children between ages four and six, the children possibly being more susceptible to training than adults.
4. Jonasson suggests that subjects were unmotivated, perhaps by the training being done at home on `Lumosity.com`; only one did the full 6 days of training, and incentives often increase performance on IQ and other tests.

### Chooi 2011

["Improving Intelligence by Increasing Working Memory Capacity"](http://etd.ohiolink.edu/send-pdf.cgi/Chooi%20Weng%20Tink.pdf?case1301710207&dl=y), PhD thesis:

> ...The current study aimed to replicate and extend the original study conducted by Jaeggi et al. (2008) in a well-controlled experiment that could explain the cause or causes of such transfer if indeed the case. There were a total of 93 participants who completed the study, and they were randomly assigned to one of three groups - a passive control group, active control group and experimental group. Half of the participants were randomly assigned to the 8-day condition and the other half to the 20-day condition. All participants completed a battery of tests at pre- and post-tests that consisted of short timed tests, a complex working memory span and a [untimed] matrix reasoning task. Participants in the active control group practiced for either 8 days or 20 days on the same task as the one used in the experimental group, the dual n-back, but at the easiest level to control for Hawthorne effect. Results from the current study did not suggest any significant improvement in the mental abilities tested, especially fluid intelligence and working memory capacity, after training for 8 days or 20 days. This leads to the conclusion that increasing one's working memory capacity by training and practice did not transfer to improvement on fluid intelligence as asserted by Jaeggi and her colleagues (2008, 2010).

[Jonathan Toomim](http://groups.google.com/group/brain-training/msg/2fe15fe6eb33a8b6) points out a concern about [statistical power](!Wikipedia): the multiple control groups means that the number of subjects doing actual n-backing is small and the null result is only trustworthy if one expects a dramatic effect from n-backing, a huge effect size taken from Jaeggi 2010 (but not Jaeggi 2008's smaller effect size). He comments: "the [effect size](!Wikipedia) for DNB training is probably less than 0.98. (Of course, that's what I believed anyway before I saw this.) The effect size could quite reasonably still be as high as 0.75." Chooi 2011 seems to have been summarized as [Chooi & Thompson 2012](/docs/dnb/2012-chooi.pdf "Working memory training does not improve intelligence in healthy young adults"), which discusses the power issue further:

> A major limitation of the study was the small sample size and possibly sample characteristic, which may have lowered the power of analyses conducted. When Jaeggi et al. (2010) repeated the study with 25 students who trained on the Raven's Advanced Progressive Matrices (RAPM) for 20 days, they obtained an effect size (Cohen's _d_) of 0.98. Additionally, participants in the Jaeggi et al. (2010) study were culturally different from the participants in the current study. Participants from the former study were undergraduates from a university in Taiwan (mean age=19.4), while those from the current study were mostly American students attending a Midwestern university. The current study was designed according to the claims put forth by Jaeggi et al. (2008) as a study of replication and extension. In that study, participants were healthy, young adults who were slightly older (mean age=25.6 years) than the current sample (mean age= 20.0), and they were recruited from a university in Bern, Switzerland. Effect sizes obtained from our study for RAPM were not as high as reported by Jaeggi et al. (2008, 2010) - _d_=0.65 and _d_=0.98 respectively. With such large effect sizes, the analysis of paired t-test could achieve a power of 0.80 with 10- 12 participants. Referring to Table 4, the highest RAPM effect size (_d_=0.50) was from the 8-day passive control group that had 22 participants and this achieved a power of 0.83. The 20-day training group (_n_=13) had an effect size of 0.06 in RAPM, and to achieve a power of 0.80 this group would need more than 1700 participants. On the other hand, the effect size from the 20-day active control group with 11 participants was 0.40, and power could be improved by increasing the number of participants to 34. These observations led us to believe that the lack of improvements in the test variables was probably due to a combination of low sample size and differences in sample characteristics, of which participants in our study had restriction of range in intellectual ability.

### Preece 2011 / Palmer 2011

["The Effect of Working Memory (_n_-back) Training on Fluid Intelligence"](http://ro.ecu.edu.au/theses_hons/54/), David Preece 2011:

> The present study replicated and extended these results by testing the fluid intelligence construct using a different type of fluid intelligence test, and employing an 'active' rather than 'no-contact' control group to account for motivational effects on intelligence test performance. 58 participants were involved and their fluid intelligence was assessed pre-training using the Figure Weights subtest from the Wechsler Adult Intelligence Scale - Fourth Edition (WAIS-IV). Participants were randomly assigned to two groups (experimental or active control), and both groups did a training task on their home computer for 20 days, for 20 minutes a day. The experimental group trained using a single _n_-back task whilst the control group completed general knowledge and vocabulary questions. After training, participants were retested using the Figure Weights subtest. Participants' Figure Weights scores were analysed using an analysis of covariance (ANCOVA). The results of this analysis revealed no significant difference between the training groups in terms of performance on the Figure Weights subtest, suggesting that the _n_-back task was not effective in increasing fluid reasoning ability. These findings were in contrast to those of Jaeggi et al. (2008) and Jaeggi et al. (2010) and suggested that differences between the working memory group and control group found in these studies were likely the result of placebo/motivational effects rather than the properties of the _n_-back task itself.

Subjects were also tested on the RAPM pre/post, but that was reported in a separate thesis, Vaughan Palmer's "Improving fluid intelligence (Gf) though training", which is not available. I have emailed the supervising professor for more information.

2 closely related theses are ["Improving Memory Using N-Back Training"](http://ro.ecu.edu.au/cgi/viewcontent.cgi?article=1064&context=theses_hons), Beavon 2012 (short-term & working memory); and ["Visual Memory Improvement in Recognition"](http://ro.ecu.edu.au/cgi/viewcontent.cgi?article=1058&context=theses_hons), Prandl 2012.

### Kundu et al 2012

["Behavioral and EEG Effects of Working Memory Training"](http://psych.wisc.edu/postlab/posters/bornali_cns_2012.pdf); 13 controls and 13 experimentals trained for 1000 minutes on dual n-back (Brain Workshop) or Tetris. "Training does not appear to transfer to _gf_ [RAPM] or complex span [OSPAN]." This is not a published study but a conference poster, so details such as RAPM scores are not included. It may be related to [Kundu et al 2011](#kundu-et-al-2011).

#### Kundu et al 2013

The interim posters Kundu 2011 & 2012 were published as ["Strengthened effective connectivity underlies transfer of working memory training to tests of short-term memory and attention"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3758887/), Kundu et al 2013:

> Although long considered a natively endowed and fixed trait, working memory (WM) ability has recently been shown to improve with intensive training. What remains controversial and poorly understood, however, are the neural bases of these training effects, and the extent to which WM training gains transfer to other cognitive tasks. Here we present evidence from human electrophysiology (EEG) and simultaneous transcranial magnetic stimulation (TMS) and EEG that the transfer of WM training to other cognitive tasks is supported by changes in task-related effective connectivity in frontoparietal and parietooccipital networks that are engaged by both the trained and transfer tasks. One consequence of this effect is greater efficiency of stimulus processing, as evidenced by changes in EEG indices of individual differences in short-term memory capacity and in visual search performance. Transfer to search-related activity provides evidence that something more fundamental than task-specific strategy or stimulus-specific representations have been learned. Furthermore, these patterns of training and transfer highlight the role of common neural systems in determining individual differences in aspects of visuospatial cognition.

### Salminen 2012

["On the impacts of working memory training on executive functioning"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3368385/), Salminen & Strobach & Schubert, _Frontiers in Human Neuroscience_:

> Recent studies have reported improvements in a variety of cognitive functions following sole working memory (WM) training. In spite of the emergence of several successful training paradigms, the scope of transfer effects has remained mixed. This is most likely due to the heterogeneity of cognitive functions that have been measured and tasks that have been applied. In the present study, we approached this issue systematically by investigating transfer effects from WM training to different aspects of executive functioning. Our training task was a demanding WM task that requires simultaneous performance of a visual and an auditory n-back task, while the transfer tasks tapped WM updating, coordination of the performance of multiple simultaneous tasks (i.e., dual-tasks) and sequential tasks (i.e., task switching), and the temporal distribution of attentional processing. Additionally, we examined whether WM training improves reasoning abilities; a hypothesis that has so far gained mixed support. Following training, participants showed improvements in the trained task as well as in the transfer WM updating task. As for the other executive functions, trained participants improved in a task switching situation and in attentional processing. There was no transfer to the dual-task situation or to reasoning skills. These results, therefore, confirm previous findings that WM can be trained, and additionally, they show that the training effects can generalize to various other tasks tapping on executive functions.

Passive control group; unspeeded RAPM test.

### Redick 2012

["No evidence of transfer after working memory training: A controlled, randomized study"](http://scottbarrykaufman.com/wp-content/uploads/2012/05/Redick-et-al-final-JEPG.pdf), Redick et al; abstract:

> Numerous recent studies seem to provide evidence for the general intellectual benefits of working memory training. In reviews of the training literature, Shipstead, Redick, and Engle (2010, in press) argued that the field should treat recent results with a critical eye. Many published working memory training studies suffer from design limitations (no-contact control groups, single measures of cognitive constructs), mixed results (transfer of training gains to some tasks but not others, inconsistent transfer to the same tasks across studies), and lack of theoretical grounding (identifying the mechanisms responsible for observed transfer). The current study compared young adults who received 20 sessions of practice on an adaptive dual n-back program (working memory training group) or an adaptive visual search program (active placebo-control group) with a no-contact control group that received no practice. In addition, all subjects completed pre-test, mid-test, and post-test sessions, comprising multiple measures of fluid intelligence, multitasking, working memory capacity, crystallized intelligence, and perceptual speed. Despite improvements on both the dual n-back and visual search tasks with practice, and despite a high level of statistical power, there was no positive transfer to any of the cognitive ability tests. We discuss these results in the context of previous working memory training research, and address issues for future working memory training studies.

75 subjects; RAPM was speeded.

### Rudebeck 2012

["A Potential Spatial Working Memory Training Task to Improve Both Episodic Memory and Fluid Intelligence"](http://www.plosone.org/article/info:doi%2F10.1371%2Fjournal.pone.0050431), Rudebeck et al 2012:

> One current challenge in cognitive training is to create a training regime that benefits multiple cognitive domains, including episodic memory, without relying on a large battery of tasks, which can be time-consuming and difficult to learn. By giving careful consideration to the neural correlates underlying episodic and working memory, we devised a computerized working memory training task in which neurologically healthy participants were required to monitor and detect repetitions in two streams of spatial information (spatial location and scene identity) presented simultaneously (i.e. a dual n-back paradigm). Participants' episodic memory abilities were assessed before and after training using two object and scene recognition memory tasks incorporating memory confidence judgments. Furthermore, to determine the generalizability of the effects of training, we also assessed fluid intelligence using a matrix reasoning task. By examining the difference between pre- and post-training performance (i.e. gain scores), we found that the trainers, compared to non-trainers, exhibited a significant improvement in fluid intelligence after 20 days. Interestingly, pre-training fluid intelligence performance, but not training task improvement, was a significant predictor of post-training fluid intelligence improvement, with lower pre-training fluid intelligence associated with greater post-training gain. Crucially, trainers who improved the most on the training task also showed an improvement in recognition memory as captured by d-prime scores and estimates of recollection and familiarity memory. Training task improvement was a significant predictor of gains in recognition and familiarity memory performance, with greater training improvement leading to more marked gains. In contrast, lower pre-training recollection memory scores, and not training task improvement, led to greater recollection memory performance after training. Our findings demonstrate that practice on a single working memory task can potentially improve aspects of both episodic memory and fluid intelligence, and that an extensive training regime with multiple tasks may not be necessary.

Speeded BOMAT ("Due to time restrictions and the possibility of ceiling effects associated with some Gf tests, participants were given 10 minutes to complete as many patterns as they could in each assessment session (for a similar procedure see Jaeggi et al 2008)."); 55 subjects total, experimentals trained for 400 minutes, passive control group. The improvement predictor sounds like a post hoc analysis and may be something like regression to the mean.

### Heinzel et al 2013

["Working memory training improvements and gains in non-trained cognitive tasks in young and older adults"](/docs/dnb/2013-heinzel.pdf), Heinzel et al 2013:

> Previous studies on working memory training have indicated that transfer to non-trained tasks of other cognitive domains may be possible. The aim of this study is to compare working memory training and transfer effects between younger and older adults (n = 60). A novel approach to adaptive n-back training (12 sessions) was implemented by varying the working memory load and the presentation speed. All participants completed a neuropsychological battery of tests before and after the training. On average, younger training participants achieved difficulty level 12 after training, while older training participants only reached difficulty level 5. In younger participants, transfer to Verbal Fluency and Digit Symbol Substitution test was found. In older participants, we observed a transfer to Digit Span Forward, CERAD Delayed Recall, and Digit Symbol Substitution test. Results suggest that working memory training may be a beneficial intervention for maintaining and improving cognitive functioning in old age.

Single n-back; passive control group; no transfer in young or old training group to "Raven's Standard Progressive Matrices (Raven's SPM) and the Figural Relations subtest of a German intelligence test (Leistungspruefsystem, LPS, Horn, 1983)" (increased but sample size is too small to reach statistical-significance in the young group); RPM speeded (7.5 minutes). See pg19 for graphs of the IQ test performance.

#### Onken 2013

["Transfer von Arbeitsgedächtnistraining auf die fluide Intelligenz"](http://www.diss.fu-berlin.de/diss/servlets/MCRFileNodeServlet/FUDISS_derivate_000000013505/diss_joahnnaonken.pdf), Johanna Onken 2013; some sort of re-reporting or version of the Heinzel data.

> Fluid intelligence describes the ability to think abstract, to adapt to new situations and to solve unknown problems. It is important for learning as well as for academic and professional success. Working memory is characterized as a cognitive system, that saves information over a short period of time in spite of possible distractions. Moreover, working memory is able to assess the relevance of information while requirements change. Effective implicit training is able to increase the working memory capacity. Furthermore it was shown that working memory training may also cause transfer effects to higher cognitive abilitys such as fluid intelligence. To clarify the underlying processes of this transfer, various transfer models were presented, which either accentuate the relevance of processing speed, executive functions or short time memory. The purpose of this survey was to confirm tansfer effects of working memory training to different cognitive abilities and, on the other hand, to investigate the mechanism of the transfer according to the proposed transfer models. 30 healthy subjects [age 22-30 years] participated in the study and were randomly assigned to either training or control group. The training group practiced an adaptive N-back working memory task for four weeks. Before, after one week and after four weeks of the training, a range of neuropsychological tasks was performed by the participants, testing for different cognitive abilities. Relative to the control group that did not participate in the training, transfer effects to processing speed, executive functions and fluid intelligence tasks have been found. Additionally, the training resulted in a significant shortening of reaction time. In summary, the present study demonstrates that complex cognitive abilities can be improved through effective working memory training. The question on which cognitive mechanisms the transfer is based could not be answered definitively by this study. The results suggest that the adaptive working memory training has led mainly to faster basal cognitive processes, which in turn resulted in a faster processing of intelligence tests.

30 subjects, passive control group, 4 weeks; controls paid 50 euros, experimentals 150 euros, 480 minutes of training, single n-back. IQ tests administered: RPM, LPS, MWT-B. On pg40 are all the post-test results: "Tabelle 3.7: Deskriptive Daten der Neuropsychologie im Posttest (t3)"; discussion of RPM results on pg46.

### Thompson et al 2013

["Failure of Working Memory Training to Enhance Cognition or Intelligence"](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0063614):

> ...The current study attempted to replicate and expand those results by administering a broad assessment of cognitive abilities and personality traits to young adults who underwent 20 sessions of an adaptive dual n-back working memory training program and comparing their post-training performance on those tests to a matched set of young adults who underwent 20 sessions of an adaptive attentional tracking program. Pre- and post-training measurements of fluid intelligence, standardized intelligence tests, speed of processing, reading skills, and other tests of working memory were assessed. Both training groups exhibited substantial and specific improvements on the trained tasks that persisted for at least 6 months post-training, but no transfer of improvement was observed to any of the non-trained measurements when compared to a third untrained group serving as a passive control. These findings fail to support the idea that adaptive working memory training in healthy young adults enhances working memory capacity in non-trained tasks, fluid intelligence, or other measures of cognitive abilities.

Covariate details:

> ...Two groups of young adults, stratified so as to be equated on initial fluid IQ scores, were randomly assigned to two conditions (a randomized controlled trial or RCT). The experimental group performed the dual n-back task (as in the original Jaeggi et al., 2008 study [6]) for approximately 40 minutes per day, 5 days per week for 4 weeks (20 sessions of 30 blocks per session, exceeding the maximum of 19 sessions of 20 blocks per day in the original Jaeggi et al., 2008 study). An active control group performed a visuospatial skill learning task, multiple object tracking (or MOT), on an identical training schedule. We also tested a no-contact group equated for initial fluid IQ in case both kinds of training enhanced cognitive abilities...Participants were given 25 minutes to complete each half of the RAPM...Participants in the training groups were paid $20 per training session, with a $20 bonus per week for completing all five training sessions in that week. All participants were paid $20 per hour for behavioral testing, and $30 per hour for imaging sessions (data from imaging sessions are reported separately)....After recruitment, participants underwent approximately six hours of behavioral testing spread across three days and two hours of structural and functional magnetic resonance imaging. [Thompson says there were additional payments for imaging not mentioned, so the true expected-value of participation was $740.]

In line with my [meta-analysis's null](DNB meta-analysis#training-time) on a dose-response effect:

> One method of assessing whether the amount of training improvement affects the degree of transfer is to measure the correlation between training and transfer gains. For both the n-back and MOT groups, a positive correlation was observed between the amount of improvement during training and the amount of improvement on the trained task between the pre- and post-assessment (n-back r = .85, p<.0001; MOT r = .77, p<.0001). However, the amount of training gain did not significantly predict improvement on any transfer task; participants who improved to a greater extent on the training tasks did not improve more or less on potential transfer tasks than did participants who improved to a lesser extent (all n-back r values <.33, all p's >.15; all MOT r values <.38, all p's >.11). Figure S2 depicts the absence of a relation between improvement on trained tasks and the post-training changes in the RAPM and the combined span tasks.

Note also that the _post hoc_ split of children into 'improvers' and not in that Jaeggi paper does not replicate here either:

> Another analysis that has previously revealed a difference in transfer between participants who exhibited larger or smaller training gains has been a division of participants into groups based on training gains above or below the group median (median split) [15]. Such a median split of participants in the present study who performed the n-back training yielded no significant differences in transfer between groups (all n-back t-ratios <1.78, all p's >.09). The only transfer measure that approached significance (at p = .09) was on the RAPM test, in which the participants who improved less on the trained n-back task had higher scores on the post-training behavioral testing. Similarly, when separating the MOT participants into two groups based on median MOT improvement, the two groups showed no significant differences in transfer performance (all MOT t-ratios <1.74, all p's >.10).

The personality correlates from Studer-Luethi 2012 also don't work:

> We also examined whether personality assessments were associated with different training or transfer outcomes. Neither the Dweck measure of attitude toward intelligence (a "growth mindset") nor measures of conscientiousness or grit correlated significantly with training gains on either training task, although there was a trend toward a significant negative correlation between the growth mindset and improvement on the n-back training task (r = −.44, p = .051), such that participants who viewed intelligence as more malleable had less improvement across their n-back training. A greater growth mindset score was positively correlated, however, with improvement on the Ravens Advanced Progressive Matrices in the n-back group (r = .53, p = .017) and in the passive control group (r = .51, p = .027), but not in the MOT control group (r = .031, p>.9). No other transfer measures were significantly predicted by growth mindset scores.
>
> Although the conscientiousness scores and "grit" scores were highly correlated in each of the three treatment groups (n-back r = .75, p<.001; MOT r = .70, p<.001; passive r = .76, p<.001), the two measures differed in their correlations with the behavioral outcome measures. A higher "grit" score predicted less improvement on the RAPM for the n-back group (r = −.45, p = .049) and the MOT group (r = −.58, p = .009), such that participants who viewed themselves as having more "grit" improved less on the RAPM after training, although this relationship did not hold for the No-Contact group (r = .17, p = .5). Similarly, a higher score on the conscientiousness measure predicted less improvement on the RAPM for the MOT group (r = −.57, p = .01), such that participants who saw themselves as more conscientious improved less on the RAPM after training, although this was not observed in either of the other two groups (n-back r = −.21, p = .37; no-contact r = −.04, p = .85). Finally, a high conscientiousness score predicted a lower Pair Cancellation improvement within the MOT group (r = −.47, p = .04), but not in the n-back or no-contact control groups (n-back r = −.07, p = .77; no-contact r = −.13, p = .58). No other transfer measures were significantly predicted by either conscientiousness or grit scores.

### Smith et al 2013

["Exploring the effectiveness of commercial and custom-built games for cognitive training"](/docs/dnb/2013-smith.pdf), Smith et al 2013

> There is increasing interest in quantifying the effectiveness of computer games in non-entertainment domains. We have explored general intelligence improvements for participants using either a commercial-off-the-shelf (COTS) game [_Brain Age_], a custom do-it-yourself (DIY) training system for a working memory task [DNB] or an online strategy game to a control group (without training). Forty university level participants were divided into four groups (COTS, DIY, Gaming, [Passive] Control) and were evaluated three times (pre-intervention, post-intervention, 1-week follow-up) with three weeks of training. In general intelligence tests both cognitive training systems (COTS and DIY groups) failed to produce [statistically-]significant improvements in comparison to a control group or a gaming group. Also neither cognitive training system produced [statistically-]significant improvements over the intervention or follow-up periods.

Dual n-back; RAPM (10 minutes each test); 1 passive control group, 2 actives; 340 minutes training; minimal compensation (course credit & entry into "a prize draw", which was worth 100£). Very small sample sizes (~10 in each of the 4 groups).

### Nussbaumer et al 2013

["Limitations and chances of working memory training"](http://mindmodeling.org/cogsci2013/papers/0566/paper0566.pdf), Nussbaumer et al 2013:

> Recent studies show controversial results on the trainability of working memory (WM) capacity being a limiting factor of human cognition. In order to contribute to this open question we investigated if participants improve in trained tasks and whether gains generalize to untrained WM tasks, mathematical problem solving and intelligence tests.
>
> 83 adults trained over a three week period (7.5 hours total) in one of the following conditions: A high, a medium or a low WM load group. The present findings show that task specific characteristics could be learned but that there was no transfer between trained and untrained tasks which had no common elements. Positive transfer occurred between two tasks focusing on inhibitory processes. It might be possible to enhance this specific component of WM but not WM capacity as such. A possible enhancement in a learning test is of high educational interest and worthwhile to be investigated further.

One of the two IQ tests was the RAPM; this was dual n-back, but it was adaptive only in the "high" experimental group (so the "medium" and "low" groups are largely irrelevant). Paper does not provide RAPM score details, so I emailed the lead author.

<!-- 7.5 hours of training
RAPM
unpaid
low (active control):
n=27 11.89 (2.24)
high (DNB):
n=29 13.69 (2.54)
-->

### Oelhafen et al 2013

["Increased parietal activity after training of interference control"](/docs/dnb/2013-oelhafen.pdf),

> ...In the current study, we examined whether training on two variants of the adaptive dual n-back task would affect untrained task performance and the corresponding electrophysiological event-related potentials (ERPs). 43 healthy young adults trained for three weeks with a high or low interference training variant of the dual n-back task, or they were assigned to a passive control group. While n-back training with high interference led to partial improvements in the Attention Network Test (ANT), we did not find transfer to measures of working memory and fluid intelligence. ERP analysis in the n-back task and the ANT indicated overlapping processes in the P3 time range. Moreover, in the ANT, we detected increased parietal activity for the interference training group alone. In contrast, we did not find electrophysiological differences between the low interference training and the control group. These findings suggest that training on an interference control task leads to higher electrophysiological activity in the parietal cortex, which may be related to improvements in processing speed, attentional control, or both.

<!--
48 healthy, young adults were recruited from an academic environment and randomly assigned to the lure training group, the non-lure training group (i.e. the active control), or the passive control group. In each group, one participant dropped out between pre- and posttesting and two dropped out during pretest EEG recording, leaving a final sample of 43 participants (mean age: 25.2 years; SD = 4.1; range: 18-34). Both training groups consisted of 14 participants (6 female and 1 left-handed each), and 15 people were assigned to the control group (8 female and 2 left-handed). Each individual submitted a written informed consent before the experimental procedure began. After completion of the study, all participants were paid 50 Swiss Francs for participation.

Each training session consisted of 20 blocks (≈ 25 minutes), and each block consisted of 20 + n trials, e.g. a 3-back block consisted of 23 trials... In sessions 1-3, daily training sessions began with a 1-back block, and in sessions 4-14 with a 2-back block.

. Bochumer Matrizentest. The short version of the Bochumer Matrizentest - advanced (BOMAT) was used to determine gF (Hossiep, Turck, & Hasella, 1999). Similar to Raven's progressive matrices (Raven, 1990), this standardized test consists of 29 increasingly difficult visual analogy problems. We used parallel forms for pre- and posttests with the original time limit of 45 minutes.

At the end of the pretest, we informed participants if they belonged to a training or the passive control group. Therefore, participants in the passive control group knew that they belonged to the control group of a training study, but participants in the two training groups were not informed about the

Both training groups trained with the dual n-back for 14 self- administered sessions at home, distributed over 3 weeks, with a weekly schedule of 5 consecutive training days and 2 days off. Individuals in the control group were not contacted during the training interval. After this interval, participants of all three groups were tested again with the tasks in the same order (posttest).

BOMAT, n=43
Lure: 14 18.5 (2.4)
Non-lure: 14 18.9 (5.1)
total n-back: 28 18.7 (3.75)
((14-1)*2.4 + (14-1)*5.1) / (14 + 14 - 2) = (2.4+5.1)/2 = 3.75
passive control: 15 19.9 (4.7)
-->

### Sprenger et al 2013

["Training working memory: Limits of transfer"](/docs/dnb/2013-sprenger.pdf), Sprenger et al 2013; abstract:

> In two experiments (totaling 253 adult participants), we examined the extent to which intensive working memory training led to improvements on untrained measures of cognitive ability. Although participants showed improvement on the trained task and on tasks that either shared task characteristics or stimuli, we found no evidence that training led to general improvements in working memory. Using Bayes Factor analysis, we show that the data generally support the hypothesis that working memory training was ineffective at improving general cognitive ability. This conclusion held even after controlling for a number of individual differences, including need for cognition, beliefs in the malleability of intelligence, and age.

<!--
> ...Of these, 138 participants (94 females) completed the pre-training assessment, the training regimen, and the post-training assessment
>
> ...Participants were compensated $30 for completing the pre-test and $70 for completing the post-test.
>
> ...Participants were randomly assigned to one of 4 possible conditions: a placebo control training group; an interference training group; a visuo-spatial training group, and a combination training group (which performed both the interference and visuo-spatial WM training tasks). Training took place remotely by logging onto a training website. Similar to Experiment 1 this study maintained a true double-blind pretest/posttest experimental design, in which neither participants nor study moderators had knowledge of the condition to which participants were assigned.
>
> ...Then participants completed an average of 14.03 h of cognitive (or active control) training, which was delivered via internet. Participants completed the training at home on their own computers, for 26 min/day for 28-35 days.
>
> ...6.3.2. Raven's Advanced Progressive Matrices (Raven, Raven, & Court, 1998)
> Participants viewed eight black and white figures arranged in a 3 × 3 grid with one figure missing. Participants chose the image that best completed the pattern from eight possible choices. Participants completed either the odd or even problems at pre-test, and they then completed the complementary set of problems at post-test. Participants were given 10 min to complete as many problems as possible. The dependent variable was the number of correctly solved problems.
>
> ...6.4.2. Memory updating & interference training
> The second group trained on the N-back task, similar to the n-back task in the first experiment, and a new task called "Floop" (a cross between the Flanker task and the Stroop task). Participants played each task for 13 min each day (for a total of 26 min of training/day). For the N-back task, participants viewed letter stimuli and decided (yes/no) whether the current stimulus matched the stimulus "n" trials before. Participants advanced through three lure conditions (no lures, lures at n + 2 and n − 2, and lures at n + 1 & n − 1). The total number of lures was the same as the number of targets. Participants advanced to the next lure or n level when they performed b=3 incorrect responses in the trial sequence, and fell back a step if they performed N=5 incorrect responses. When participants made 4 incorrect responses, they remained at the same difficulty level. Points were awarded for correct responses to 'targets' only, using the equation: 15 ∗ (2^(M − 1)), where M = the lesser of N + 2 or the number of correct responses from last incorrect response.
>
> ...6.4.4. Combination training
> The fourth group trained on N-back, Floop, Shapebuilder, and Memnosyne (block span) each day, playing each task for 6.5 min/day.

RAPM scores, post-test, from appendix B:

control: n=37, 9.95 (3.42)
interference (n-back): n=34, 9.76 (3.68)
combination (n-back+others): n=34, 9.24 (3.34)

training time:

interference: 13min/day, 28-35 days, so 13*((28+35)/2) = 410
combination: 6.5min/day, so 6.5*((28+35)/2) = 205
-->

### Colom et al 2013

["Adaptive n-back training does not improve fluid intelligence at the construct level; gains on individual tests suggest training may enhance visuospatial processing"](/docs/dnb/2013-colom.pdf), Colom et al 2013:

> Short-term adaptive cognitive training based on the n-back task is reported to increase scores on individual ability tests, but the key question of whether such increases generalize to the intelligence construct is not clear. Here we evaluate fluid/abstract intelligence (Gf), crystallized/verbal intelligence (Gc), working memory capacity (WMC), and attention control (ATT) using diverse measures, with equivalent versions, for estimating any changes at the construct level after training. Beginning with a sample of 169 participants, two groups of twenty-eight women each were selected and matched for their general cognitive ability scores and demographic variables. Under strict supervision in the laboratory, the training group completed an intensive adaptive training program based on the n-back task (visual, auditory, and dual versions) across twenty-four sessions distributed over twelve weeks. Results showed this group had the expected systematic improvements in n-back performance over time; this performance systematically correlated across sessions with Gf, Gc, and WMC, but not with ATT. However, the main finding showed no significant changes in the assessed psychological constructs for the training group as compared with the control group. Nevertheless, post-hoc analyses suggested that specific tests and tasks tapping visuospatial processing might be sensitive to training.

<-- One hundred and sixty nine psychology undergraduates completed a battery of twelve intelligence tests and cognitive tasks measuring fluid-abstract and crystallized-verbal intelligence, working memory capacity, and attention control. After computing a general index from the six intelligence tests, two groups of twenty-eight females were recruited for the study. They were paid for their participation€. Members of each group were carefully matched for their general intelligence index, so they were perfectly overlapped and represented a wide range of scores. All participants were right handed, as assessed by the Edinburgh Test (Oldfield, 1971). They also completed a set of questions asking for medical or psychiatric disorders, as well as substance intake. The recruitment process followed the Helsinki guidelines (World Medical Association, 2008) and the local ethics committee approved the study. Descriptive statistics for the demographic variables and performance on the cognitive measures for the two groups of participants (training and control) can be seen in the Appendix (Table A.1.).  [200€ if assigned to the training group and 100€ if assigned to the control group.] [150 euros in dollars is $204]

The collective psychological assessment for the pretest stage was done from September 19 to October 14 2011. Participants were assessed in groups not greater than twenty-five. The data obtained for the complete group (N = 169) were analyzed for recruiting the training (N = 28) and control (N = 28) groups based on the general index computed from the measures of fluid and crystallized intelligence (Table A.1.). The adaptive cognitive training program began in November 14 2011, remained active until 17 February 2012, and lasted for twelve weeks (with a break from December 24 2011 to January 9 2012). The psychological assessment for the posttest was done individually from February 20 to March 09 (intelligence tests) and from March 12 to March 30 (cognitive tasks) 2012.

Intelligence and cognitive constructs were assessed by three measures each. As noted above, fluid intelligence (Gf) requires abstract problem solving abilities, whereas crystallized intelligence (Gc) involves the mental manipulation of cultural knowledge. Gf was measured by screening versions (odd numbered items and even numbered items for the pretest and posttest evaluations, respectively) of the Raven Advanced Progressive Matrices Test (RAPM), the abstract reasoning subtest from the Differential Aptitude Test (DAT-AR), and the inductive reasoning subtest from the Primary Mental Abilities Battery (PMA-R). Gc was measured by screening versions (odd numbered items and even numbered items for the pretest and posttest evaluations, respectively) of the verbal reasoning subtest from the DAT (DAT-VR), the numerical reasoning subtest from the DAT (DAT-NR), and the vocabulary subtest from the PMA (PMA-V). Gf and Gc were measured by tests with (PMA subtests) and without (RAPM and DAT subtests) highly speeded constraints.

The framework for the cognitive training program followed the guidelines reported by Jaeggi et al. (2008) but it was re-programmed for Visual Basic (2008 Version). Nevertheless, there were some differences: (a) the training began with four sessions (weeks 1 and 2) with a visual adaptive n-back version and four sessions (weeks 3 and 4) with an auditory adaptive n-back version before facing the sixteen sessions of the adaptive n-back dual program (weeks 5 to 12), and (b) while the training program is usually completed in one month, here we extended the training period to three months (12 weeks). There were two training sessions per week lasting around 30 min each and they took place under strict supervision in the laboratory. Participants worked within individual cabins and the experimenter was always available for attending any request they might have. Data were analyzed every week for checking their progress at both the individual and the group level. Participants received systematic feedback regarding their performance. Furthermore, every two weeks participants completed a motivation questionnaire asking for their (a) involvement with the task, (b) perceived difficulty level, (c) perceived challenging of the task levels, and (d) expectations for future achievement. At the end of the training period participants were asked with respect to their general evaluation of the program. Using a rating scale from 0 to 10, average values were (a) 8.1 (range 8.0 to 8.2 across sessions), (b) 7.9 (range 7.4 to 8.5 across sessions), (c) 8.0 (range 7.8 to 8.2 across sessions), and (d) 7 (range 6.5 to 7.7 across sessions).  [12 weeks * 2 sessions * 30 minutes = 720 minutes]

The control group was passive. After the recruitment process, members of this no-contact control group were invited to follow their normal life as university students. As reasoned in some of our previous research reports addressing the potential effect of cognitive training, and according to the main theoretical framework, we were not interested in comparing different types of training, but in the comparison between a specific cognitive training and doing nothing beyond regular life.

Four out of six intelligence tests were applied without severe time constraints. For the RAPM there was more than one minute per item (20 minutes for 18 items). For DAT-AR DAT-NR and DAT-VR, there were approximately 30 seconds per item (10 minutes for 20 items). For the speeded tests (PMA-R and PMA-V) there were between 5 and 12 seconds per item (PMA-R: 3 minutes for 15 items and PMA-V: 2 min for 25 items.)

posttest:
training, n=28
RAPM: 37.25 (6.23)
control n=28
RAPM: 35.46 (8.26)

DAT-AR
PMA-R

Mean differences between the odd and even items were significant (p < 0.001 for all the tests, excluding the DAT-VR) which implies that pretest (odd) and posttest (even) scores must not be directly compared.
-->

### Burki et al 2014

["Individual differences in cognitive plasticity: an investigation of training curves in younger and older adults"](/docs/dnb/2014-burki.pdf), Burki et al 2014

> To date, cognitive intervention research has provided mixed but nevertheless promising evidence with respect to the effects of cognitive training on untrained tasks (transfer). However, the mechanisms behind learning, training effects and their predictors are not fully understood. Moreover, individual differences, which may constitute an important factor impacting training outcome, are usually neglected. We suggest investigating individual training performance across training sessions in order to gain finer-grained knowledge of training gains, on the one hand, and assessing the potential impact of predictors such as age and fluid intelligence on learning rate, on the other hand. To this aim, we propose to model individual learning curves to examine the intra-individual change in training as well as inter-individual differences in intra-individual change. We recommend introducing a latent growth curve model (LGCM) analysis, a method frequently applied to learning data but rarely used in cognitive training research. Such advanced analyses of the training phase allow identifying factors to be respected when designing effective tailor-made training interventions. To illustrate the proposed approach, a LGCM analysis using data of a 10-day working memory training study in younger and older adults is reported.

Republication of a thesis.

<!--
> A total of 128 individuals, 63 younger and 65 older adults met the inclusion criteria (i.e., aged between 18 and 38 years for the younger adults and 60 years and older for the older adults group, practice of French for at least 5 years) and participated in the study. The present study was approved by the ethics committee of the Faculty of Psychology and Educational Sciences of the University of Geneva. All participants signed a written informed consent for participation. Several control tests were performed such as the Freiburg Visual Acuity and Contrast Test (FrACT; Bach 1996) for visual acuity control, a self-rating health questionnaire and a crystallized intelligence test, the French adaptation of the Mill Hill Vocabulary Scale (Deltour 1993; Raven et al. 1998). The participants were pseudo-randomly (age-, gender- and education-matched) assigned to either a cognitive training group or an active or a passive control group. The experimental groups did not differ from each other within the age-groups regarding the characteristics listed in Table 1, to the exception of the total number of years of education [F(2, 62) = 3.38, p =0.04]. The older no-contact control group had significantly less years of education compared to its trained peers.
> The WM training group performed an adaptive verbal N-back task training adapted from the dual N-back paradigm used by Jaeggi et al. (2008). A single task condition with verbal stimuli based on Chicherio (2006) and Ludwig et al. (2008) was used. The task consisted in judging whether the current letter matches the letter N positions back in a sequence of letters presented one by one (see Fig. 1, left panel). The level of difficulty (N-level) was varied by adapting the load in each block to the participant’s performance reached in the preceding block.
>
> The active control group was trained with an implicit sequence learning task which served as a placebo training to control for the confounding variables resulting from a training setting. The goal was to present a task requiring as little attentional resources as possible during this training. Therefore, a simple implicit sequence learning task was chosen. The sequence should be implicitly learned while attentional control decreases and speed increases in the course of the training (Gaillard et al. 2009; Howard et al. 2004; Parkin 1993). The task consisted of four light gray squares presented horizontally aligned in the center of the screen (see Fig. 1, right panel). One stimulus consisted of one pink and three gray squares, each square changing color from gray to pink in turn. The participants had to respond as fast and as accurately as possible by pressing the key matching the position of the pink square. Similar to the WM training, 15 blocks per training session were provided.
>
> Between pretest and posttest, participants in the training groups performed ten training sessions distributed over 2–4 weeks. The participants were trained with either a WM task or an implicit sequence learning task. The training sessions lasted about 30 min per day and were administered individually. The control groups without training were not contacted during 2–4 weeks between pretest and posttest.
>
> [30 minutes * 10 sessions = 300 minutes total training]
>
> Several far-transfer tasks were further included. First, a fluid intelligence measure, the Raven’s Progressive Matrices (Raven 1958, 1962), was administered. In order to avoid a ceiling effect in younger adults and a floor effect in older adults, both versions, i.e., the Raven’s Standard Progressive Matrices and the Raven’s Advanced Progressive Matrices, were used. Odd and even trials of each version were separated and administered as different forms at pretest and posttest as described in Jaeggi et al. (2010).
>
> In terms of the remaining transfer tasks, the Updating task, the Raven task, the Letter Comparison task and the SRT task, transfer scores showed a significant positive correlation with intercept or slope. The correlation with intercept revealed that the Letter Comparison transfer score was high for participants who showed a high initial training level. The correlation with slope indicated that the participants who showed a steep training curve also showed a high transfer score in the Updating, the Raven and the SRT task, beyond the effect of age and fluid intelligence.

No compensation was mentioned.

300

Raw data, Raven post-test:

RAPM ==1:
young adults, n-back (n=22): 37.41 (6.43)
young adults, active control (n=20): 35.95 (7.35)
young adults, passive control(n=21): 36.86 (6.55)

RSPM ==2:
old adults, n-back(n=22): 28.86 (7.10)
old adults, active control (n=20): 31.20 (6.67)
old adults: passive control (n=23): 27.61 (6.82)
-->

### Pugin et al 2014

["Working memory training shows immediate and long-term effects on cognitive performance in children and adolescents"](http://f1000research.com/articles/3-82/v1), Pugin et al 2014:

> Working memory is important for mental reasoning and learning processes. Several studies in adults and school-age children have shown performance improvement in cognitive tests after working memory training. Our aim was to examine not only immediate but also long-term effects of intensive working memory training on cognitive performance tests in children and adolescents. Fourteen healthy male subjects between 10 and 16 years trained a visuospatial n-back task over 3 weeks (30 min daily), while 15 individuals of the same age range served as a passive control group. Significant differences in immediate (after 3 weeks of training) and long-term effects (after 2-6 months) in an auditory n-back task were observed compared to controls (2.5 fold immediate and 4.7 fold long-term increase in the training group compared to the controls). The improvement was more pronounced in subjects who improved their performance during the training. Other cognitive functions (matrices test and Stroop task) did not change when comparing the training group to the control group. We conclude that spatial working memory training in children and adolescents boosts performance in similar memory tasks such as the auditory n-back task. The sustained performance improvement several months after the training supports the effectiveness of the training.

<!--
> Thereafter, the participants were asked to train at home for a maximum of 30 minutes per day over the following 3 weeks. Within these 20 days, they were visited once at home at a planned date by a research assistant (not the cognitive test examiner).

20* 30 = 600 minutes total

> All participants received a present for taking part in the 3 cognitive test sessions. For participating in the 3 weeks of training, the members of the training group were given a small present of choice.

Visual n-back training

> Matrix reasoning task [MAT, Test of Nonverbal Intelligence (TONI IV)]. As a measure of fluid intelligence, we used the matrix reasoning task TONI-IV including two versions, from which the order (A, B, A or B, A, B) was balanced between the groups (age standardized values available). In the task, the participants had to choose the only pattern that completed the matrix presented from a given sample of patterns. The tasks stopped when three of five consecutive trials were not correctly solved or if the maximal trial number was reached (60, none of the subjects reached the maximum).

Training group TONI scores:
n=14 111.29 (2.86)

passive control group scores:
n=15 111.13 (2.86)

"Question 1: Here are Mean, SEM for the raw values (not age-corrected):
        Training group:
                PRE 36.29, 2.38
                POST 40.29, 2.30
                FU 43.36, 2.30

        Control group:
                PRE 40.20, 1.92
                POST 41.33, 1.97
                FU 40.64, 2.26"
-->


## Meta-analysis

> I construct a meta-analysis of the >19 studies which measure IQ after an n-back intervention, confirming that there is a [gain](DNB meta-analysis#analysis) of small-to-medium effect size. I also investigate several n-back claims, criticisms, and indicators of bias, finding:
>
> - [active vs passive control groups](DNB meta-analysis#control-groups) criticism: found, and it accounts for half the total effect size (similar to [Zehdner et al 2009](/docs/dnb/2009-zehdner.pdf "Memory training effects in old age as markers of plasticity: a meta-analysis") & [Melby-Lervåg & Hulme 2013](http://www.apa.org/pubs/journals/releases/dev-49-2-270.pdf "Is Working Memory Training Effective? A Meta-Analytic Review"))
> - [dose-response relationship](DNB meta-analysis#training-time) of n-back training time & IQ gains claim: not found
> - [payment reducing performance](DNB meta-analysis#paymentextrinsic-motivation) claim: not found
> - [kind of n-back](DNB meta-analysis#training-type) matters: not found
> - [publication bias](DNB meta-analysis#biases) criticism: not found
> - [speeding of IQ tests](DNB meta-analysis#iq-test-time) criticism: not found

Due to its length and technical detail, my meta-analysis has been moved to [a separate page](DNB meta-analysis).

## Does it really work?

### N-back improves working memory

There are quite a few studies showing significant increases in working memory: WM is something that can be trained. See for example ["Changes in cortical activity after training of working memory - a single-subject analysis."](http://lib.bioinfo.pl/pmid:17597168) or ["Increased prefrontal and parietal activity after training of working memory"](http://www.nature.com/neuro/journal/v7/n1/abs/nn1165.html).

There are a few studies showing that DNB training enhances _Gf_; see the [support](#support) section. There is also a study showing that WM training (not DNB) enhances _Gc_^[[Alloway 2009](http://precedings.nature.com/documents/3697/version/1), ["The efficacy of working memory training in improving crystallized intelligence"](http://precedings.nature.com/documents/3697/version/1/files/npre20093697-1.pdf) (PDF) 7 children with learning disabilities received the training for 8 weeks; _Gc_ was measured using the vocabulary & math sections of the Wechsler IQ test.].

### IQ Tests
#### Measuring

Because N-back is supposed to improve your pure '[fluid intelligence](!Wikipedia)' (_Gf_), and not, say, your English vocabulary, the most accurate tests for seeing whether N-back has done anything are going to be ones that avoid vocabulary or literature or tests of subject-area knowledge. That is, 'culture-neutral' IQ tests. (A non-neutral test focuses more on your 'crystallized intelligence', while N-back is supposed to affect 'fluid intelligence'; they do affect each other a little but it's better to test fluid intelligence with a fluid intelligence test.)

As one ML member writes:

> The WAIS test involves crystallized intelligence and is unsuitable for judging fluid intelligence. High working memory will not spawn the ability to solve complex mathematical and verbal problems on its own, you have to put your extended capacity to learning. All very-high-level IQ tests are largely crystallized IQ tests, therefore working memory gains will not be immediately apparent by their measure.[ref](http://groups.google.com/group/brain-training/browse_thread/thread/8af44f3b20df9904)

#### Available tests

The gold-standard of culture-neutral IQ tests is [Raven's progressive matrices](!Wikipedia). Unfortunately, Raven's is not available for free online, but there are a number of clones one can use - bearing in mind their likely inaccuracy and the fact that many of them do not randomize their questions. It's a very good idea, if you plan to n-back for a long time, to take an IQ test before and an IQ test after, both to find out whether you improved and so you can tell the rest of us. But the interval has to be a long one: if you are testing at the beginning and end of your training there is probably going to be a [practice effect](!Wikipedia) which will distort your second score upwards[^salthousepractice]; it's strongly recommended you take a particular test only once, or with intervals on the order of months (and preferably years).

[^salthousepractice]: The practice effect can last for many years. ["Influence of Age on Practice Effects in Longitudinal Neurocognitive Change"](http://www.apa.org/pubs/journals/releases/neu-24-5-563.pdf), Salthouse 2010:

    > [Longitudinal comparisons](!Wikipedia "Longitudinal study") of neurocognitive functioning often reveal stability or age-related increases in performance among adults under about 60 years of age. Because nearly monotonic declines with increasing age are typically evident in [cross-sectional comparisons](!Wikipedia "Cross-sectional data"), there is a discrepancy in the inferred age trends based on the two types of comparisons....Increased age was associated with significantly more negative longitudinal changes with each ability. All of the estimated practice effects were positive, but they varied in magnitude across neurocognitive abilities and as a function of age. After adjusting for practice effects the longitudinal changes were less positive at younger ages and slightly less negative at older ages. Conclusions: It was concluded that some, but not all, of the discrepancy between cross-sectional and longitudinal age trends in neurocognitive functioning is attributable to practice effects positively biasing the longitudinal trends.

The tests are:

- [iqout.com](http://www.iqout.com/)
- [iqtest.dk](http://www.iqtest.dk/main.swf) (interpret scores with caution: it seems that the maintainer renormed it on the population of online test-takers, which means it will be low)
- [Mensa Norway](http://mensa.no/olavtesten/)
- [Quasi-ravens (unnormed?)](http://www.knowl.demon.co.uk/page111.html)
- <http://iqtest.org.uk/>
- <http://iqtest-australia.com/>
- ["European IQ test"](http://sifter.org/iqtest/)
- ["IQ Comparison Site Advanced Culture Fair IQ Test"](http://iqcomparisonsite.com/)
- [The Brainforce Test](http://zolly.site88.net/brainforce/index.htm) (culture-neutral supposedly up to IQ 150)
- [Queendom.com's "Culture Fair IQ Test"](http://queendom.com/tests/access_page/index.htm?idRegTest=1112)
- ["Culture Fair Numerical & Spatial Exam"](http://www.etienne.se/cfnse/) -(formerly used by high-IQ societies)
- [Jouve-Cerebrals test of induction](http://www.cogn-iq.org/?page_id=734) (formerly "Tri 52")
- [High IQ Society Online Test](http://www.highiqsociety.org/iq_tests/) (includes eCMA)
- [RAPM](http://www.talentlens.com.au/product_details&id=418&cat_id=13) (for fee)
- <http://www.iqtest.com/> (for fee)
- [the International High IQ Society admission test](https://www.gigiassessment.com/shop/index.php) (for fee; but apparently designed to minimize practice effects)

Raven-style matrix tests can be mechanically generated by the [Sandia Generated Matrix Tool](https://web.archive.org/web/20130215172858/http://cognitivescience.sandia.gov/Software/matrixtool.html); the generated matrix test scores statistically look very similar to SPM test scores according to the paper, ["Recreating Raven's: Software for systematically generating large numbers of Raven-like matrix problems with normed properties"](http://www.springerlink.com/content/q5l066578756rw45/fulltext.pdf).

If Raven-style tests bore you or you've gone through the previous ones, there are a wealth of difficult tests at Miyaguchi's ["Uncommonly Difficult IQ Tests"](http://tthqi.free.fr/Uncommonly%20Difficult%20IQ%20Tests.php), and a [five-factor](!Wikipedia "Big Five personality traits") personality test, the [IPIP-NEO](http://www.personalitytest.net/ipip/index.html), is free (although the connection to IQ is minimal).

Other tests that might be useful include [digit-span](!Wikipedia) tests: they provide a non-dual-N-back method of measuring WM before one begins training and then after. There is also [Cogtest](http://www.cogtest.com/coglib_demtest.html) suite of spans and attention tasks or the <http://cognitivefun.net/> site (which implements many tasks). The [Automated Operation Span (OSPAN) Task](http://www.millisecond.com/download/samples/v3/OSPAN/) could be used as well.

#### IQ test results

Reports of IQ tests have been mixed. Some results have been stunning, others have shown nothing.

##### Improvement

> LSaul [posted about](http://groups.google.ca/group/dualnback/browse_thread/thread/97b2340497476ecc/9959b6da18f8fbea) his apparent rise in IQ back in October. From what I remember, he had recently failed to qualify for MENSA, which requires a score of about 131 (98th percentile). He then got a 151 (99.97th percentile) on a professionally administered IQ test (WAIS) three months later, after 2 months of regular dual-n-back use. --[MR](http://groups.google.com/group/brain-training/browse_thread/thread/8af44f3b20df9904)

(A >20 point gain sounds very impressive. But possible confounding factors here are that LSaul apparently took 2 different IQ tests; besides the general incomparability of different IQ tests, it sounds as if the first test was a culture-neutral one, while the WAIS has components such as verbal tests - the second might well be 'easier' for LSaul than the first.)

[Mike L.](http://groups.google.com/group/brain-training/browse_thread/thread/a5a02094c97f30c5) writes:

> Empirically speaking, however: I took a WAIS-IV IQ test (administered professionally) around a year ago and got a 110. I took a derivative of the same test recently (mind you, after about 20 days of DNB training) and got a score of 121.

The blogger of ["Inhuman Experiment"](http://inhumanexperiment.blogspot.com/), who played for ~22 days and went from ~2.6-back to ~4-back, [reports](http://inhumanexperiment.blogspot.com/2009/03/increasing-intelligence-by-playing.html):

> The other test proved to be quite good (you can find it [here](http://www.iqout.com/)). In this one, the questions vary, the difficulty is adjusted on the go depending on whether you answer them correctly, and there's a time limit of 45 seconds per question, which makes this test better suited for re-taking. My first test, taken before playing the game, gave me a score of 126; my second test, taken yesterday, gave me a score of 132 (an increase of about 5%)....As you can see, it's kind of difficult to draw any meaningful conclusions from this. Yes, there was a slight increase in my score, but I would say a similar increase could've been possible even without playing the game. I think the variation in the IQ test questions reduces the "learning by heart" effect, but that's impossible to say without a control group.

[Pontus Granström](http://groups.google.com/group/brain-training/browse_thread/thread/fe4ee2f0c994e40e) writes that

> I scored 133 on www.mensa.dk/iqtest.swf today. I have never scored that high before I really feel the "dnb thinking" kicking in.

(He apparently took that test about a year ago, and avers that his original score on it 'was 122. Well below 130.')

Pheonexia [writes](http://groups.google.com/group/brain-training/browse_thread/thread/fe4ee2f0c994e40e/088dd6260d3d6031):

> Approximately three years ago I took the "European IQ Test." It was posted on some message board and the author of the thread said the test was credible. At that time, I scored 126.
>
> I've been n-backing since early February, so I figured I'd try it again today. I googled "European IQ Test" and clicked the first result, a test from Nanyang Technological University in Singapore.. I don't recall any of the exact questions for the first one I took three years ago, but the format of this test seemed almost identical. Today I scored 144, 18 points higher than before. <http://www3.ntu.edu.sg/home/czzhao/iq/test.htm>
>
> To me, this is anecdotal evidence that n-backing does increase intelligence. I'll try again for another three months and take a completely different test. I will admit, however, that I recognized one of the first questions as the Fibonacci sequence, so I attribute that to crystallized, not fluid intelligence. The highest score this test allows for is 171, meaning you got ZERO questions wrong. I got 6 wrong and 3 half questions wrong where it requires two answers (that was my worst section), so either 7.5 or 9 out of 33 questions wrong.

[Colin](http://groups.google.com/group/brain-training/browse_thread/thread/513e4738a3d5fb78):

> I took one of the IQ tests I did previously [previously linked as "High IQ Society Online Test"] and scored 109 on, I just took it again and scored 116...I don't know about retest effect, but all the questions were different.

Toto writes in ["TNB(PIA) may improve intelligence"](http://groups.google.com/group/brain-training/browse_thread/thread/255ce47293132240/7da5497fb1691528):

> While DNB proved ineffective for me (at least it didn't increase my IQ, though it improved memory) TNB may have made a difference. I took 2 high-range tests during the last 2 months and the results were higher than I expected - my IQ was somewhere between 130 and 135 on good online tests, I scored 132 on a supervised test (Raven's SM). My results on CFNSE <http://www.etienne.se/cfnse/> and GET-γ <http://www.epiqsociety.net/get/>  were approximately 10 points higher - 6 on CFNSE (8 on my second attempt) and 21 on G.E.T . It could be because of a flaw of these tests, or they may not test the same ability as timed tests (though the correlation between them and famous supervised timed tests is said to be very high), it may be for some other reason as well, but it could be because of TNB. I had tried CFNSE long ago and scored 0 (but I probably didn't try hard enough then).

[christopher lines](http://groups.google.com/group/brain-training/browse_thread/thread/624d88b9d2f11860) reports:

> I did a couple of the online IQ tests after about 10 days (scored 126 in one of them [iqtest.dk] and 106 [iqout.com] in another); I repeated the same tests about a month later (about 1 month ago) and scored (133 and 109). I have no idea why the tests gave such big differences in scores but I definately [sic] think its easier the second time you do the tests because I remembered the strategies for solving the problems which took some time to figure out when I first did the tests. I am kind of against keep re-doing the tests because of learning effects and a bit truobled [sic] that different test produce such different results.

Tofu [writes](http://groups.google.com/group/brain-training/browse_thread/thread/d1e53e8c69c95c3a):

> I've purposely not been doing anything to practice for the tests or anything else I thought could increase my score so I wouldn't have to factor other things into an improvement in iq, which makes improvements more likely attributable to dual n-back.  Before I took the test I scored at 117, a score about 1 in about 8 people can get (7.78 to be exact), and yesterday I scored at 127 (a score that 1 in 28 people would get).  Its a pretty big difference I would say.

After a year of N-backing, Tofu has [3 sets of IQ test results](http://groups.google.com/group/brain-training/browse_thread/thread/3af97e7c14585851) using _Self-Scoring IQ Tests_ ([Victor Serebriakoff](!Wikipedia)). To summarize:

1. 0 months (D3B; ~71%): 27,25 = 117 IQ
2. 3 (T4B; ~76%): 37,40[^division] = 128
3. 12 (Q6B; ~47%): 34,42 = 128

[^division]: Tofu: "I should also add, my score on the number test jumped dramatically from the first test to the second test probably because I taught myself how to do long division before the second test (which was the only studying I did for all 3 tests)."

Other relevant tests for Tofu:

> As a sidenote- after 6 months I took a practice [LSAT](!Wikipedia) without any studying and got a 146, roughly 30th percentile, and I took an IQ test from http://iqtest.dk/main.swf after 1 year which I scored a 115 on. Also, in high school I took a professionally administered IQ test and got a 137 which may have been high because they took my age into account in the scoring like the old school IQ tests used to do, but I'm not sure if they actually did that.

[Rotem](http://groups.google.com/group/brain-training/browse_thread/thread/e5ed78ad41d397cb):

> Last year I scored 123 in www.iqtest.dk and today I made 140. If you eliminate statistic deviations, even if it's just 5-10 points it's very good IMO.

[milestones](http://groups.google.com/group/brain-training/browse_thread/thread/7e859b3a7bfeb0d8):

> I do actually have gains to report on the "Advanced Culture Fair Test" found on iqcomparisonsite.com that I just took today. Facts: I scored 29 raw (out of 36) IQ 146 or 99.9%ile, compared to my 130 or 98%ile raw 21 that I scored when I took the test over a year ago.
>
> ...For comparison to other fluid measures, this result is 3 points higher than my Get-gamma score and 2 points higher than my GIGI certified and 13 points higher than my iqtestdk result which lands in the same place every time I take it (last time I took it was less than a month ago). My current DNB level averages 8+ over multiple (10-20) sessions.

milestones [later reported](http://groups.google.com/group/brain-training/browse_frm/thread/f0c0a9cd8cd0ba0a):

> Last night I retook the iqtest.org.uk and scored higher on a second try than I did a few months ago -- 145 up from 133.  This could be due to 1. consistent quad back practice 2. being back on creatine as I have been for the last month 3. Omega 3/epa/fish oil 4. just a normal swing in scores due to other factors, including familiarity with the items. Or, of course, maybe some combination of the above

[Lachlan Jones](http://groups.google.com/group/brain-training/browse_frm/thread/da01159674cc5af0):

> Hey guys I've been using brain workshop (Dual N back) for about 2 months now and would like to report an increase in IQ from 124 to 132 (on professionally administered IQ tests that were supervised) The IQ tests were separated by a period of about a year as well.

(It's a little unclear whether this was an improvement or not; the second score was on the [Cattell Culture Fair III](!Wikipedia) test, but Lachlan hasn't said what the first one was. Since different tests have different norms and what not, Lachlan's scores could actually be the same or declining.)

[Jan Pouchlý](http://groups.google.com/group/brain-training/browse_frm/thread/e60e1a0ae699f21d):

> 2008/06 DNB for cca 1 month, 1 - 2 hours a day, 5 times a week; after 2 weeks probably gain +8 points IQ (I think it was Wechsler IQ 140 administered by school psychologist and after 5 months Raven IQ 148 administered by some Mensa guy). No problems at all. Better dream recall.

Argumzio comments on Jan:

> The difference between (what I assume both are) WAIS-III and RAPM is fairly significant; the former is about 2.667 sigma (FSIQ), and the latter is just over 3 sigma. For those who wish to know, both are set with a standard deviation of 15.
>
> Keep in mind, however, that with WAIS-III you get the full treatment while with RAPM your fluid ability is assessed as in the original Jaeggi study, so Jan's performance on other factors may have depressed and concealed his (already) high Gf, or performance, capabilities. That's why it is paramount to use the same test, or a test that is essentially of the same design.

[MugginBuns](http://groups.google.com/group/brain-training/browse_frm/thread/69e49db27f3924f8) (gains may've been from practice, or from a 'feature selection'-based game MugginBuns is developing):

> "http://www.iqtest.dk/main.swf  - 126 3 months ago
>
> https://www.gigiassessment.com/shop/index.php  - 126 3 months ago
>
>
> http://www.iqtest.dk/main.swf  - 140 2 weeks ago
>
> http://www.cerebrals.org/wp/?page_id=44 - 137 yesterday"

[Min Mae](http://groups.google.com/group/brain-training/browse_frm/thread/3083e3a448bba61):

> Previously I had a RAPM  IQ test result was 112 by certified psychologist. In 2009 August I practiced DNB 2.5 hours a day for 20 days with time off two days, Saturday and Sunday. After 20 days I took RAPM test in my university  by certified psychologist. I got IQ gain was  12.1 points. In the test i was only able to answer more questions that related to changes in position of objects in the test (RAPM).
>
> At 2010/04/12 I started SNB(single N-Back, visual modality) with training time was same as on DNB training time for 20 days. IQ gain by RAPM test was no change.That time i also was able  to answer more questions that related to changes in position of objects in the  RAPM test.

[Colin Dickerman](http://groups.google.com/group/brain-training/browse_thread/thread/453bef518ddd535d) in the thread "IQ test in one month!":

> I took a free android IQ test (I'm computerless) and scored 123 about 4 months ago. I've started n-backing again and after 4 weeks of consistent effort, I average around 70% at dual 5 back...Okay, I jumped the gun a little bit and retook the test a day early. I scored 126...[My] N-back level stagnated in the mid 60% at 6-back.

[marleydelupa](https://groups.google.com/group/brain-training/browse_thread/thread/3d5a890354d6b207):

> Ok, so a few months ago the most i could get on the http://mensa.dk/iqtest/ was about 95,115-126. Well now its 136....with the standard deviation 24 of course...I got to admit that score was on one of my bad days, and I wasn't really focused, plus I didn't spend much time on the questions. Probably 2 months ago the highest was 126.

[StephenK](https://groups.google.com/group/brain-training/browse_thread/thread/e5cb4e26f5ff3ac9):

> Before n-backing, my IQ lay in the region between 109 and 120 (most online tests always put me in the 113-120 range, but the MENSA test only gave me a result of 109). I've probably completed 10 IQ tests over the last 3 years and my scores seem to be relatively consistent....So, I've spent about one and a half months on dual-n-back. I did the http://www.iqtest.dk/main.swf test and got an IQ of 123.

[genvirO](https://groups.google.com/group/brain-training/browse_thread/thread/7b9807e881f6b1a3):

> iqtest.dk (english) I first attempted this test more than 2 years ago where I obtained a score of between 110-115. I attempted this test again today where I achieved a score of 138.
>
> N-level: Well, as most of you may know, overtime I've very much just been rolling around in the mud of what BW has to offer, so because I haven't stayed in one country long enough to call it home, it's pretty much impossible for me to attribute my new 'world view' to one particular mode or another...DNB: 4-back - 8-back = Time taken to reach level, 10+ months Quad-n-back: 2-back - 6-back = Time taken to reach level, between 6-8 months Variable-arithmetic n-back: 3-back - 7-back = Time taken to reach level, between 3-4 months... [description of daily routine]

[Windt](http://groups.google.com/group/brain-training/browse_thread/thread/92a06b84a3f74b52):

> I've done about 40 half-hour sessions of dual n back and have made gains within the task-ie higher nback score. Personally I don't feel much smarter but I've noticed I read faster and can comprehend what I am reading at a faster speed as well. Previously I scored a 109, then after 40 sessions I scored at 122 on the Denmark Mensa IQ test. http://www.iqtest.dk/main.swf. My concern is that this supposed gain has not made a noticable improvement in my real-world intelligence, and that the denmark IQ test is unreliable...I recently took a Mensa puzzle brain-teaser and scored a 18/30, which seems fairly mediocre? I don't know..I was pretty stumped by some of the questions. Didn't make me feel too smart....Update: I took the same IQ test(Denmark Mensa) again and I scored a 126, 4 points higher than my previous score of 122. Between taking the tests I had practiced dual n back for about 14 half hour sessions.

[hypersenses](https://groups.google.com/group/brain-training/browse_thread/thread/43ca7f18d595304):

> I liked IQ tests, especially the iqtest.dk. I did it for the last time between 1-2 years ago. My score was 110, i'm pretty sure.
> I scored never higher than 110, but also not much lower than 110. Guess i had just average intelligence and i was feeling that way too. On the weschler test i did 3-4 years ago i scored 107. so if i formulate it correctly my fluid intelligence was in line with my general intelligence.
> Now i do it for 11 consequent days, just 25 minutes a day and mostly at this point 2-3-4 back.
> But after the third day i felt much more clarity and better ability to formulate things, because my memory seemed so much better. Take note that i'm really an extremely sensitive person, so that is probably the reason i felt it so quickly.
> Today i decided to do the iqtest.dk test again, because i was excited to do it and not wait till the 19th day. My expectation was a iq in the 110-115 range, but guess what i scored 126...A few minutes ago i ended the iqtest.dk again and scored 122. This means for me i'm approximately as smart as 12 days before, furthermore i think i shouldn't be any differences in raw intelligence.

He later reported [additional results](https://groups.google.com/d/msg/brain-training/sXRurbk_R6I/XRPVmAad0dwJ):

> i did the full wais-iii 12-13 months ago. i scored 111 on the POI, which i think is the best measure for gf (although not a pure measure, but more comprehensive than just matrices) This where the scores within the POI:
>
> - Picture completion 11
> - Block Design 10
> - Matrix Reasoning 15
>
> I trained 2.5 months from february to april 2012. Note that i am 21 years old (intelligence is in some degree malleable till 22/23 years old, right?) Well, i did the wais-iii again and have the results since a week.  My POI is now 125 and this is how it looks:
>
> - Picture completion 8 (-3)
> - Block Design 18 (+8)
> - Matrix reasoning 16 (+1)

[karthik bm](https://groups.google.com/d/msg/brain-training/N7u8uTp1Krs/vjHCHRnmJBwJ "Noticeable gains after N backing, meditation and juggling"):

> I started dual n backing about 5months ago.After training for first 2 months, I took an IQ test and Iqtest.dk. My score was in low 120s.(took the test multiple times and got almost the same score each and every time)
>
> For the next 3 months apart from n backing, I included meditation, image streaming and juggling into my schedule. Yesterday I took the same test at iqtest.dk and got the score as 133.

[friemelaar](https://groups.google.com/d/msg/brain-training/uGg1zj05A7w/7m8XX6O14tQJ):

> I first started playing DNB some 3 years ago, trying to play three rounds everyday (skipped at most 20% of the total days), regardless of the n-back level.
>
> - 2 years ago (highest consistent DNB: 4), I took my first MENSA test - and scored 130 (SD 24), top 10%.
> - 2 weeks ago (highest consistent DNB: 7), I took my second MENSA test - and scored 156 (SD 24), top 1%, so I am joining.

##### No improvement

Some have not:

> I took the Online Denmark IQ test again [after N-back training] and I got 140 (the same result) I took a standardized (and charged) online IQ test from www.iqtest.com and I got 134 (though it may be a bit higher because English is not my mother tongue) --[Crypto](http://groups.google.com/group/brain-training/browse_thread/thread/8af44f3b20df9904/c397c36355355d4c)

jttoto [reports](http://groups.google.com/group/brain-training/browse_thread/thread/448c86efa0b645d1) a null result:

> 6 months ago I posted my IQ on this site after taking the Mensa Norway test... [see IQ tests section] I scored a 135.  After 6 months of dual n-back, triple n-back, and quad n-back training, I took the same exact test.  I scored exactly the same, 135. Granted, I took 7 less minutes to complete the test, but this was due to familiarity of some of the questions. That being said, I have been seeing significant increases in my digit span and other WM gains, so while my aptitude on questions like the Raven's may not have increased, my memory has.

(It's worth noting that Jttoto's experience doesn't rule out an IQ increase of some sort, as the original 135 score was from an IQ test he took after at least 10 hours of n-backing over 5 days, according to an [earlier email](http://groups.google.com/group/brain-training/browse_thread/thread/448c86efa0b645d1); what it shows is that Jttoto didn't benefit or the benefits happened early on, or there's some other confounding factor. Test results can be very difficult to interpret.)

moe [writes](http://groups.google.com/group/brain-training/browse_thread/thread/7e859b3a7bfeb0d8):

> "After 6 months of training I decided to take the tri 52 again and there has been no improvement in intelligence (or should I say abstract reasoning ability), I'm still at 144 sd15 on that test.  My digit span has gone up a bit from 9 forward 8 reverse to between 10-12 forward and reverse depending on how I'm feeling. I'm still not sure if the improvements in digit span are genuine memory improvements or increased skill at chunking."

Jttoto further wrote in response to moe:

> "Yes, I've continued to train QnB myself (about 3-4 times a week). Based on the iqout.com test, if anything, I've gone down a little!. This is not surprising and probably not attributed to n-backing.  I'm at the age where cognitive decline begins and I was depressed that day.  At the same time, one would think I would see measurable gains by now."

- [darren](http://cognitivefun.net/talk/post/22963):

> "I've had pretty unimpressive findings. I've used Brain Workshop 4.4 for about four months, with about a half-hours use 4-6 days a week. I used Denmark IQ test and scored a 112 and after dnb I scored a 110.
>
> My max dnb level was 11. Hours and hours and no gain in iq."

[Mike](http://groups.google.com/group/brain-training/browse_frm/thread/dd799cde5c43321):

> "...here they are (in order of test taking): 119, 125, 125, 107, 153, 131, (and I would say between 125 and 131 was my real iq) from different online tests almost 2 years ago before starting n-backing. after two years (I took the same bunch of online iq tests 3 weeks ago before trying faster trials) I got: 126, 135, 124, 125. so there wasn't much of a change. but I had been playing n-back softly for a long time. I expected my iq to jump at least by 5 to 10 points, from what I felt in my life. then after my week of faster trials, I did this iq test: and I got 149. if anyone who already knows his iq wants to try it, I'd be curious to know if they also score higher than expected, at first try of course. I thought the 153 I once got was pure chance, but maybe it wasn't completely, and that would be cool."

[punter](https://groups.google.com/group/brain-training/browse_thread/thread/8fba10a29aba0270):

> "Been training for about 5 weeks now, 30 mins a day and made very quick progress initially, and now shuttling between n=7 and n=8 and occasionally reaching n=9 (when I set out, I begin with n=2 and the value of N for the next round depends on my performance in the round I just finished)...I took a few intelligence tests (mostly culture insensitive), and the scores have actually "DROPPED" some 3-4 percent. Although I guess that doesn't mean much because I took those tests towards the end of the day at work and was somewhat exhausted, but it sure as hell means that there is no increase in my intelligence either!!"

[Eggs](http://cognitivefun.net/talk/post/31097#rpy_31098):

> "I have used dual, single and combination n back regularly for almost 2 years and no positive results come from it. I have the exact same IQ as I have according to denmark IQ test. Not even a couple of points higher....Just to clarify, I have used n back, seen no improvement based on IQ tests or real-life benefits."

Keep in mind, that if IQ is improved, that doesn't necessarily mean anything unless one employs it to some end. It would be a shame to boost one's IQ through N-back, but never use it because one was too busy playing!

### Other effects

Between 2008 and 2011, I collected a number of anecdotal reports about the effects of n-backing; there are many other anecdotes out there, but the following are a good representation - for what they're worth[^anecdotes].

[^anecdotes]: Shipstead, Redick, & Engle 2012 mention an amusing study I hadn't heard of before:

    > Greenwald et al. (1991) provided a useful demonstration of the problems associated with subjective reports. Participants in this study received commercially produced audiotapes that contained subliminal messages intended to improve either self-esteem or memory. Unknown to the participants, half of the tapes that were designed to improve memory were relabeled "self-esteem" and vice versa. At a 5-week posttest, participants' scores on several standard measures of self-esteem and memory were improved, but this change was independent of the message and the label on the audiotape (i.e., participants showed across the board improvement). However, in response to simple questions regarding perceived effects, roughly 50% of participants reported experiencing improvements that were consistent with the label on the audiotape, while only 15% reported improvements in the opposite domain. The self-report measures were neither related to actual improvements in transfer task performance nor related to the content of the intervention. Instead, they were attributable to expectation of outcome.

Besides these collected reports, there is an ongoing [group survey](http://spreadsheets.google.com/viewform?formkey=dDdYbzhnOUFPTUd1ZERES1Q5TjJZd3c6MQ) ([spreadsheet results](https://spreadsheets.google.com/ccc?key=t7Xo8g9AOMGudDDKT9N2Yww)); n-backers are strongly encouraged to submit their datapoints & opinions there as well.

#### Benefits

- Ashirgo: "To be honest, I do not feel any obvious difference. There are moments in which I perceive a significant improvement, though, as well as particulars task which are much easier now."

    "I have also experienced better dream recalling, with all these reveries and other hallucinations included. I am more happier now than ever. I did doubt it would be ever possible! I am also more prone to get excited...Now people in my motherland are just boring to listen to. They speak too slow and seem as though it took them pains to express anything. I did not notice that after I had done my first ninety days of n-back, but now (after 2.5 months) it is just conspicuous."[ref](http://groups.google.com/group/brain-training/browse_thread/thread/1c44c7570cdb4a35)

     "My change of opinion^[from at 90 days seeing little effect, to 2.5 months later producing the second testament] can be easily attributed to the improvement of mood, in coincidence with the mere fact that the winter days have passed and now there is a bright and sunny Spring in my country"; when asked if the previous means Ashirgo attributes all the improvement to the weather, Ashirgo [replied](http://groups.google.com/group/brain-training/browse_thread/thread/8f3f840e05a90509): "Fortunately, I can attribute many changes to n-back, I can now handle various tasks with little effort and it takes me much less time in comparison with others (especially when I know what to do). Nevertheless, the main problem for me is that I am also occupied with few things that I suppose to be able to test my newly acquired potential, therefore I cannot say that 'changes' are explicit everywhere.

    On the other hand, I am starting to believe that any improvements (that one can expect) so smoothly and swiftly become a natural part of one's capabilities that it makes them hardly noticeable until some tests/measures are taken."
- chinmi04: "For me, it definitely has taught me how to focus. But I'm still not sure whether that has something to do with merely coming to realize the importance of focusing, or whether the program has really physically rewired my brain to focus better. In any case, it appears that I'm now faster at mental reasoning, creative thinking and speaking fluency. But again, the effects are not so clear as to completely eliminate any doubt regarding the connection with the n-back program."

    "I have been maintaining a personal blog on wordpress since 3 years ago. Average post per month : a little over 1. Then I started with dual-n-back at the end of november... number of posts in January : 7! (none are about n-back)"
- ArseneLupin: "Not much, yet, but I feel that I can easier get a hold of a discussion. The feeling is the same as when I am mastering a certain n-back in the game (a bit hard to explain)."
- John: "I feel much sharper since I started in the middle of last November...My productivity is much higher these days. I'm a non-fiction writer, so having a higher working memory and fluid intelligence directly leads to better (and faster) performance. It's amazing to see the stuff I produce today and compare it to before I began the Dual N-Back training. Also, I am simultaneously learning German, French and Spanish, and I'm certain this is helping me learn those languages faster."
- Ginkgo: "DN-Back has probably helped me with one of my hobbies."
- BamaDoc: "I note a subjective difference in recall.  There might be some increase in attention, but I certainly do notice a difference in recall.  It might be placebo, but I am convinced enough that I continue to find time to use the program."[ref](http://groups.google.com/group/brain-training/browse_thread/thread/f11ff07eb9eba1a4)
- karnautrahl: "Since November however, I began to read the Neuroscience book in more detail. I mentioned late December I think that I was finding I could understand more stuff. I've spent about £1000 on books since November. The large majority are books on the brain, source from Amazon reviews, reading lists and out of my own pirate list when I liked a book. I stopped Dual n Back in December, early. The benefits have stayed however. I tested this the other day, very easily going to 3 n back, which was mostly where I was before. I guess in a way I'm trying to say that for me, whilst the focus may have been on G increase and IQ etc, now the focus is on--what's *really* happened and what can I do with it. What I can do with it is choose to concentrate long enough to genuinely understand fairly technical in depth chapters on subjects often new to me."[ref](http://groups.google.com/group/brain-training/browse_thread/thread/7a674cf0305a6f5c) Karnautrahl writes more on his self-improvements in his thread ["Second lot of training started-and long term experience overall."](http://groups.google.com/group/brain-training/browse_thread/thread/9c233ef7c68b16dc), and [describes](http://groups.google.com/group/brain-training/browse_thread/thread/aa849686781add83) an incident in which though he stopped using DNB 3 months previously, he still dealt with a technical issue much faster and more effectively than he feels he would've before.
- negatron: "One perhaps coincidental thing I noticed is that dream recollection went up substantially. A good while after I stopped I developed an odd curiosity for what I previously considered unpleasant material, such as advanced mathematics. Never imagined I'd consider the thought of advanced calculus exciting. I began reading up on such subjects far more frequently than I used to. This was well after I've long forgotten about dual n-back so I find it hard to attribute it to a placebo effect, believing that I'm more adapted to this material. On the other hand I don't recall reading anything about motivational benefits to dual n-back training so I still consider this conjecture and perhaps an eventful coincidence just the same."[ref](http://groups.google.com/group/brain-training/browse_thread/thread/7a674cf0305a6f5c)
- sutur: "i didn't really notice any concrete changes in my thinking process, which probably, if existent, are rather hard to detect reliably anyway. one thing i did notice however is an increased sense of calmness. i used to move my legs around an awful lot while sitting which i now don't feel the urge to anymore. but of course this could be placebo or something else entirely. i also seem to be able to read text (in books or on screen) more fluently now with less danger of distraction. however, personally i am quite skeptic when people describe the changes they notice. changes in cognitive capacity are probably quite subtle, build up slowly and are hard to notice through introspection."[ref](http://groups.google.com/group/brain-training/browse_thread/thread/1c44c7570cdb4a35)
- astriaos: "By 'robust', I mean practically everything I do is qualitatively different from how I did things 30 days previous to the dual n-back training. For instance, in physics class I went from vaguely understanding most of the concepts covered in class to a mastery thorough enough that now my questions usually transcend the scope of the in-class and textbook material, routinely stupefying my physics teacher into longer-than-average pauses. It's the same experience for all of my classes. Somehow, I've learned more-than-I usually learn of physics/government/ etc. (all of my classes, and any topic in general) information from sources outside of class, and without what I consider significant effort. I feel like my learning speed has gone up by some factor greater than 1; I can follow longer arguments with greater precision; my vocabulary has improved; I can pay attention longer; my problem solving skills are significantly better... Really, it's amazing how much cognition depends on attention!"[ref](http://groups.google.com/group/brain-training/browse_thread/thread/1c44c7570cdb4a35)
- flashquartermaster reports N-back cured his [chronic fatigue syndrome](http://groups.google.com/group/brain-training/browse_thread/thread/84d227fee313b60a)?
- [UOChris1](http://groups.google.com/group/brain-training/browse_thread/thread/18eeddd23451f1f0): "Harry Kahne was said to have developed the ability to perform several tasks at one time involving no less the 16 different areas of the brain....Surprisingly, I am slowly developing the ability simultaneously perform quad combination 3-back while reciting the alphabet backwards.  The practice is very difficult and requires loads of concentration but I am experiencing perceivable gains in clarity of thought from one day of practice to the next whereas my gains from brainworkshop alone were not perceivable on a daily basis." UOChris1 [wrote](http://groups.google.com/group/brain-training/browse_frm/thread/183b75e34b11b3e1) of another mode: "Triple-N-Back at .5sec intervals and piano notes instead of letters has greatly improved my subjectively perceived fluidity of thought.  I am much more engaged in class, can read much quicker, and am coming up with many more creative solutions now than ever before.  I didn't notice the improvements as much when I was using slower intervals--I feel I make more decision cycles in a given amount of time before coming to a solution."
- [Pontus Granström](http://groups.google.com/group/brain-training/browse_thread/thread/5e7ac7e6432b0d77/7ec15b99a2a1a2f0) "I certainly feel calmer happier and more motivated after doing DNB, it has to do with the increase of dopamine receptors no doubt!"
- [Chris Warren](#hardcore) summarizes the results of his intensive practice (covered above): "For those that are curious, I noticed the largest change in my thought processes on Wednesday. My abilities were noticeably different, to the extent that, at some points, it was, well, startling. I've started getting used to the feeling, so I can't really compare my intelligence now vs. Wednesday. However, I'm completely confident that I've become smarter. Under the kind of stress I've put my brain through, I can't imagine a scenario where that *wouldn't* happen."

    "After the first couple days of training, I experienced a very rapid increase in intelligence. It suddenly became easier to think. I can't give you any hard evidence, since I didn't bother to take any tests before I started. However, I can give you this: when I woke up Wednesday morning, I felt the same as I did after the first time I tried n-back. Except the feeling was 10 times stronger, and my thinking was noticeably faster and more comprehensive."
- [Raman](http://groups.google.com/group/brain-training/browse_thread/thread/5e7ac7e6432b0d77/7ec15b99a2a1a2f0) reports an initial null result: "19 days with n-back are over... no subjective benefits as such. But I am aware at what point I am comfortable or not. e.g. y'day playing the game was effortless, and today my brain felt sort of sticky, the sequence was just not sticking in my brain. very strange what a few hours can do."
- [iwan tulijef](http://groups.google.com/group/brain-training/browse_thread/thread/5e7ac7e6432b0d77/9f3b791ee0fb1c38) says that "Long time ago I was diagnosed Adhd [sic] and for long time I took meds and this training helped me to reduce my meds nearly to zero, compared with the doses I took before. Unfortunately this haven't fixed the whole thing. But what I noticed was, hmm... those things are very difficult to describe.... that time by time I got more control about my mental life. Obvious effects in social matters were e.g. that I could follow conversations better and behave more naturally. In my education matters, e.g. that I understood maths proofs better. There are a lot of details. Interesting was, as these issues are, to understate it a bit, not unimportant for me, that in the beginning when I remarked changes, I got a bit euphoric, so the first effects of n-back feeled like the strongest." and warns us that "It's very difficult and very questionable to take objective informations out of subjective self evaluation." (Iwan trained for 3-4 months, 20 rounds a day in the morning & evening.)
- [jttoto](http://groups.google.com/group/brain-training/browse_thread/thread/448c86efa0b645d1) saw no gain on an IQ test, but thinks he's benefited anyway: "My friends have always called me inattentive and absent-minded, but since playing n-back no one has called me that for a while. I now never forget where I park my car, when I used to do that nearly every other day. I feel more attentive. Even if my ability to solve problems hasn't improved, the gains in my memory are real and measurable."
- [reece](http://groups.google.com/group/brain-training/browse_thread/thread/4a0b81bb5f72647f): "Not that I've noticed [an improvement in [lateral thinking](!Wikipedia)]. I have noticed an improvement in my working memory however -- seems easier to juggle a few ideas in my head at the same time which presumably the quad-n-back has helped with."
    "I recently noticed that it appears to have made me better at playing ping pong and tetris. Oddly enough however, it doesn't appear to have improved my reaction time...Working memory has improved, however other things I've always struggled with such as uncued long term memory recall have not... I'm still very absent-minded and believe n-back has made me more easily distractable (lowered latent inhibition?), although to be fair, I may have brought this on myself by playing quad n-back and this was not something I noticed when only playing dual n-back. I seem to be able to get by on about one hour less sleep per night and perform better cognitively when sleep deprived. Dream recall has increased significantly as has lucid dreaming. I do take a few nootropics, however I've been taking the same ones for years...Verbal fluency appears to have improved, proper spelling and punctuation are things I've always struggled with and do not appear to have ameliorated resultant from n-back training." ([Poll](http://groups.google.com/group/brain-training/browse_thread/thread/3008683d4b314f6/5e833c4c0df9fb9b))
    "In my experience with dual and multimodal n-back, the benefits I've most observed have been increased multitasking ability and increased concentration in the presence of distractions. For me, the benefits of n-back training are most apparent on days I don't take my ADHD medication. I have been training DNB with position-sound and color-image modes lately. I used QNB for several months in the past, however I (subjectively) believe DNB is giving me the most benefit." ([ADHD](http://groups.google.com/group/brain-training/browse_frm/thread/efec0cfe9f67e91f) thread)
- [Michael Campbell](http://groups.google.com/group/brain-training/browse_thread/thread/3008683d4b314f6/5e833c4c0df9fb9b): "Something very minor to some, but was good for me; I'm able to concentrate while reading a lot more than I have been able to in the past."
- [exigentsky](http://groups.google.com/group/brain-training/browse_thread/thread/3008683d4b314f6/5e833c4c0df9fb9b): "I've seen improvements in executive function and motivation. After DNB, I am more inclined to study and complete long pending items. However, there is a confounding variable. I don't usually do DNB when in an unhealthy state of mind (for example, with little sleep and extremely high stress). Still, I believe that I can attribute some of the effects only to DNB.

    In terms of working memory and other cognitive measures, I'm not sure. I don't notice anything dramatic but also haven't stuck to a DNB regime for more than a few weeks."
- [cev](http://groups.google.com/group/brain-training/browse_thread/thread/3008683d4b314f6/5e833c4c0df9fb9b): "I think I've put my finger on a particular benefit of dnb training: it seems to help my brain's 'internal clock' - I am better able to order my thoughts in time.

    DNB has also helped my foosball (!) playing: at a high level the game involves complex strings of motor movements and since I've been training, I've found that my coordination of these movements has greatly improved despite no longer practising."

- [erm](http://lesswrong.com/lw/1sm/akrasia_tactics_review/1nl4): "I can rely on this to drastically reduce anxiety, flightiness, improve concentration. It also seems to whet my appetite for intellectual work and increase purposefulness across the board."
- [Tofu](http://groups.google.com/group/brain-training/browse_thread/thread/3af97e7c14585851), after a year of n-backing:
     "N-back training may have somehow improved my verbal intelligence, but since verbal intelligence is a form of crystallized intelligence and training working memory is supposed to primarily improve fluid intelligence, it probably didn't. My score on the verbal subtest went up and then down which would make no sense if it did have any influence...Since my IQ score increased from the first test to the second test, and stayed the same from the second test to the third test it could be possibly that working memory only contributes to IQ up to a certain point.  All in all, I feel more inclined to say that n-back training has only a little if any effect on IQ though which is why reason I'm probably going to stop doing the n-back training.

     On a more positive note, since I started n-back training I have noticed better concentration which I had a serious problem with before.  In general, I feel like I think more clearly and I at least feel like I've become smarter too. I've reached a pretty high level in n-back and any gains I've made in the last month or two have been small, so I think I've reached a long-term plateau which is another reason for me to stop the training.  From my experience when I stop the n-back training for a month or two and return to n-back training I still perform at the same level anyway.  It seems like the effects from training are going to last a while which is also good news. Overall, I feel like the n-back training was worth it but if I had it to do over I would have probably stopped after a couple of months."
- [kriegerlie](http://groups.google.com/group/brain-training/browse_thread/thread/e5ed78ad41d397cb): "i've defnitely had some benefit, like pontus said, dunno about being smarter, but my focus is incredible now. I can do what i thought I could never do, purely because I can focus more. Placebo or not. It's a definite effect."
- [Rotem](http://groups.google.com/group/brain-training/browse_thread/thread/e5ed78ad41d397cb): "DNB works, It's one of the best investments I made in my life. I have much less anxiety ( I suffered from GAD my life was a nightmare), more confidence and I guarantee more intelligence - I can feel it..."
- [chortly](http://www.metafilter.com/91797/working-working-memory-with-dual-nback#3081757): "For a while I imagined that my working memory muscles were indeed strengthening, the main sensation being that I could retain the various threads of a complicated conversation better as they dangled and were forgotten by the other conversationalists. But that was probably just wishful thinking. Because it's boring and difficult, I haven't stuck with it, though I keep intending to."
- [JHarris](http://www.metafilter.com/91797/working-working-memory-with-dual-nback#3108845): "I've been working with the dual n-back program for a bit of time now. Improvement is slow, but seems to be happening; I just had a 68% run at dual 3-back. Observations like this are not really scientific and hellishly subject to bias, but I think I may be noticing it slightly easier to think effectively."
- [Neurohacker](http://groups.google.com/group/brain-training/browse_frm/thread/efec0cfe9f67e91f) (in a thread on [ADHD](!Wikipedia)): "I'm definitely finding it helpful, even if it's just giving me some practice at focusing...as a complementary strategy [to medication], it's certainly working wonders."
- [iwan tulijef](http://groups.google.com/group/brain-training/browse_frm/thread/efec0cfe9f67e91f): "n-Back helped me a lot. Especially in the beginning when I started with DNB, the effect was astounding. I got much faster in understanding written and spoken words.In the beginning I think the function of my working memory was really bad. What then happened is that I got habituated to the effect and the increases were smaller, so
noticing improvements got more difficult."
- [Arkanj3l](http://groups.google.com/group/brain-training/browse_frm/thread/2f2ea6c33bc04e53): "On a side, I really enjoy the lucid feeling I get after an hour of n-back. I start to look at things and ideas seem to flow into my head very vividly (I've made some of my best Lego creations after an n-back session :p)."
- [Michael Logan](http://www.logan-counseling.com/rockford-counseling-discoveries-brain-training-to-improve-memory-boosts-fluid-intelligence-2/): "...and learned very, very quickly that I had a short term memory and attention issue. The dual n back task laughed at me, but I vowed to overcome my inattention and short term memory issues, and within a few practices, I noticed an improvement not only in my scores on the computerized game, but in session with my clients....So [Mind Sparke](http://www.mindsparke.com/) does provide that kind of novel learning challenge.   I have not taken an IQ test, but I do believe the use of the tool is helping me build cognitive reserve for the later stages of my life."
- [milestones](http://groups.google.com/group/brain-training/browse_frm/thread/75639eb9ec430d09/f43418c779ad241d): "I'm grateful for the gains I seemed to have received from training dual n back. I used to be extremely forgetful with remembering where I put things and now it's very easy to retrace steps and recall where I placed xyz item. As far as IQ tests go, I did see a gain on a well designed (untimed) culture fair test of about 1 standard deviation after training one DNB on and off for close 2 years. (Other tests with lower ceilings, however, showed no or marginal gains)." A [later post](http://groups.google.com/group/brain-training/browse_frm/thread/f0c0a9cd8cd0ba0a/8d224603737f4bae): "The gains I'm seeing are: faster encoding speed; faster and more accurate retrieval of data from long term memory; as well as an increase in data-sequencing speed (the latter is a relative weakness of mine that now seems to have been helped by consistent quad-back training -- though I've not tested any transfer so this is subjective). Also, though my fluid intelligence has probably ceased gaining, it seems I'm functioning at higher bands of ability far more regularly -- even when I'm tired or sluggish."
- [Lachlan Jones](http://groups.google.com/group/brain-training/browse_frm/thread/da01159674cc5af0) wrote, after a before/after IQ report, "The most significant real word application for me has been improvements in my piano playing. I am a pianist and can report significant improvements in my sight reading and the rate at which I learn new pieces."
- [unfunf](http://www.rohitab.com/discuss/topic/36820-brain-workshops-dual-n-back/page__view__findpost__p__10079255): "While I haven't taken an IQ test to see if it has garnered any IQ improvement, I can say I started off at dual 4-back only 2 weeks ago and I am now nearing dual 6-back. I can also attest to a pretty large working memory improvement, beyond what I would call placebo (the effects of which I am very well aware). Even if it is not very effective, I still say this game is fun."
- [NeuroGuy](http://www.longecity.org/forum/topic/40615-a-new-forum-for-spaced-repetition/page__p__436525#entry436525): "Dual-N-Back has subjectively done more for me in less then two weeks than any single nootropic, can hardly imagine it combined with spaced-repepition."
- [TeCNoYoTTa](http://groups.google.com/group/brain-training/browse_frm/thread/358765b641a1a43c/913ebdab64728332): "I also want to report that after training on DNB I found that I am dreaming almost every day...by the way I remember that this effect was not directly after training...unfortunately I stopped using DNB from about 2 months or something like that and now I dream less"
- [dimecoin](http://www.reddit.com/r/cogsci/comments/er2jl/rcogsci_how_do_you_keep_your_brain_sharp/c1a90cp): "I make no claims, other than anecdotal - in that it seems to relax me and able to handle stress better when I do it regularly."
- [Arbo Arba](http://groups.google.com/group/brain-training/browse_frm/thread/183b75e34b11b3e1): "I did find a lot of changes come to my brain and personality, but I'm not sure if it's from improving WM or if it's just from spending a lot of time in an alpha-wave dominant state. I think it's being in a prolonged alpha-brain wave dominant state, tbh, because I found that when I was younger and took up heavy reading projects I felt the same improvements--that is, having more focus, being able to 'hear' myself think very distinctly to the point where I could compose poems/emails in my head without effort. I don't know why this is, but it happens so much with me that I can't doubt that there is a real effect on my personality and default mental state when I'm doing 'intellectual' things."
- [Akiyama Shinichi](http://groups.google.com/group/brain-training/browse_thread/thread/d32fd7b43efb31f3): "I train 3 times a day and every sesion last about 20 minutes. After a month a went to my chess club and completely crash players who was at completely different level. I chose one of the strongest player (at my
level of course), because he was able to tell me if I've realy improved. Then I had to reveal my secret, and after month I tell how it works for them. I notice that I improve not only in chess. I'm a piano player and it's really challenging. I was learning very slowly, but yesterday my teacher told me that in two weeks I learnt much more than in the last 2 months. He was even suspecting me that I take lectures from other teacher, not only from him. And that's not all. I'm a student and one a month every of us have to prepare presentation on some topic. A few days ago was my turn. I didn't notice it by myself, but one of my friend told me that I was very well-prepared, because I stopped making that annoying sound like "umm", "yyyy" when I was thinking what to say. When I was performing my presentation I don't have to think what to say next because I'd already know and didn't have to think about it much."
- [whoisbambam](https://groups.google.com/group/brain-training/browse_thread/thread/88b25686081cd3a8): "My mind feels faster. I also seem to have less mental fatigue during studying, nbacking, etc. I am more confident. I am confident that my memory has improved independent of nbacking (what they call 'far transfer' effect). I am not saying it is a HUGE difference. I can not say the same is true for any supplement i have taken other than possibly some small effect with magnesium l-threonate which also seems to make me 'less mentally tired' in particular, interestingly."
- [Christopher Działo](https://groups.google.com/group/brain-training/browse_thread/thread/dbf15a5180d39814): "I've trained with n-back for several months and have noticed a profound ability to sight read music and locate the notes, my speed and overall dexterity has drastically increased and I shall continue to n-back and grow my musical talent."

#### No benefits

- Confuzedd: "[asked if felt 'sharper']: Nothing."
- Chris: "One thing I have noticed is the recollection of a number of very unpleasant images in dreams. Specifically, images of bodily disease, mutilation, injury and post-mortem decomposition. I find it difficult to believe it's just a coincidence, because I can't remember when I last had such a dream, and I've had maybe half a dozen since I started dual n-back. But perhaps it's simply owing to better recall."[ref](http://groups.google.com/group/brain-training/browse_thread/thread/7a674cf0305a6f5c)
- [Pheonexia](http://groups.google.com/group/brain-training/browse_thread/thread/8f3f840e05a90509): "now I'm at 6-back and am consistently between 50 and 80% accurate....All that said, I have NOT noticed any differences in my mental capacity, intelligence, daily life, or even ability to remember things that just happened. I still sometimes forget people's names right after they tell me them. I'm going to keep training though, because just because I haven't consciously noticed these things, I have faith in scientific studies, so with enough training hopefully I'll yield some positive benefits."
- [TheQ17](http://groups.google.com/group/brain-training/browse_thread/thread/d1eb1dbfb5cce800) reports little to no benefit: "At any rate, I don't feel studying is any easier although it wasn't really difficult to begin with for me. Perhaps I'll give it another go over break and report back. My goal originally was to get to P5B before adding a second sound stimulus making a Sextuple Nback but I don't know if Shamanu made an updated version to make that any easier. I'm also kind of on the fence about the effect on the depth of training. It may have been more beneficial to do higher N levels instead of more stimuli."
- [Jonathan Graehl](http://lesswrong.com/lw/2l6/taking_ideas_seriously/2fkb): "I can do dual 4-back with 95%+ accuracy and 5-back with 60%, and I've likely plateaued (naturally, my skill rapidly improved at first). I enjoy it as "practice focusing on something", but haven't noticed any evidence of any general improvement in memory or other mental abilities."
- [Will Newsome](http://lesswrong.com/lw/2l6/taking_ideas_seriously/2fkc): "After doing 100 trials of dual N back stretched over a week (mostly 4 back) I noticed that I felt slightly more conscious: my emotions were more salient, I enjoyed simple things more, and I just felt generally more alive. There were tons of free variables for me, though, so I doubt causation."
- [steven0461](http://lesswrong.com/lw/2l6/taking_ideas_seriously/2i7b): "I did maybe 10-15 half-hour sessions of mostly D5B-D6B last year over the course of a few weeks and didn't notice any effects."
- [EggplantWizard](http://www.reddit.com/r/neuro/comments/81vkd/worried_about_your_working_memory_capacity/c080v66) (D3B->D10B): "I would say that there has been some form of improvement -- though it's not clear if the improvement is task-specific. I haven't noticed any significant difference in my day to day life, but (to be immodest in the name of efficiency for a moment) I had a very good memory to begin with, and I would say strong fluid intelligence. It's possible that people starting from positions of lower fluid intelligence would see a more pronounced benefit."
- [Matt](http://marginalrevolution.com/marginalrevolution/2011/06/does-this-reliably-increase-your-fluid-intelligence.html#comment-body-157452569): "...I've certainly improved at n-back type tasks, I can't say that I've noticed any improvement while handling real life problems. I think the effects do generalize - I'm quite good at highly g-loaded tasks like the [PASAT](!Wikipedia) now, even without much practice - but the range of tasks which are subject to improvement from n-backing seems limited. I'm better at tasks involving mental updating, but my short term memory has only slightly improved, if at all. I don't have an accurate way of measuring my change in Gf (or g), as most of the fluid reasoning tasks available online use the same/similar rule patterns or aren't accurately normed, but as I said before, my real life problem solving abilities have not subjectively improved..."
- [Jelani Sims](https://groups.google.com/group/brain-training/browse_thread/thread/0217449d6e1cedcd): "I've been doing DNB since the group started, I haven't noticed anything out of the ordinary in terms of cognition. But I never took a before and after IQ test and I haven't really done anything that I found mentally difficult before. So it's very hard for me to gauge mental improvements with nothing for me to base it on. I also changed my diet, started mindfulness meditation and exercising around the same time I started DNB, in an overall attempt to delay brain decline. Making it even more difficult to attribute anything directly to DNB. What I can say is I have been stuck on 12 for 4 months now, each level was increasingly more difficult to pass and 12 seems to be some sort of temporary plateau."
- [argumzio](https://groups.google.com/group/brain-training/browse_thread/thread/fcaf852fb9f09466): "I've seen no net benefit. Compared to improved nutrition, exercise, sleeping, and the occasional nootropic (e.g., Piracetam, Alpha GPC, CDP Citicoline, Resveratrol, Kre-Alkalyn & Creatine Monohydrate, etc.), DNB did nothing. However, in terms of subjectively improved focus (counting the near-certain possibility that the aforementioned changes also influenced it), QNB* did the most for me, that is, allowing me to absorb information for longer periods of time and maintain this effort much later into the evening while mitigating the deleterious effects of fatigue and allowing me to feel rested after unusually shorter periods of sleep."

#### Creativity

One of the worries occasionally cited is that DNB training mostly serves to increase one's focus on the task one is thinking about. Which is great in most contexts but, the fear goes, the ability to focus on one thing is the ability to exclude ('inhibit') thoughts on all other topics - which is crucial to creativity. Working memory and ability to shift attention has a strong correlation with being able to solve [insight problem](!Wikipedia)s with lateral thinking, but as with the WM-IQ link, that doesn't say what happens when one intervenes on one side of the correlation (correlation is not causation):

> Individuals may have difficulty in keeping in mind alternatives because multiple possibilities can exceed their working memory capacity (Byrne, 2005; Johnson-Laird and Byrne, 1991; 2002). They also need to be able to switch their attention between the alternative possibilities to reach a solution. On this account, key component skills required in insight problem solving include attention switching and working memory skills....Attention and working memory may be crucial for different aspects of successful insight problem solving. Planning a number of moves in advance may be important to solve insight problems such as the well-known nine-dot problem (Chronicle, Ormerod and MacGregor, 2001). Attention may play a role in helping people to decide what elements of a problem to focus on or in helping them to direct the search for relevant information internally and externally.
>
> ...Individuals who are good at solving insight problems are also good at switching attention. Correct performance on the insight problems was associated with correct performance on the visual elevator task (r=.515, p<.01). Correct performance on the insight problems was associated with correct performance on the plus-minus problems (r=-.511, n=32, p<.001)...Consistent with this account individuals who are better at storing and processing information in working memory are better at solving insight problems. [correlation with problem score: r=.39 for digit span, r=.511 for sentence span]^[["Attention and Working Memory in Insight Problem-Solving"](http://www.psych.unito.it/csc/cogsci05/frame/poster/2/f285-murray.pdf), Murray 2011. The study does not seem to have controlled for IQ, so it's hard to say whether the WM/attention are responsible for increased performance or not.]

The major piece of experimental evidence is [Takeuchi 2011](#takeuchi-2011) & Vartanian 2013, treated at length in the following subsection and well worth consideration; the rest of this section will discuss other lines of evidence.

[Dopamine](!Wikipedia) is related to changes caused by n-backing (see the [McNab](#whats-some-relevant-research) receptor study & for a general review, [Söderqvist et al 2011](/docs/dnb/2011-soderqvist.pdf "Dopamine, Working Memory, and Training Induced Plasticity: Implications for Developmental Research")), and increase in dopamine has been shown to cause a narrowing of focus/associations in [priming](!Wikipedia "Priming (psychology)") tasks[^ldopa]. There are other related correlations on this; for example, Cassimjee 2010^[["Temperament and character correlates of neuropsychological performance"](http://jtoomim.org/brain-training/n-back%20and%20novelty%20seeking.pdf), June 2010, _Psychological Society of South Africa_] report that "...the temperament dimension of [Novelty Seeking](!Wikipedia) was inversely related to performance accuracy on the LNB2 (Letter-N-Back)." But as ever, [correlation is not causation](!Wikipedia); this result might not mean anything about someone deliberately increasing performance accuracy by practice - we might take it to mean just that narrow uninterested people had a small advantage at n-backing when they first began. Cassimjee 2010 cites 2 other studies suggesting what this correlation means: "...participants with higher impulsivity may lack the attentional resources to retain critical information and inhibit irrelevant information. The activation of reactive control, which is a system that monitors, modulates and regulates reactive aspects of temperament, is inhibited in individuals high in novelty seeking..." This suggests the performance difference is a weakness that can be strengthened, not to a fundamental trade-off.

[^ldopa]: From Sanderberg/Bostrom 2006:

     > Giving [L-dopa](!Wikipedia), a dopamine precursor, to healthy volunteers did not affect direct semantic priming (faster recognition of words directly semantically related to a previous word, such as "black-white") but did inhibit indirect priming (faster recognition of more semantically distant words, such as "summer-snow") (Kischka et al. 1996). This was interpreted by the authors of the study as dopamine inhibiting the spread of activation within the semantic network, that is, a focusing on the task.

Reports from n-backers are mixed. One negative report is from [john21012101](http://groups.google.com/group/brain-training/browse_thread/thread/f1eefb13c1658b24/e2490a2f19cb4abb):

> I've done the dual n-back task avidly for over a month and while I find it makes me mentally sharper, that comes a high cost - the loss of creativity and lateral thinking.  In fact, I experience what is called as severe directed attention fatigue (see `www.troutfoot.com/attn/dafintro.html`).
>
> ...and even short booster sessions severely impair creativity to the point that one becomes very mentally flat, single-minded, and I'd even say zombie-ish.

Ashirgo, chinmi04, & putomayo begged to differ in the same thread, with biped plumping for a null result.

There are some theoretical reasons to believe DNB isn't causing gains at the expense of creativity, as there is that Jaeggi study showing _Gf_ gains, and _Gf_ is mildly correlated with creativity, according to [exigentsky](http://groups.google.com/group/brain-training/browse_thread/thread/f1eefb13c1658b24/8c267394bb1081cc):

> "Furthermore, if the preliminary results hold and dual-n-back actually increased Gf, it should actually contribute to creativity for most people. After all, studies have shown that creativity (according to standard tests) and IQ are significantly correlated to a certain point (~120 on most). While both tests are imperfect and incomplete, they do give a general picture.
>
> I have not felt a decrease in my creativity and am skeptical of the idea that dual-n-back harms it. If the purported mechanism is increasing [latent inhibition](!Wikipedia), that would be an even bigger breakthrough than increasing IQ. The former is still largely considered immutable."

[Vlad](http://groups.google.com/group/brain-training/browse_thread/thread/1b0b307f9605ba4e/47d0a2fcc41ae4d3) has some more details on those correlations:

> "Last but not least, there was this research "Relationship of intelligence and creativity in gifted and non-gifted students", which I studied because of this today, and they found *positive* correlation IQ vs verbal and figural creative processes (fluency, flexibility, object designing, specific traits, insight...). And this mild correlation (of 0.3 - 0.5), did not differ for different IQ levels (higher IQs had mild higher creativity, lower IQs had mild lower creativity - always mild relationship, so exceptions too, but in general more IQ meant more creativity)."

On the other hand, Vlad [also points out](http://groups.google.com/group/brain-training/browse_thread/thread/1b0b307f9605ba4e?hl=en) that:

> "...there are few theories how WM works, and one of the most explaining is, that WM and attention are tied closely together (Ash always emphasizes this and he is right :). This should work through the fact, that higher WM means more sources for inhibition of distraction. So, the more WM, the better you can concentrate. They tested this with cocktail party effect: in general, only 33% of persons catch their name from irrelevant background noise, while concentrating on some task. Now they found, that only 20% of high WM people caught their name, but 65% of low WM. On the other side, contemporary researches sometimes differ between WM, STM, primary / secondary WM, even LTM... But the point is, attention works at least partly as a filter, and it gets better with higher WM.
>
> Now the issue with creativity. I find this interesting, because I think somebody here worried already about being subjectively less creative than before BW training, and I got this feeling few times too.
>
> ...Every creator must deeply concentrate on his work. Maybe there are different kinds of creativity: "ADHD" creativity, meaningful creativity, brainstorming creativity, appreciation of art, and so on.
>
> Btw after training dnb, I got this interest in art - I downloaded lots of classical and other artistic pictures (never before), and really enjoyed choosing which I like. Or have you ever seen "the hours"? I fell in love with that movie and even started to read things from virginia woolf"

As well, Pheonexia points out that [McNab 2008](http://www.klingberglab.se/pub/McNab2008.pdf "Common and unique components of inhibition and working memory: An fMRI, within-subjects investigation") & McNab 2009 demonstrated increases in various things related to [dopamine](!Wikipedia) because of DNB, and that there is one study that ["Dopamine agonists disrupt visual latent inhibition in normal males using a within-subject paradigm"](/docs/dnb/2009-swerdlow.pdf "Swerdlow et al 2009").

##### Takeuchi 2011

["Working Memory Training Using Mental Calculation Impacts Regional Gray Matter of the Frontal and Parietal Regions"](http://www.plosone.org/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0023175&representation=PDF), Takeuchi et al 2011:

> Training working memory (WM) improves performance on untrained cognitive tasks and alters functional activity. However, WM training's effects on gray matter morphology and a wide range of cognitive tasks are still unknown. We investigated this issue using voxel-based morphometry (VBM), various psychological measures, such as non-trained WM tasks and a creativity task, and intensive adaptive training of WM using mental calculations (IATWMMC), all of which are typical WM tasks. IATWMMC was associated with reduced regional gray matter volume in the bilateral fronto-parietal regions and the left superior temporal gyrus. It improved verbal letter span and complex arithmetic ability, but deteriorated creativity. These results confirm the training-induced plasticity in psychological mechanisms and the plasticity of gray matter structures in regions that have been assumed to be under strong genetic control.

Takeuchi 2011 has many points of interest:

- these subjects are really high quality students and grad students - which is why a number of them hit the RAPM ceiling (!); and it's implied they are all [Tohoku University](!Wikipedia) students. Tohoku isn't Tokyo U, but it's still really good, Wikipedia telling me "It is the third oldest Imperial University in Japan and is a member of the National Seven Universities. It is considered as one of the top universities in Japan, and one of the top 50 universities in the world."
- While high quality, there aren't that many of them; Jaeggi 2008 had 35 subjects doing WM training, while this one has 18 doing the adaptive and another 18 doing non-adaptive, and the last of the 55 were pure control. So a little more than half as many; this is reflected in some of the weak results, so while rather disturbing, this isn't a definitive refutation or anything.
- the WM task subjects did *not* see any relative IQ gains, or much of a gain at all; the IATWMMC (adaptive arithmetic) group went from 27.3±1 to 31.3±0.7, and the placebo group (non-adaptive arithmetic) went from 29.1±0.9 to 32.0±0.8. This doesn't show any noticeable difference, the authors describing the IQ as 'probably void'.
- 20 hours of training is more than twice as much training as Jaeggi 2008's longest group^[Jaeggi 2008's notes say the daily training was ~25 minutes; the longest group was 19 days; $\frac{25 \times 19}{60} = 7.9$ hours.], so one should not dismiss this solely on the grounds 'if only they had trained more'
- adaptive arithmetic doesn't seem like much of a WM task; they did do some n-backing (mentioned briefly) during the fMRI pre/post, but not clear why they chose arithmetic over n-back. On the other hand, don't many n-backers use the arithmetic modes...?
- the adaptiveness is *really* important; they say the group doing non-adaptive arithmetic was the same as the no-intervention group on every measure! Including 'a complex arithmetic task'
- one of the key quotes:

    > Behavioral results comparing the combined control group, and the IATWMMC group showed a significantly larger pre- to post- test increase for performance of a complex arithmetic task (P = 0.049), for performance of the letter span task (P = 0.002), and for reverse Stroop interference (P = 0.008) in the IATWMMC group. The IATWMMC group showed a significantly larger pre- to post- test decrease in creativity test performance (P = 0.007) (for all the results of the psychological measures, see Table 1). Also the IATWMMC group showed a statistical trend of increase in the mental rotation task (P = 0.064).
- About the only good news for n-backers is that the results were not huge enough to easily survive multiple-comparison correction

    > We performed several psychological tests and did not correct for the number of comparisons between statistical tests, as is almost always the case with this kind of study. When corrected using the [Bonferroni correction](!Wikipedia), even after removing the probably void tests (RAPM and WAIS arithmetic), the statistical value for the effect of IATWMMC on the creativity tests marginally surpassed the threshold of P = 0.05 (P = 0.06). Thus, the results should be interpreted with caution until replicated.

##### Vartanian 2013

["Working Memory Training Is Associated with Lower Prefrontal Cortex Activation in a Divergent Thinking Task"](/docs/dnb/2013-vartanian.pdf); emphasis added:

> Working memory (WM) training has been shown to lead to improvements in WM capacity and fluid intelligence. Given that divergent thinking loads on WM and fluid intelligence, we tested the hypothesis that WM training would improve performance and moderate neural function in the Alternate Uses Task (AUT)-a classic test of divergent thinking. We tested this hypothesis by administering the AUT in the functional magnetic resonance imaging scanner following a short regimen of WM training (experimental condition), or engagement in a choice reaction time task not expected to engage WM (active control condition). Participants in the experimental group exhibited significant improvement in performance in the WM task as a function of training, as well as a significant gain in fluid intelligence. Although *the two groups did not differ in their performance on the AUT*, activation was significantly lower in the experimental group in ventrolateral prefrontal and dorsolateral prefrontal cortex-two brain regions known to play dissociable and critical roles in divergent thinking. Furthermore, gain in fluid intelligence mediated the effect of training on brain activation in ventrolateral prefrontal cortex. These results indicate that a short regimen of WM training is associated with lower prefrontal activation - a marker of neural efficiency - in divergent thinking.

## Non-IQ or non-DNB gains

This section is for studies that tested non-DNB WM interventions on IQ, or DNB interventions on non-IQ properties, and miscellaneous.

<!--
A recent review (Morrison & Chein, 2011) of the broader WM training literature with young adult subjects detailed: (a) four studies reporting significant transfer to reasoning and intelligence measures (Klingberg, Forssberg, & Westerberg, 2002; JBJP 2008; Olesen, Westerberg, & Klingberg, 2004; Westerberg & Klingberg, 2007); (b) three published studies reporting no significant transfer to reasoning and intelligence measures (Chein & Morrison, 2010; Dahlin et al., 2008; Owen et al., 2010); and (c) one study reporting significant transfer to some intelligence measures but not others (Schmiedek, Lovden, & Lindenberger, 2010). Two of the significant transfer studies in the review (Klingberg et al., 2002; Olesen et al., 2004) had training group samples of n = 4 and 3, respectively. The subjects in Olesen et al. (2004) were the same as those in Westerberg and Klingberg (2007; T. Klingberg, personal communication, February 14, 2010). Note that the positive intelligence transfer observed in JSBSJP (2010), and the lack of transfer observed in Seidler et al. (2010), were not included in Morrison and Chein's review. In addition, Morrison and Chein's (2011) assessment of training benefits may also have been unwittingly biased because of the file-drawer problem (Rosenthal, 1979), in which non-significant transfer results such as those described in the current research are less likely to be published.
-->

### Chein 2010

["Expanding the mind's workspace: Training and transfer effects with a complex working memory span task"](/docs/dnb/2010-chein.pdf) ([FLOSS implementation](https://github.com/BXQ/CWM)); from the introduction:

> In the present study, a novel working memory (WM) training paradigm was used to test the malleability of WM capacity and to determine the extent to which the benefits of this training could be transferred to other cognitive skills. Training involved verbal and spatial versions of a complex WM span task designed to emphasize simultaneous storage and processing requirements. Participants who completed 4 weeks of WM training demonstrated significant improvements on measures of temporary memory. These WM training benefits generalized to performance on the Stroop task and, in a novel finding, promoted significant increases in reading comprehension. The results are discussed in relation to the hypothesis that WM training affects domain-general attention control mechanisms and can thereby elicit far-reaching cognitive benefits. Implications include the use of WM training as a general tool for enhancing important cognitive skills.

While WM training yielded many valuable benefits such as increased reading comprehension, it did not improve IQ as measured by an unspeeded Advanced Progressive Matrices (APM) IQ test;

> However, such power limitations do not readily account for our failure to replicate a transfer of WM training benefits to measures of fluid intelligence (as was observed by Jaeggi et al., 2008), since we did not find even a trend for improvement in trained participants on Raven's APM. Beyond statistical explanations, differences in the training paradigms used for the two studies may explain the differences in transfer effects. The training program used by Jaeggi et al. (2008) involved 400 trials per training session, with a dual n-back training paradigm designed to emphasize binding processes and task management. Conversely, our training paradigm included only 32 trials per session and more heavily emphasized maintenance in the face of distraction. Finally, the seemingly conflicting results may be due to differences in intelligence test administration. As was pointed out in a recent critique (Moody, 2009), Jaeggi et al. (2008) used atypical speeded procedures in administering their tests of fluid intelligence, and these alterations may have confounded the apparent effect of WM training on intelligence.

<!-- meta-analysis: "Chein"      2010   21   12.2      0.65   21    12.4    0.72 -->

<!--
In young adults: "Participants who completed 4 weeks of WM training demonstrated significant improvements on measures of temporary memory"
doi:10.3758/PBR.17.2.193

In older adults: "Relative to controls, trained participants showed transfer improvements in Reading Span and the number of repetitions on the CVLT."
doi:10.1037/a0023631

"Participants who completed 4 weeks of WM training demonstrated significant improvements on measures of temporary memory. These WM training benefits generalized to performance on the Stroop task and, in a novel finding, promoted significant increases in reading comprehension. The results are discussed in relation to the hypothesis that WM training affects domain-general attention control mechanisms and can thereby elicit far-reaching cognitive benefits. Implications include the use of WM training as a general tool for enhancing important cognitive skills."
doi:10.3758/PBR.17.2.193
-->

### Colom 2010

["Improvement in working memory is not related to increased intelligence scores"](http://dx.doi.org/10.1016/j.intell.2010.06.008) ([full text](/docs/dnb/2010-colom.pdf)) trained 173 students on WM tasks (such as the [reading span task](!Wikipedia)) with randomized difficulties, and found no linked IQ improvement; the IQ tests were "the Advanced Progressive Matrices Test (APM) along with the abstract reasoning (DAT-AR), verbal reasoning (DAT-VR), and spatial relations (DAT-SR) subtests from the Differential Aptitude Test Battery". None were speeded as in Jaeggi 2008. Abstract:

> The acknowledged high relationship between working memory and intelligence suggests common underlying cognitive mechanisms and, perhaps, shared biological substrates. If this is the case, improvement in working memory by repeated exposure to challenging span tasks might be reflected in increased intelligence scores. Here we report a study in which 288 university undergraduates completed the odd numbered items of four intelligence tests on time 1 and the even numbered items of the same tests one month later (time 2). In between, 173 participants completed three sessions, separated by exactly one week, comprising verbal, numerical, and spatial short-term memory (STM) and working memory (WMC) tasks imposing high processing demands (STM-WMC group). 115 participants also completed three sessions, separated by exactly one week, but comprising verbal, numerical, and spatial simple speed tasks (processing speed, PS, and attention, ATT) with very low processing demands (PS-ATT group). The main finding reveals increased scores from the pre-test to the post-test intelligence session (more than half a standard deviation on average). However, there was no differential improvement on intelligence between the STM-WMC and PS-ATT groups.

Commentators on the [ML discussion](http://groups.google.com/group/brain-training/browse_frm/thread/dfefba647545fb57) criticized the study for:

1. Not using DNB itself
2. apparently little training time on the WM tasks (3 sessions over weeks, each of unclear duration)
3. the randomization of difficulty (as opposed to DNB's adaptiveness)
4. the large increase in scores on the WM tasks over the 3 sessions (suggesting growing familiarity than real challenge & growth)
5. and the statistical observation that if IQ gains were linear with training and started small then 173 participants is not enough to observe with confidence any improvements.

### Loosli et al 2011

["Working memory training improves reading processes in typically developing children"](/docs/dnb/2011-loosli.pdf), Loosli, Buschkuehl, Perrig, and Jaeggi:

> The goal of this study was to investigate whether a brief cognitive training intervention results in a specific performance increase in the trained task, and whether there are transfer effects to other nontrained measures. A computerized, adaptive working memory intervention was conducted with 9- to 11-year-old typically developing children. The children considerably improved their performance in the trained working memory task. Additionally, compared to a matched control group, the experimental group significantly enhanced their reading performance after training, providing further evidence for shared processes between working memory and reading.

This is showing connection to useful tasks, but not for showing any gain to IQ. The difference in score improvement between groups was small, half a point, and the training period fairly short; the authors write:

> Due to the short training time, we did not expect large effects on _Gf_ (cf. Jaeggi et al., 2008), also since two other studies that trained ADHD children observed transfer effects on _Gf_ only after 5 weeks involving sessions of 40 minutes each (Klingberg et al., 2002, 2005).
>
> In addition, the same group failed to show transfer on _Gf_ with a shorter training (Thorell et al., 2008). Thus, considering that our training intervention was merely 10 sessions long, our lack of transfer to _Gf_ is hardly surprising; although there is now recent evidence that transfer to _Gf_ is possible with very little training time ([Karbach & Kray, 2009](http://www.psychology.uwo.ca/undergraduate/psych3441G/Karbach2009Developmental%20ScienceHow%20useful%20is%20executive%20control.pdf "How useful is executive control training? Age differences in near and far transfer of task-switching training"); [poster](http://web.archive.org/web/20100705042933/http://www.uni-saarland.de/fak5/entwicklung/juniorprofessurkray/Dokumente/Praesentationen/CAC08_Karbach.pdf)). Our results, however, are comparable to those of Chein and Morrison (2010), who also trained their participants on a complex WM task and found no transfer to _Gf_.

(Similar studies have also found improvement in reading skills after WM training, eg [Dahlin 2011](https://mondo.su.se/access/content/group/87a78313-bf21-438a-842a-a4d680ccf6f9/Dahlin%20artikel%202010.pdf "Effects of working memory training on reading in children with special needs") and [Shiran & Breznitz 2011](/docs/dnb/2011-shiran.pdf "The effect of cognitive training on recall range and speed of information processing in the working memory of dyslexic and skilled readers"), but I do not believe others used n-back or looked for possible IQ gains.)

### Nutley 2011

["Gains in fluid intelligence after training non-verbal reasoning in 4-year-old children: a controlled, randomized study"](http://www.klingberglab.se/pub/BergmanNutley_fluid_intelligence_2011.pdf), Sissela Bergman Nutley et al:

> Fluid intelligence (_Gf_) predicts performance on a wide range of cognitive activities, and children with impaired _Gf_ often experience academic difficulties. Previous attempts to improve _Gf_ have been hampered by poor control conditions and single outcome measures^[See the critical review of WM training research, ["Does working memory training generalize?"](http://psychology.gatech.edu/renglelab/2010/shipsteadredickengle.pdf) (Shipstead et al 2010).]. It is thus still an open question whether _Gf_ can be improved by training. This study included 4-year-old children (N = 101) who performed computerized training (15 min/day for 25 days) of either non-verbal reasoning, working memory, a combination of both, or a placebo version of the combined training. Compared to the placebo group, the non-verbal reasoning training group improved significantly on _Gf_ when analysed as a latent variable of several reasoning tasks. Smaller gains on problem solving tests were seen in the combination training group. The group training working memory improved on measures of working memory, but not on problem solving tests. This study shows that it is possible to improve _Gf_ with training, which could have implications for early interventions in children.

Points:

1. The WM tasks were not n-back:

    > "The WM training was the same as described in [Thorell et al. (2009)](http://www.klingberglab.se/pub/Thorell2009.pdf) developed by Cogmed Systems Inc. There were seven different versions of visuo-spatial WM tasks, out of which three were trained every day on a rotating schedule. Briefly, the tasks all consisted of a number of animated figures presented in different settings (e.g. swimming in a pool, riding on a rollercoaster). Some of the figures (starting with two figures and then increasing in number depending on the child's performance) made a sound and changed colour during a short time period. The task then consisted of remembering which figures had changed colour and in what order this had occurred."
2. The magnitude of _Gf_ increase was not suspiciously large:

    > "The NVR training group showed transfer both when this was estimated with single tests, as well as when Gf was measured as a latent variable. The magnitude of this improvement was approximately 8% (compared to the placebo group) which is comparable with previously reported gains of _Gf_ of 5-13.5% ([Hamers et al., 1998](http://igitur-archive.library.uu.nl/fss/2007-0507-200457/Hamers(1998)_inductive%20reasoning.pdf); [Jaeggi et al., 2008](#jaeggi-2008); [Klauer & Willmes, 2002](/docs/dnb/2002-klauer.pdf "Inducing Inductive Reasoning: Does It Transfer to Fluid Intelligence?"); [Stankov, 1986](/docs/dnb/1986-stankov.pdf "Kvashchev's Experiment: Can We Boost Intelligence?"))."
3. There are some possible counter-arguments to generalizing the lack of _Gf_ gains in the WM-only group, mostly related to the young age:

    > "This could mean that WM is not a limiting factor for 4-year-old children solving reasoning problems such as Raven's CPM and Block Design. The moderate correlations between the Grid Task and the reasoning tests (between 0.3 and 0.6, see Table 1) point to the somewhat counterintuitive conclusion that correlation between two underlying abilities is not a sufficient predictor to determine amount of transfer of training effects between these abilities. A similar conclusion was drawn after the lack of training effects on WM after training inhibitory functions (Thorell et al., 2009). In that study WM capacity correlated with performance on the inhibitory tasks at baseline (R = 0.3). An imaging study also showed that performance on a WM grid task and inhibitory tasks activate overlapping parts of the cortex ([McNab, Leroux, Strand, Thorell, Bergman & Klingberg, 2008](http://www.klingberglab.se/pub/McNab2008.pdf)). Inhibitory training improved performance on the trained tasks, yet there was no transfer seen on WM tasks. The principles governing the type of cognitive training that will transfer are still unclear and pose an important question for future studies.
    >
    > One way to find these principles may be through understanding the neural mechanisms of training. For example, WM training in 4-year-olds might have a more pronounced effect on the parietal lobe, compared to the less mature frontal lobe. If the transfer to Gf is dependent on prefrontal functions, it may explain the lack of transfer from WM training to Gf in 4-year-olds. In other words, transfer effects may differ with the progression of development."

### Zhao et al 2011

["Effect of updating training on fluid intelligence in children"](/docs/dnb/2011-zhao.pdf), _Chinese Science Bulletin_:

> Recent studies have indicated that working memory (WM) training can improve fluid intelligence. However, these earlier studies confused the impact of WM storage and central executive function on the effects of training. The current study used the running memory task to train the updating ability of [33] 9-11 year-old children using a double-blind controlled design. The results revealed that children's fluid intelligence was significantly improved by memory-updating training. Overall, our findings suggest that the increase in fluid intelligence achieved with WM training is related to improving central executive function.

### Roughan & Hadwin 2011

["The impact of working memory training in young people with social, emotional and behavioural difficulties"](/docs/dnb/2011-roughan.pdf), Laura Roughan & Julie A. Hadwin 2011:

> This study examined the impact of a working memory (WM) training programme on measures of WM, IQ, behavioural inhibition, self-report test and trait anxiety and teacher reported emotional and behavioural difficulties and attentional control before and after WM training and at a 3 month follow-up. The WM training group (N=7) showed significantly better post-training on measures of IQ, inhibition, test anxiety and teacher-reported behaviour, attention and emotional symptoms, compared with a non-intervention passive control group (N=8). Group differences in WM were also evident at follow-up. The results indicated that WM training has some potential to be used to reduce the development of school related difficulties and associated mental health problems in young people. Further research using larger sample sizes and monitoring over a longer time period is needed to replicate and extend these results.

The WM training was done using [Cogmed](http://www.cogmed.com/); it's unclear whether the Cogmed tasks use DNB or not (they seem to have *similar* tasks available in it, at least), but the study did find IQ gains:

> Considering T1 T2 IQ difference scores, the analysis revealed a significant group effect with a large ES (_F_(1,14) = 10.37, _p_<.01, _n_=.44); the intervention group showed increased IQ difference scores (_N_=7, mean=5.36, _SD_=6.52, range= -2.5 to 17.5) compared with the control group (_N_=7, mean=-6.35, _SD_=7.21, range = -15 to 5). T1 T3 analyses indicated that the T1 T3 difference was not significant (see Fig. 1).

Note the means as compared with the standard deviation; these are very troubled young people.

<!-- meta-analysis data: "Roughan"    2011   7    5.36      6.52   8     -6.35   7.21 -->

### Brehmer et al 2012

["Working-memory training in younger and older adults: training gains, transfer, and maintenance"](http://jtoomim.org/files/Brehmer%20et%20al.%20-%202012%20-%20Working-memory%20training%20in%20younger%20and%20older%20adult.pdf):

> Working memory (WM), a key determinant of many higher-order cognitive functions, declines in old age. Current research attempts to develop process-specific WM training procedures, which may lead to general cognitive improvement. Adaptivity of the training as well as the comparison of training gains to performance changes of an active control group are key factors in evaluating the effectiveness of a specific training program. In the present study, 55 younger adults (20-30 years of age) and 45 older adults (60-70 years of age) received 5 weeks of computerized training on various spatial and verbal WM tasks. Half of the sample received adaptive training (i.e., individually adjusted task difficulty), whereas the other half-worked on the same task material but on a low task difficulty level (active controls). Performance was assessed using criterion, near-transfer, and far-transfer tasks before training, after 5 weeks of intervention, as well as after a 3-month follow-up interval. Results indicate that (a) adaptive training generally led to larger training gains t/han low-level practice, (b) training and transfer gains were somewhat greater for younger than for older adults in some tasks, but comparable across age groups in other tasks, (c) far-transfer was observed to a test on sustained attention and for a self-rating scale on cognitive functioning in daily life for both young and old, and (d) training gains and transfer effects were maintained across the 3-month follow-up interval across age.

Used Cogmed, which Jaeggi says is not dual n-back.

<!-- deleted data from meta-analysis:

"Brehmer"    2012   55   8.3       2.775  25    8.556   2.581

Brehmer et al data: pooled young and old scores on the Raven[^Brehmer-pooled]

^Brehmerpooled: Experimental: young adaptive: 29 9.93 2.2 old adaptive: 26 6.5 3.42 young none: 26 9.81 2.1 old none: 19 6.84 3.25

    1. sample size: 29+26=55
    2. mean: $\frac{(29 \times 9.93) + (26 \times 6.5)}{29+26} = 8.3$
    3. pooled variance: $\frac{((29-1) \times 2.2) + ((26-1) \times 3.42)}{29 + 26 - 2} = 2.775$

    Control:

    1. sample size: 26+19=45
    2. mean: $\frac{(26 \times 9.81) + (19 \times 6.84)}{26+19} = 8.556$
    3. pooled variance: $\frac{((26-1) \times 2.1) + ((19-1) \times 3.25)}{26 + 19 - 2} = 2.581$
-->

## Saccading

 One fascinating psychology result is that strongly [right-handed](!Wikipedia "Right-handedness") people can improve their memory (and possibly N-back performance) by simply taking 30 seconds and flicking ("[saccading](!Wikipedia "saccade")") their eyes left and right (for a summary, see ["A quick eye-exercise can improve your performance on memory tests (but only if you're right-handed)"](http://scienceblogs.com/cognitivedaily/2009/04/16/a-quick-eye-exercise-can-impro/)).

Version 4.5 of Brain Workshop introduced a saccading feature: a dot alternates sides of the screen and one is to follow it with one's eyes. You activate it by pressing 'e' while in fullscreen mode (setting `WINDOW_FULLSCREEN = True` in the configuration file). It may or may not a bad idea to alternate rounds of N-back with rounds of saccading. At my request, saccading logs are now kept by BW so at some point in the future, it should be possible to request logs from users and see whether saccading in general correlates with N-back performance; I personally randomized use of saccading, but saw no benefits (see next section).

Ashirgo [writes](http://groups.google.com/group/brain-training/browse_thread/thread/3e9b194bd55401d6) that her previous advice encompasses this eye-movement result; Pheonexia reports that after trying the saccading before a BW session, he "performed better than I ever have before".

The most recent study on this effect seems to be ["Eye movements enhance memory for individuals who are strongly right-handed and harm it for individuals who are not"](http://psych.wustl.edu/memory/Roddy%20article%20PDF%27s/Lyle%20et%20al%20(2008)_PBR.pdf). It says:

> Subjects who make repetitive saccadic eye movements before a memory test subsequently exhibit superior retrieval in comparison with subjects who do not move their eyes. It has been proposed that eye movements enhance retrieval by increasing interaction of the left and right cerebral hemispheres. To test this, we compared the effect of eye movements on subsequent recall (Experiment1) and recognition (Experiment2) in two groups thought to differ in baseline degree of hemispheric interaction-individuals who are strongly right-handed (SR) and individuals who are not (nSR). For SR subjects, who naturally may experience less hemispheric interaction than nSR subjects, eye movements enhanced retrieval. In contrast, depending on the measure, eye movements were either inconsequential or even detrimental for nSR subjects. These results partially support the hemispheric interaction account, but demand an amendment to explain the harmful effects of eye movements for nSR individuals.

(Note that very important caveat: this is a useful technique *only* for strongly right-handed people; weak righties and lefties are outright harmed by this technique.)

See also ["Interhemispheric Interaction and Saccadic Horizontal Eye Movements: Implications for Episodic Memory, EMDR, and PTSD"](http://www.sandernieuwenhuis.nl/pdfs/PropperChristman.pdf "Propper & Christman 2008"); ["The efficacy and psychophysiological correlates of dual-attention tasks in eye movement desensitization and reprocessing (EMDR)"](/docs/dnb/2011-schubert.pdf "Schubert et al 2011"); ["Horizontal saccadic eye movements enhance the retrieval of landmark shape and location information"](/docs/dnb/2009-brunye.pdf "Brunyé et al 2009"); ["Reduced misinformation effects following saccadic bilateral eye movements"](/docs/dnb/2009-parker.pdf "Parker et al 2009"); ["Is saccade-induced retrieval enhancement a potential means of improving eyewitness evidence?"](/docs/dnb/2010-lyle.pdf "Lyle & Jacob 2010")

### Self-experiment

Brain Workshop now has logging of saccading implemented; this was added at my request to make experimenting with saccading easier, since you can't compare scores unless you know when you were saccading or not. After this was added (thanks Jonathan etc), I began to randomize each day to saccading or not-saccading before rounds with a coin flip. Blinding is impossible, so I did nothing about that. After 158 rounds over roughly 35 days between 10 September and 5 November 2012, the result is: no difference. Not even close. So apparently though I am strongly right-handed as the original study's memory effect required, saccading makes no difference to my n-back performance.

#### Analysis

My [BW data](/docs/dnb/gwern-bw-stats.txt) had to be parsed by hand and some Emacs macros because I couldn't figure out a nice clean programmatic way to parse it and spit out scores divvied by whether they were on a saccade on or off day (so if you want to replicate my analysis, you'll have to do that yourself). The analysis[^R-best-saccading] using [BEST](http://www.indiana.edu/~kruschke/BEST/) reveals a difference of less than 1% right (+0.4%) per round, and the estimates of effect size are negative almost as often as they are positive:

![Bayesian MCMC estimates of difference in saccading and non-saccading scores](/images/dnb/saccading.png)

[^R-best-saccading]: The R code:

    ~~~{.R}
    R> on <- c(35,31,27,66,25,38,35,43,60,47,38,58,50,23,50,45,60,37,22,28,50,20,41,42,47,55,47,42,35,
               40,44,40,33,44,19,58,38,41,52,41,33,47,45,45,55,20,31,42,53,27,45,50,65,33,33,30,52,36,
               28,43,55,40,31,30,45,45,60,37,22,38,45,64,50,44,38)
    R> off <- c(17,43,46,50,36,31,38,33,66,30,68,42,40,29,69,40,41,45,37,18,44,60,31,46,46,45,27,35,45,
                30,29,47,56,37,50,33,40,47,41,25,50,20,25,30,70,45,50,27,29,55,47,47,42,40,35,36,54,64,
                25,28,31,15,47,64,35,33,60,38,28,60,50,42,31,50,30,35,61,56,30,44,37,43,38)
    R> length(c(on,off))
    [1] 158
    R>
    R> source("BEST.R")
    R> mcmcChain = BESTmcmc(off, on)
    R> postInfo = BESTplot(off, on, mcmcChain) # image
    R> postInfo
               SUMMARY.INFO
    PARAMETER       mean   median     mode  HDIlow  HDIhigh pcgtZero
      mu1       40.96178 40.95536 40.93523 38.1887  43.7104       NA
      mu2       41.37400 41.37365 41.39874 38.8068  44.0550       NA
      muDiff    -0.41222 -0.41368 -0.45968 -4.2497   3.3593    41.54
      sigma1    12.32844 12.27614 12.28283 10.3024  14.4116       NA
      sigma2    11.21408 11.15464 10.99812  9.2924  13.1895       NA
      sigmaDiff  1.11436  1.10736  0.94511 -1.6011   3.9756    78.73
      nu        45.65240 37.49245 22.16426  5.3586 108.1555       NA
      nuLog10    1.56504  1.57394  1.61572  0.9956   2.1157       NA
      effSz     -0.03528 -0.03519 -0.03547 -0.3588   0.2851    41.54
    ~~~

    For those who prefer a regular two-sample test:

    ~~~{.R}
    R> wilcox.test(off,on)

        Wilcoxon rank sum test with continuity correction

    data:  off and on
    W = 3004, p-value = 0.7066
    alternative hypothesis: true location shift is not equal to 0
    ~~~

Since there's hardly any evidence even though this looks like plenty of data, I think I'll stop doing saccading. I can only speak for myself, so I would be pleased if other right-handed n-backers could adopt a similar procedure and see whether perhaps I am an exception.

## Sleep

["Sleep Accelerates the Improvement in Working Memory Performance"](/docs/dnb/2008-kuriyama.pdf), Kuriyama 2008:

> Working memory (WM) performance, which is an important factor for determining problem-solving and reasoning ability, has been firmly believed to be constant. However, recent findings have demonstrated that WM performance has the potential to be improved by repetitive training. Although various skills are reported to be improved by sleep, the beneficial effect of sleep on WM performance has not been clarified. Here, we show that improvement in WM performance is facilitated by posttraining naturalistic sleep. A spatial variant of the _n_-back WM task was performed by 29 healthy young adults who were assigned randomly to three different experimental groups that had different time schedules of repetitive _n_-back WM task sessions, with or without intervening sleep. Intergroup and intersession comparisons of WM performance (accuracy and response time) profiles showed that n-back accuracy after posttraining sleep was significantly improved compared with that after the same period of wakefulness, independent of sleep timing, subject's vigilance level, or circadian influences. On the other hand, response time was not influenced by sleep or repetitive training schedules. The present study indicates that improvement in _n_-back accuracy, which could reflect WM capacity, essentially benefits from posttraining sleep.

(In this test, the baseline/unpracticed performance of the two groups was the same; but the schedule in which subjects trained at 10 PM and went to bed resulted in greater improvements in performance than schedules in which subjects trained when they got up at 8 AM and went to bed ~10 PM.)

## Lucid dreaming

[Stephen LaBerge](!Wikipedia), pioneer of [lucid dreaming](!Wikipedia) writes^["The Psychophysiology of Lucid Dreaming", collected in [_Conscious Mind, Sleeping Brain_](http://www.amazon.com/Conscious-Mind-Sleeping-Brain-Gackenbach/dp/0306428490/).]

> "Why then is CNS activation necessary for lucid dreaming? Evidently the high level of cognitive function involved in lucid dreaming requires a correspondingly high level of neuronal activation. In terms of Antrobus's (1986) adaptation of Anderson's (1983) ACT* model of cognition to dreaming, working memory capacity is proportional to cognitive activation, which in turn is proportional to cortical activation. Becoming lucid requires an adequate level of working memory to active the presleep intention to recognize that one is dreaming. This level of activation is apparently not always available during sleep but normally only during phasic REM."

[Allan Hobson](!Wikipedia) has apparently speculated^[pg 97 of his book [_The Dream Drugstore_](http://www.amazon.com/Dream-Drugstore-Chemically-Altered-Consciousness/dp/0262582201/) (2001)] that WM and the prefrontal cortex is partially de-activated during REM sleep and this is why dreamers do not realize they are dreaming - the same region that n-back tasks activate.[^fmri] The suggestion then goes that n-back training will enable greater dream recognition & recall, which are crucial skills for any would-be lucid dreamer. A number of people have reported *only* dreams and lucid dreams as the result of n-back training (eg. [Boris & Michael](http://marginalrevolution.com/marginalrevolution/2011/06/does-this-reliably-increase-your-fluid-intelligence.html#comment-157452725)).

[^fmri]: ["Differential dorsolateral prefrontal cortex activation during a verbal n-back task according to sensory modality"](http://www.fmri.uji.es/papers/Rodriguez-Jimenez_2009_Differential%20dorsolateral%20nback.pdf "Rodriguez-Jimenez et al 2009"); abstract:

     > Functional neuroimaging studies carried out on healthy volunteers while performing different n-back tasks have shown a common pattern of bilateral frontoparietal activation, especially of the dorsolateral prefrontal cortex (DLPFC). Our objective was to use functional magnetic resonance imaging (fMRI) to compare the pattern of brain activation while performing two similar n-back tasks which differed in their presentation modality. Thirteen healthy volunteers completed a verbal 2-back task presenting auditory stimuli, and a similar 2-back task presenting visual stimuli. A conjunction analysis showed bilateral activation of frontoparietal areas including the DLPFC. The left DLPFC and the superior temporal gyrus showed a greater activation in the auditory than in the visual condition, whereas posterior brain regions and the anterior cingulate showed a greater activation during the visual than during the auditory task. Thus, brain areas involved in the visual and auditory versions of the n-back task showed an important overlap between them, reflecting the supramodal characteristics of working memory. However, the differences found between the two modalities should be considered in order to select the most appropriate task for future clinical studies.

On the other hand, I have seen anecdotal reports that *any* intense mental exercise or learning causes increased dreaming, even if the exercise is domain-specific (eg. the famous [Tetris effect](!Wikipedia)) or just memorization (as in use of Mnemosyne for [spaced repetition](Spaced repetition)), and LaBerge also remarks (pg 165 of [_Exploring the World of Lucid Dreaming_](http://www.amazon.com/Exploring-World-Dreaming-Stephen-LaBerge/dp/034537410X/)):

> Most people assume that a major function of sleeping and dreaming is rest and recuperation. This popular conception has been upheld by research. Thus, for humans, physical exercise leads to more sleep, especially delta sleep. Growth hormone, which triggers growth in children and the repair of stressed tissues, is released in delta sleep. On the other hand, mental exercise or emotional stress appears to result in increases in REM sleep and dreaming.

## Aging

General cognitive factors like working memory and processing speed (& perceptual processing^[Schneider, B, Pichora-Fuller, MK. "Implications of perceptual deterioration for cognitive aging Research". In: Craik, FI, Salthouse, TA, editors. [_The handbook of aging and cognition_](http://www.amazon.com/Handbook-Aging-Cognition-Third/dp/080585990X/), Psychology Press, 2000. ISBN-10: 080585990X]) are traits that peak in early adult hood and then [decline over a lifetime](http://psychology.ucdavis.edu/labs/ferrer/pubs/dp_2002.pdf "'Comparative Longitudinal Structural Analyses of the Growth and Decline of Multiple Intellectual Abilities Over the Life Span', McArdle et al 2002"); the following image was adapted by Gizmodo from a study of age-related decline, ["Models of visuospatial and verbal memory across the adult life span"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.6787&rep=rep1&type=pdf)^[Abstract: "The authors investigated the distinctiveness and interrelationships among visuospatial and verbal memory processes in short-term, working, and long-term memories in 345 adults. Beginning in the 20s, a continuous, regular decline occurs for processing-intensive tasks (e.g., speed of processing, working memory, and long-term memory), whereas verbal knowledge increases across the life span [Besides Salthouse, for the verbal fluency claim see Schaie, K. W. [_Intellectual Development in Adulthood: The Seattle Longitudinal Study_](http://www.amazon.com/Intellectual-Development-Adulthood-Seattle-Longitudinal/dp/0521430143/). Cambridge University Press, 1996]. There is little differentiation in the cognitive architecture of memory across the life span. Visuospatial and verbal working memory are distinct but highly interrelated systems with domain-specific short-term memory subsystems. In contrast to recent neuroimaging data, there is little evidence for dedifferentiation of function at the behavioral level in old compared with young adults." That the neuroimaging shows no change in general locations of activity is probably interpretable as the lower performance being due to general low-level problems and inefficiencies of age, and not the elderly's brains starting to 'unlearn' specific tasks.]. The units are [z-scores](!Wikipedia "Standard score"), units of standard deviations (so for the 80 year olds to be two full units below the 20 year olds indicates a profound fall in the averages^["The Z-score represents the age-contingent mean, measured in units of standard deviation relative to the population mean. More precisely, the Z-score is (age-contingent mean minus population mean) / (population standard deviation)." --Agarwal et al 2009]); the first image is from [Park et al 2002](http://www.ncbi.nlm.nih.gov/sites/entrez/12061414):

![Graph of multiple mental traits due to age-related decline (in standard deviations)](/images/2011-gizmodo-brainage.jpg) ![Schaie 1996, _Intellectual Development..._](/images/1996-seattle-aging.jpg)

A [cross-section of thousands of participants](http://www.uwo.ca/its/brain/iqmyth/Hampshire%20Owen%20IQ%20Neuron.pdf "'Fractionating human intelligence', Hampshire et al 2013?") in the Cambridge brain-training study found "Age, was by far the most significant predictor of performance, with the mean scores of individuals in their 60s ~1.7 SDs below those in their early 20s (Figure 4a). (Note, in intelligence testing, 1 SD is equivalent to 15 IQ points)." These declines in reasoning affect valuable real-world activities like personal finance[^age-of-reason], and simple everyday questions:

![from Agarwal et al 2009, "The Age of Reason: Financial Decisions over the Life-Cycle with Implications for Regulation"](/images/2009-agarwal-pg10.png "Age-related decline in immediate & delayed word recall; counting backwards; ordinary facts like the current day; understanding percentages; and division of a lottery prize")

These results may be surprising because some studies did not find such dramatic declines, but apparently part of the decline can be hidden by practice effects[^salthousepractice], and they are consistent with other results like the lifelong changes in [Big Five personality traits](!Wikipedia) ([decreases](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2562318/) in [Extraversion](!Wikipedia "Extraversion and introversion") & [Openness to experience](!Wikipedia)[^old-attitudes], the latter decline possibly [ameliorated by cognitive exercise](http://europepmc.org/articles/PMC3330146 "'Can an old dog learn (and want to experience) new tricks? Cognitive training increases openness to experience in older adults', Jackson et al 2012")). Longitudinal studies are pessimistic, finding declines early on, in one's 40s ([Sing-Manoux et al 2011](http://www.bmj.com/content/344/bmj.d7622)). The degradation of white matter and its effects on episodic memory retrieval have been [observed physically](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2396513/ "'Age-related slowing of memory retrieval: contributions of perceptual speed and cerebral white matter integrity', Bucur et al 2008") using [fractional anisotropy](!Wikipedia). Another 2011 study testing 2000 individuals between 18 and 60 found that "Top performances in some of the tests were accomplished at the age of 22. A notable decline in certain measures of abstract reasoning, brain speed and in puzzle-solving became apparent at 27."^[["Cognitive Decline Begins In Late 20s, Study Suggests"](http://www.sciencedaily.com/releases/2009/03/090320092111.htm), _Science Daily_] (Of course, like the previous study, a correlation over many individuals of varying ages [is not as good](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2680669/) as having a series of performance measurements for one aging individual. But time will cure that fault, hopefully.) The [abstract](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2683339/) of this Salthouse study says:

> ...Results from three methods of estimating retest effects in this project, together with results from studies comparing non-human animals raised in constant environments and from studies examining neurobiological variables not susceptible to retest effects, converge on a conclusion that some aspects of age-related cognitive decline begin in healthy educated adults when they are in their 20s and 30s.

<!-- Personality mutability:
> Later studies have since suggested that personality is instead set like "soft" plaster, in that personality does change, albeit only marginally, beyond 30 and across the entire life cycle (Srivastava, John, Gosling, & Potter, 2003). Some of this research is problematic to interpret, as it has been based on cross-sectional differences in the mean level of personality traits across age groups, which could represent either real change or simply cohort effects (where, for example, people at a certain age only appear to have different personality profiles due to events that historically happened to their cohort in youth). More recent longitudinal research, however, suggests that personality change does take place, with the same people giving different responses to personality questionnaires on different occasions (e.g. Helson, Jones, & Kwan, 2002; Lucas & Donnellan, 2011; Roberts, Walton, & Viechtbauer, 2006a; Roberts, Wood, & Caspi, 2008) and as such a relatively broad consensus that personality does change has developed (Costa & McCrae, 2006; Roberts, Walton, & Viechtbauer, 2006b)
http://www.powdthavee.co.uk/resources/Is+Personality+Fixed_SOICRev1.pdf

"Can An Old Dog Learn (and Want To Experience) New Tricks? Cognitive Training Increases Openness to Experience in Older Adults", Jackson et al 2012

> The present study investigated whether an intervention aimed to increase cognitive ability in older adults also changes the personality trait of openness to experience. Older adults completed a 16-week program in inductive reasoning training supplemented by weekly crossword and Sudoku puzzles. Changes in openness to experience were modeled across four assessments over 30 weeks using latent growth curve models. Results indicate that participants in the intervention condition increased in the trait of openness compared with a waitlist control group. The study is one of the first to demonstrate that personality traits can change through nonpsychopharmocological interventions.

> A 16-week home-based intervention had two components: (a) an inductive reasoning training program developed by Margrett and Willis (2006) that had been adapted from the protocols used in the ACTIVE trials (Advanced Cognitive Training for Independent and Vital Elderly; Ball et al., 2002), and (b) puzzles that relied in part on inductive reasoning (and also made the program more enjoyable). Both of these activities were adaptive as skill level changed, with the goal that participants would feel challenged but not overwhelmed (Payne, Jackson, Noh, & Stine-Morrow, in press). At two points during the program, 1-hr instructional sessions in a classroom format were available for participants (a session detailing strategies for Sudoku and crosswords and a session that introduces participants to the home-based training program for reasoning). For the remaining weeks, participants in the intervention only came to the lab each week to turn in materials and pick up a new packet; these visits typically lasted 10 to 15 min. The 16 program weeks actually spanned 20 -22 weeks to accommodate weather cancellations and winter holidays.
-->

[^age-of-reason]: ["The Age of Reason: Financial Decisions over the Life-Cycle with Implications for Regulation"](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=973790), Agarwal et al 2009 ([slides](http://web.archive.org/web/20121206060934/http://www.economics.harvard.edu/faculty/laibson/files/Age+of+Reason.pdf "The Age of Reason")):

    > ...The prevalence of dementia explodes after age 60, *doubling* with every 5 years of age.^5^ In the cohort above age 85, the prevalence of dementia exceeds 30%. Moreover, many older adults without a strict diagnosis of dementia, still experience substantial cognitive impairment. For example, the prevalence of the diagnosis "cognitive impairment without dementia" is nearly 30% between ages 80 and 89.^6^ Drawing these facts together, among the population between ages 80 and 89, about half of the population either has a diagnosis of dementia or cognitive impairment without dementia.
    >
    > - 5: [Ferri et al (2006)](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2850264/ "Global prevalence of dementia: a Delphi consensus study")
    > - 6: [Plassman et al (2008)](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2670458/ "Prevalence of Cognitive Impairment without Dementia in the United States"). They define cognitive impairment without dementia as a Dementia Severity Rating Scale score of 6 to 11
    >
    > ...Third, using a new dataset, we document a link between age and the quality of financial decision-making in debt markets. In a cross-section of prime borrowers, middle-aged adults borrow at lower interest rates and pay fewer fees relative to younger and older adults. Averaging across ten credit markets, fee and interest payments are minimized around age 53. The measured effects are not explained by observed risk characteristics. Combining multiple data sets we do not find evidence that selection effects and cohort effects explain our results. The leading explanation for the patterns that we observe is that experience rises with age, but analytical abilities decline with it.
    >
    > ...Neurological pathologies represent one important pathway for age effects in older adults. For instance, dementia is primarily attributable to Alzheimer's Disease (60%) and vascular disease (25%). The prevalence of dementia doubles with every five additional years of lifecycle age (Ferri et al., 2006; [Fratiglioni, De Ronchi, and Agüero-Torres, 1999](/docs/dnb/1999-fratiglioni.pdf "Worldwide Prevalence and Incidence of Dementia")).^10^ For example, Table 1 reports that the prevalence of dementia in North America rises from 3.3% for adults ages 70-74, to 6.5% for adults ages 75-79, to 12.8% for adults ages 80-84, to 30.1% for adults at least 85 years of age (Ferri et al. 2006). Many older adults also suffer from a less severe form of cognitive impairment, which is diagnosed as "cognitive impairment without dementia." For example, the prevalence of this diagnosis rises from 16.0% for adults ages 71-79, to 29.2% for adults ages 80-89.
    >
    > - 10: There is also growing literature that identifies age-related changes in the nature of cognition (see Park and Schwarz, 1999 [_Cognitive Aging: A Primer_]; and [Denburg, Tranel, and Bechara 2005](/docs/dnb/2005-denburg.pdf "The ability to decide advantageously declines prematurely in some normal older persons")). [Mather and Carstensen (2005)](http://www.usc.edu/projects/matherlab/pdfs/MatherCarstensen2005.pdf "Aging and Motivated Cognition: The Positivity Effect in Attention and Memory") and [Carstensen (2006)](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2790864/ "The Influence of a Sense of Time on Human Development") identify age-variation in cognitive preferences. Subjects with short time horizons or older ages attend to negative information relatively less than subjects with long time horizons or younger ages.
    >
    > ...Figure 4d plots naive and control performance in the Telephone Interview of Cognitive Status (TICS) task. This task asks the respondent ten trivial questions and assigns one point for each correct answer: What is the current year? Month? Day? Day of the week? What do you usually use to cut paper? What do you call the kind of prickly plant that grows in the desert? Who is the current president? Vice president? Count backwards from twenty to ten (twice). At age 63, the average score is 9.2 out of 10. By age 90, the average (control) score is 7.5. Finally, we present two measures of practical numeracy. 4e plots naive and control performance in response to the question: If the chance of getting a disease is 10 percent, how many people out of 1,000 would be expected to get the disease? At age 53, 79% answer correctly. By age 90, 50% answer correctly. Figure 4f plots naive and control performance in response to the question: If 5 people all have the winning numbers in the lottery and the prize is two million dollars, how much will each of them get? We believe that this question is imprecisely posed, since the logical answer could be either $2,000,000 or $400,000. However, the results are still interesting, since the fraction answering $400,000 (the official correct answer) drops precipitously. At age 53, 52% answer $400,000. By age 90, 10% give this answer.
    >
    > ...For the 1989, 1998, 2001, and 2004 surveys, we compute the ratios of income, education, and net worth for borrowers to the population as a whole, by age group; results are presented in the online appendix. We find that within age groups, borrowers almost always have higher levels of income and education than the population as a whole, and often have higher levels of net worth. Moreover, older borrowers appear to have relatively higher levels of income and education relative to their peers than middle-aged borrowers do. Hence these data suggest that selection effects by age go in the opposite direction: older borrowers appear to be a better pool than middle-aged borrowers. We present additional results in the online appendix showing that borrowing by age does not appear to vary by race, and that older borrowers do not appear to have disproportionately lower incomes, FICO score, or higher debt levels. None of these analyses lend support to the idea that sample selection effects contribute to the U-shape patterns that we see in the data.
    >
    > ...The effects we find have a wide range of dollar magnitudes, reported in Table 4. We estimate that, for home-equity lines of credit, 75-year-olds pay about $265 more each year than 50-year-olds, and 25-year-olds pay about $295 more. For other quantities, say, credit card fees, the implied age differentials are small - roughly $10-$20 per year for each kind of fee. The importance of the U-shaped effects we estimate goes beyond the economic significance of each individual choice, however: it lies in the fact that the appearance of a U-shaped pattern of costs in such a wide variety of circumstances points to a phenomenon that might apply to many areas.
[^old-attitudes]: Perhaps surprisingly, the common wisdom that people adopt conservative attitudes as part of the aging process may not be correct, and the observed conservatism of old people due to their coming from a more conservative time (ie. the past, as the 20th century saw a grand sweep of liberal beliefs through First World societies); ["Population Aging, Intracohort Aging, and Sociopolitical Attitudes"](/docs/dnb/2007-danigelis.pdf), Danigelis et al 2007's abstract ([excerpts](http://groups.google.com/group/brain-training/browse_thread/thread/4252f4072cd75684#body)):

    > Prevailing stereotypes of older people hold that their attitudes are inflexible or that aging tends to promote increasing conservatism in sociopolitical outlook. In spite of mounting scientific evidence demonstrating that learning, adaptation, and reassessment are behaviors in which older people can and do engage, the stereotype persists. We use U.S. General Social Survey data from 25 surveys between 1972 and 2004 to formally assess the magnitude and direction of changes in attitudes that occur within cohorts at different stages of the life course. We decompose changes in sociopolitical attitudes into the proportions attributable to cohort succession and intracohort aging for three categories of items: attitudes toward historically subordinate groups, civil liberties, and privacy. We find that significant intracohort change in attitudes occurs in cohorts-in-later-stages (age 60 and older) as well as cohorts-in-earlier-stages (ages 18 to 39), that the change for cohorts-in-later-stages is frequently greater than that for cohorts-in-earlier-stages, and that the direction of change is most often toward increased tolerance rather than increased conservatism. These findings are discussed within the context of population aging and development.

From the optimistic perspective, Salthouse tested Fortune 500 CEOs and found that their membership by average age didn't start dropping until their 60s, suggesting that they remained reasonably mentally sharp or were, in practice, compensating for the many insults of age;[^ceos] this way of thinking has obvious flaws for the rest of us.

[^ceos]: ["This Is Your Brain. Aging. Science is reshaping what we know about getting older. (The news is better than you think.)"](http://www.newsweek.com/2010/06/18/this-is-your-brain-aging.print.html), _Newsweek_:

    > The [Salthouse] graph shows two roller-coastering lines. One represents the proportion of people of each age who are in the top 25% on a standard lab test of reasoning ability-thinking. The other shows the proportion of CEOs of [Fortune 500](!Wikipedia) companies of each age. Reasoning ability peaks at about age 28 and then plummets, tracing that well-known plunge that makes those older than 30 (OK, fine, 40) cringe: only 6% of top scorers are in their 50s, and only 4% are in their 60s. But the age distribution of CEOs is an almost perfect mirror image: it peaks just before age 60. About half are older than 55. And the number under 40 is about zero.
    >
    > ...Salt-house deduces more counterintuitive, and hopeful, lessons. The first is that in real life, rather than in psych labs, people rely on mental abilities that stand up very well to age and discover work-arounds for the mental skills that do fade.

There are a number of results indicating that the elderly, perhaps because they have so much severer cognitive deficits than the young, respond better to treatment. (This is common in [Nootropics](), finding that something does not work in the young but does in the elderly: eg. [creatine](#creatine).) IQ gains in young adults are difficult and minimal even in [Jaeggi 2008](#jaeggi-2008), but older adults improve about as much as young adults in [Brehmer et al 2012](http://www.frontiersin.org/human_neuroscience/abstract/22906 "Working-memory training in younger and older adults: Training gains, transfer, and maintenance") and instructing older adults to think aloud during an IQ test boosts scores (yet not younger adults)[^fox], and training >65-year olds in one adaptive WM task similar to SNB lead to gains of ~6 IQ points on the [Cattell Culture Fair IQ Test](!Wikipedia "Cattell Culture Fair III") which were still present 8 months later; ["Working Memory Training in Older Adults: Evidence of Transfer and Maintenance Effects"](/docs/dnb/2010-borella.pdf) & [Carretti et al 2012](/docs/dnb/2012-carretti.pdf "Benefits of training working memory in amnestic mild cognitive impairment: specific and transfer effects") makes for interesting reading[^Borella]:

> Few studies have examined working memory (WM) training-related gains and their transfer and maintenance effects in older adults. This present research investigates the efficacy of a verbal WM training program in adults aged 65-75 years, considering specific training gains on a verbal WM (criterion) task as well as transfer effects on measures of visuospatial WM, short-term memory, inhibition, processing speed, and fluid intelligence. Maintenance of training benefits was evaluated at 8-month follow-up. Trained older adults showed higher performance than did controls on the criterion task and maintained this benefit after 8 months. Substantial general transfer effects were found for the trained group, but not for the control one. Transfer maintenance gains were found at follow-up, but only for fluid intelligence and processing speed tasks. The results are discussed in terms of cognitive plasticity in older adults.

[^fox]: ["How to Gain Eleven IQ Points in Ten Minutes: Thinking Aloud Improves Raven's Matrices Performance in Older Adults"](http://www.tandfonline.com/doi/pdf/10.1080/13825580903042668), Fox et al 2009:

    > "Few studies have examined the impact of age on reactivity to concurrent think-aloud (TA) verbal reports. An initial study with 30 younger and 31 older adults revealed that thinking aloud improves older adult performance on a short form of the Raven's Matrices (Bors & Stokes, 1998, _Educational and Psychological Measurement_, 58, p. 382) but did not affect other tasks. In the replication experiment, 30 older adults (mean age = 73.0) performed the Raven's Matrices and three other tasks to replicate and extend the findings of the initial study. Once again older adults performed significantly better only on the Raven's Matrices while thinking aloud. Performance gains on this task were substantial (_d_ = 0.73 and 0.92 in Experiments 1 and 2, respectively), corresponding to a fluid intelligence increase of nearly one standard deviation."
[^Borella]: Some relevant excerpts:

    > Buschkuehl et al. (2008) proposed an adaptive visual WM training program to old-old adults: Their results showed substantial gains in the WM trained tasks. Short and long-term transfer effects were found only for tasks with the same stimuli content. Similarly, Li et al. (2008) found in young and older adults specific improvement in the task practiced-a spatial 2 n-back WM task-that involved two conditions: one standard, one more demanding. Transfer effects were found on a more demanding 3 n-back visual task as well as on numerical n-back tasks. Although near transfer effects to the same (visual) and also different (numerical) modality were shown, no far transfer effects to more complex WM tasks (operation and rotation span tests) were found.
    > With regard to maintenance effects, Buschkuehl et al. (2008) failed to find any maintenance 1 year after completion of training, in comparison with pretest. In contrast, Li et al. (2008) showed a maintenance of practice gains and of near-transfer effects at 3-month follow-up; nonetheless, in contrast with young adults, older participants showed a performance decrement from postpractice to follow-up.
    >
    > ...Common measures used in cognitive aging research, and theoretically related to WM, were chosen: short-term memory, fluid intelligence, inhibition, and processing speed (Craik & Salthouse, 2000; Verhaeghen, Steitz, Sliwinski, & Cerella, 2003). For nearest-transfer effects, a visuospatial WM task (Dot Matrix task; adapted from Miyake, Friedman, Rettinger, Shah, & Hegarty, 2001) was included. This task involves processes (elaboration and processing phase) similar to the one practiced. However, the nature of the material and the secondary requirement are different from those of the trained task. The Forward and Backward Digit Span tests were used to assess near-transfer effects because they are part of the general memory factor, but the task requests were different from those of the WM tasks (see Bopp & Verhaeghen, 2005). Because these tasks measure the same narrow or same broad ability, we expect transfer effects onto them. To determine the presence of far transfer effects, we chose classic tasks: the Cattell task to measure nonverbal reasoning ability; the Stroop Color test to index inhibition-related mechanisms; and the Pattern Comparison test to assess processing speed. The transfer abilities were chosen with consideration of their relationship to WM processes. Working memory impairment in older adults is generally attributed to general mechanisms such as inhibition and processing speed (Borella et al., 2008). Furthermore, WM is frequently advanced as one of the mechanisms that also accounts for age-related differences in intelligence tasks (de Ribaupierre & Lecerf, 2006; Rabbitt & Lowe, 2000; Schaie & Hertzog, 1986)...
    >
    > The Categorization Working Memory Span task (CWMS; Borella et al. 2008; De Beni, Borella, Carretti, Marigo, & Nava, 2008) is similar to the classic WM tasks, such as the Listening Span test (Borella et al., 2008), the only difference being that it involves processing lists of words rather than sentences, limiting the role of semantic processing. The materials consisted of 10 sets of words, each set comprising 20 lists of words, which were organized in series of word lists of different lengths (from 2 to 6). Each list contained 5 words of high-medium frequency. Furthermore, the lists contained zero, one, or two animal nouns, present in any position, including last. An example list is house, mother, dog, word, night. Of the total number of words (200) in the task, 28% were animal words.
    > Participants listened to the lists of words audiorecorded presented at a rate of 1 s per word and had to tap their hand on the table whenever they heard an animal noun (processing phase). The interval between series of word lists was 2 s (the presentation was thus paced by the experimenter). At the end of the series, participants recalled the last word of each string in serial order (maintenance phase). Two practice trials of 2-word length were given before the experiment started.
    > Words recalled were written down by the experimenter on a prepared form. The total number of correctly recalled words was used as the measure of WM performance (maximum score 20). This score has been demonstrated to show large correlations with visuospatial (Jigsaw Puzzle test) and verbal (Listening Span test) WM tasks (Borella et al., 2008), and measures of fluid intelligence (Borella et al., 2006).
    >
    > ...Culture Fair test, Scale 3 (Cattell & Cattell, 1963). Scale 3 of the Cattell test consists of two parallel forms (A and B), each containing four subtests to be completed in 2.5 to 4 min, depending on the subtest. In the first subtest, Series, participants saw an incomplete series of abstract shapes and figures and had to choose from six alternatives that best completed the series. In the second subtest, Classifications, participants saw 14 problems comprising abstract shapes and figures and had to choose which 2 of the 5 differed from the other 3. In the third subtest, Matrices, participants were presented with 13 incomplete matrices containing four to nine boxes of abstract figures and shapes plus an empty box and six choices: Their task was to select the answer that correctly completed each matrix. In the final subtest, Conditions, participants were presented with 10 sets of abstract figures, lines, and a single dot, along with five alternatives: Their task was to assess the relationship among the dot, figures, and lines, then choose the alternative in which a dot could be positioned in the same relationship.
    > The dependent variable was the number of correctly solved items across the four subsets (maximum score of 50).
    > One of the two parallel forms (A or B) was administered at pretest, the other at posttest in counterbalanced fashion across testing sessions.
    >
    > ...Far-transfer effect. For the Cattell test, results indicated that trained participants performed significantly better than did controls (Mdiff ϭ 3.22, p Ͻ .001). Posttest and follow-up performances were significantly better than on pretest (Mdiff ϭ 3.40, p Ͻ .001, and Mdiff ϭ 2.75, p Ͻ .001, respectively). No significant difference was found between posttest and follow-up. Post hoc comparisons revealed that only the trained group showed significant improvement in performance between pretest and both posttest ( p Ͻ .001) and follow-up ( p Ͻ .001), although posttest performance was not different from that of follow-up. By contrast, no significant difference was found for the control group. The trained group performed better at both posttest and follow-up than did the control group ( p Ͻ .001).
    >
    > ...First, the participants involved in our study were young-old (mean age of 69 years), whereas in Buschkuehl et al.'s (2008) study as well as that of Li et al. (2008), they were old-old adults (mean age of 80.1 and 74.5 years, respectively). In the context of episodic memory, the meta-analysis by Verhaeghen et al. (1992) has pointed out that the benefit of interventions is negatively related to participant age (see also Singer, Lindenberger, & Baltes, 2003). It has been shown that cognitive plasticity is reduced over the adult life span (Jones et al., 2006), with young-old exhibiting larger training-related gains than old-old (Singer et al., 2003). The importance of participant age is evident from considering the results of training focused on executive control tasks-for example, task-switching (Buchler, Hoyer, & Cerella, 2008; Karbach & Kray, 2009; Kramer, Hahn, & Gopher, 1999), dual tasks (Bherer et al., 2005, 2008), or general executive functions (Basak et al., 2008)-for which transfer effects emerged with a sample comprising young-old (age range between 60 and 75 years, mean age between 65 and 71 years; Basak et al., 2008; Bherer et al., 2005, 2008; Karbach & Kray, 2009; Kramer et al., 1995). The question of whether transfer effects of WM training can also be determined by participant age range is of interest and should be addressed in further research.
    >
    > Second, as is mentioned at the beginning of this section, the task and the procedure used to train participants can be considered an important source of difference. For example, Buschkuehl et al. (2008) reported that trained participants claimed to have generated task-specific strategies in one of the variants of the WM task in which they were trained, leading to greater training gains (62%) with respect to the other two variants (44% and 15%, respectively). The difficulty of transferring the gains obtained in a specific task to other tasks suggests that the WM training by Buschkuehl et al. did not foster an increase in flexibility, but simply the tendency to find a strategy to recall as many items as possible but in the context of each WM task. In the case of Li et al. (2008), the modest transfer effects to the WM task can be explained by reflecting on the nature of the trained task: n-back task, which involves the manipulation and maintenance of information as well as updating of temporal order and contextual information and binding processes between stimuli and certain representation (Oberauer, 2005). Although the n-back shares common processing mechanisms with complex span tasks, the underlying mechanisms of the n-back are not completely understood (Schmiedek, Hildebrandt, Lövden, Wilhelm, & Lindenberger, 2009). Moreover, the few studies that used it with other WM tasks- complex span tasks- have shown variable correlations (from very low or null-Kane, Conway, Miura, & Colflesh, 2007; Roberts & Gibson, 2002-to large-Schmiedek et al., 2009; Shamosh et al., 2008).

<!-- http://www.cla.temple.edu/cnl/research/documents/richmondetal2011.pdf -->
<!--
    For example,
Borella, Carretti, Riboldi, and De Bini (2010) trained older adults on a WM-span-like
task, and compared them to a control group of older adults that completed questionnaires
instead. They reported positive transfer to several tasks, including the Cattell Culture-Fair
Test. However, other studies have been less optimistic. For example, three other recent
studies reported no transfer to different versions of the Raven Progressive Matrices after
WM-span training in older adults (Brehmer, Westerberg, & Bäckman, 2012; Richmond,
Morrison, Chein, & Olson, 2011; Zinke, Zeintl, Eschen, Herzog, & Kliegel, 2012).
Likewise, a recent review (Shipstead et al., in press) found that the majority of published
studies using developmental and patient samples have not observed transfer to fluid
intelligence after WM training. The Melby-Lervåg and Hulme (2013) meta-analysis
confirmed that age was not a significant moderator of transfer to nonverbal abilities after
WM training. In fact, young children (d = .03) and adolescent children (d = -.05) showed
no evidence of transfer to nonverbal intelligence, inconsistent with the idea that younger
children may be more susceptible to WM training and improvements in intelligence
because of their increased brain plasticity relative to adults. Again, our findings do not
appear to be an aberration - there is little evidence for transfer from WM training to fluid
intelligence.
-->

For more on aging and the brain, [Mike Darwin](!Wikipedia) [recommends reading](http://chronopause.com/chronopause.com/index.php/2011/05/31/going-going-gone%E2%80%A6-part-2/index.html)

> Hedden T, Gabrieli JD. _Nat Rev Neuroscience._ 2004 Feb;5(2):87-96. 'Insights into the aging mind: a view from cognitive neuroscience'. PMID 14735112, which is available as full text from this link: <http://brainybehavior.com/blog/wp-content/uploads/2007/11/agingbrain.pdf>. I cannot recommend this paper highly enough. Additionally, the Salt Cognitive Aging Laboratory, which oversees the Virginia Cognitive Aging Project (VCAP) at the University of Virginia, is the premier facility in the US (and arguably the world) undertaking active, longitudinal studies of aging. The VCAP study has done comprehensive cognitive assessments in adults ranging from 18 to 98 years of age. Approximately 3,800 adults have participated in their three-session (6-8 hour) assessment at least once, with about 1,600 participating at least twice, and about 450 of them participating three or more times. The data from this project have served as the basis for a veritable cornucopia of scientific publications which are available in the Resources Section of their website <http://faculty.virginia.edu/cogage/links/publications/>. Nearly 200 papers on the cognitive impact of aging are available free of charge on their website. It is necessary to register with your name and email address to access the papers, but it is well worth it.

## TODO

Others to follow up on:

- McNab F, Klingberg T (2008) Prefrontal cortex and basal ganglia control access to working memory. Nat Neurosci 11:103-107.

- Colom et al. ["Memory Span and General Intelligence: A Latent-Variable Approach"](http://www.eric.ed.gov/ERICWebPortal/custom/portlets/recordDetails/detailmini.jsp?_nfpb=true&_&ERICExtSearch_SearchValue_0=EJ724232&ERICExtSearch_SearchType_0=no&accno=EJ724232); _Intelligence_, v33 n6 p623-642 Nov-Dec 2005

> There are several studies showing that working memory and intelligence are strongly related. However, working memory tasks require simultaneous processing and storage, so the causes of their relationship with intelligence are currently a matter of discussion. The present study examined the simultaneous relationships among short-term memory (STM), working memory (WM), and general intelligence (_g_). Two hundred and eight participants performed six verbal, quantitative, and spatial STM tasks, six verbal, quantitative, and spatial WM tasks, and eight tests measuring fluid, crystallized, spatial, and quantitative intelligence. Especial care is taken to avoid misrepresenting the relations among the constructs being studied because of specific task variance. Structural equation modeling (SEM) results revealed that (a) WM and g are (almost) isomorphic constructs, (b) the isomorphism vanishes when the storage component of WM is partialed out, and (c) STM and WM (with its storage component partialed out) predict _g_.

- Colom et al. "General intelligence and memory span: Evidence for a common neuroanatomic framework"; _Cognitive Neuropsychology_, Volume 24, Issue 8 December 2007 , pages 867 - 878

> General intelligence (_g_) is highly correlated with working-memory capacity (WMC). It has been argued that these central psychological constructs should share common neural systems. The present study examines this hypothesis using structural magnetic resonance imaging to determine any overlap in brain areas where regional grey matter volumes are correlated to measures of general intelligence and to memory span. In normal volunteers (N = 48) the results (p < .05, corrected for multiple comparisons) indicate that a common anatomic framework for these constructs implicates mainly frontal grey matter regions belonging to Brodmann area (BA) 10 (right superior frontal gyrus and left middle frontal gyrus) and, to a lesser degree, the right inferior parietal lobule (BA 40). These findings support the nuclear role of a discrete parieto-frontal network.

# Software
## Online

There are many free implementations in Flash etc. online:

- <http://themindflow.com/>
- <http://dual-n-back.com/nback.thml>
- <http://cognitivefun.net/test/22>
- <http://cognitivefun.net/test/5>
- <http://www.soakyourhead.com/>
- <http://brainscale.net/>
- <http://www.cogtest.com/tests/cognitive_int/db.html> (Single N-back only)
- <http://alpha.brainturk.com/games>
- <http://namuol.github.com/banal-duck/>
- <http://www.teambrainz.com/>

Paid:

- [Lumosity](http://www.lumosity.com/training_applications/dual-n-back)
- [Mind Sparke](http://www.mindsparke.com/)

## Desktop

Free:

- [Brain Workshop](http://brainworkshop.sourceforge.net/) ([Free Software](!Wikipedia)/FLOSS).

    BW is the standard implementation used by members of the [DNB ML](http://groups.google.com/group/brain-training); it is Free, featureful, and well-supported.
- [hback](!Hackage) (FLOSS)

Paid:

- [Brain Twister](http://www.apn.psy.unibe.ch/content/application/braintwister) (the 'official' DNB program, used in the Jaeggi studies)
- [High IQ Pro](http://www.highiqpro.com/high-iq-pro)
- [Mind Sparke](http://www.mindsparke.com/)

## Mobile
### Android

See also Lucas Charles's August 2011 [review of 6 Android DNB apps](http://www.limitlesschannels.com/main/2011/8/10/android-n-back-app-comparison.html).

Free:

- [Brain N Back](https://market.android.com/details?id=quazar.BrainNBack) (Quazar)
- [Brain N-Back](https://market.android.com/details?id=phuc.entertainment.dualnback) (Phuc Nguyen)
- [Dual N-Back](https://market.android.com/details?id=com.Projet4A.DualNBack) (Polytech Marseille)
- [IQ Boost](https://market.android.com/details?id=appinventor.ai_viana_octavio.IQ_Boost_free) (the free version)
- [N-Back](https://market.android.com/details?id=cz.wie.p.nback) (Piotr Wieczorek)
- [Single N-back](https://market.android.com/details?id=com.ankerl.singlenback) (Martin Ankerl; note the title)

Paid:

- [IQ Boost](https://market.android.com/details?id=appinventor.ai_viana_octavio.IQ_Boost)
- [N-Back Maestro](http://www.appbrain.com/app/n-back-maestro/org.urbian.android.games.nback) (Urbian)
- [NBack](http://waterdev.com/apps/nback) (waterdev)
- [Brainturk](https://play.google.com/store/apps/details?id=com.bodhi.brainturk) (?)

### iPhone

Free:

- [BetterBrain Lite](https://itunes.apple.com/app/id307920888)

Paid:

- [BetterBrain](http://itunes.apple.com/us/app/betterbrain/id307922453)
- [Finger Friendly N-back Suite](http://itunes.apple.com/us/app/n-back-suite/id306071267)
- [IQ Training](http://itunes.apple.com/us/app/iq-training/id322311996)
- [IQboost](http://itunes.apple.com/us/app/iq-boost/id286574399)
- [Mind Sparke](http://itunes.apple.com/us/app/brain-fitness-pro/id325059712)
- [iBrain Fit-IQ](http://itunes.apple.com/us/app/ibrain-fit-iq/id318329007)
- [Double Dynamo](https://itunes.apple.com/us/app/double-dynamo-matching-rhythm/id703947966) ([video gameplay demonstration](http://www.youtube.com/watch?v=VoU5dNfbeNE))
- [Brainturk Brain Trainer](https://itunes.apple.com/WebObjects/MZStore.woa/wa/viewSoftware?id=787822673)

### Windows

- [Dual N-Back](http://www.windowsphone.com/en-us/store/app/dual-n-back/2fd9dc9e-d117-44d1-9a62-b43ad1b09bf4)

## Offline N-back

You can play N-back in the real world, without a computer, if you like. See the ML thread ["Non-electronic game version of N-back task"](http://groups.google.com/group/brain-training/browse_thread/thread/e85b55de47df536d) and the [SnapBack rules](http://www.toothycat.net/wiki/wiki.pl?DouglasReay/SnapBackGameRules). Jonathan Toomin points out that N-back can be easily done with a deck of cards alone, and the FAQ's author [suggests](http://groups.google.com/group/brain-training/browse_thread/thread/1dc53aa5f88e5c81) a simple mental arithmetic routine suitable for meditation that is much like SNB.

# What else can I do?

[tDCS](!Wikipedia) may increase WM, although it remains unclear whether the performance gains persist afterwards. See [Boggio et al 2005](http://www.tmslab.org/publications/154.pdf "Effects of transcranial direct current stimulation on working memory in patients with Parkinson's disease"), [Fregni et al 2005](/docs/dnb/2005-fregni.pdf "Anodal transcranial direct current stimulation of prefrontal cortex enhances working memory"), [Ohn et al 2007](http://diyhpl.us/~bryan/papers2/neuro/Time-dependent%20effect%20of%20transcranial%20direct%20current%20stimulation%20on%20the%20enhancement%20of%20working%20memory.pdf "Time-dependent effect of transcranial direct current stimulation on the enhancement of working memory"), [Boggio et al 2008](/docs/dnb/2008-boggio.pdf "Temporal cortex direct current stimulation enhances performance on a visual recognition memory task in Alzheimer disease"), [Andrews et al 2011](/docs/dnb/2011-andrews.pdf "Improving working memory: the effect of combining cognitive activity and anodal transcranial direct current stimulation to the left dorsolateral prefrontal cortex") & [Tseng et al 2012](http://www.journalofvision.org/content/12/9/177.abstract?sid=8ab3ce55-344f-4b31-a6c1-c711d060237a "Improving visual working memory performance with transcranial direct current stimulation") (but also [Marshall et al 2005](http://www.biomedcentral.com/1471-2202/6/23/ "Bifrontal transcranial direct current stimulation slows reaction time in a working memory task")). One informal & incomplete [tDCS-DNB](http://lesswrong.com/r/discussion/lw/aa1/link_shutting_down_the_destructive_internal/5x3v#body_t1_5x3v) experiment showed no trend towards benefit.

Forum members have recommended a number of other things for general mental fitness:

- [Spaced repetition]() programs such as [Mnemosyne](!Wikipedia "Mnemosyne (software)") are very useful for exploiting the spacing effect in order to memorize & remember things
- Buddhist-style [meditation](!Wikipedia) has been recommended (there is a good [Vipassana](!Wikipedia) textbook available online; see [_Mindfulness in Plain English_](http://www.urbandharma.org/udharma4/mpe.html), and the <http://openfocus.com/> website has been mentioned). Meditation has been [well-studied](!Wikipedia "Research on meditation") and shown to induce [physical changes](http://www.newscientist.com/article/dn8317-meditation-builds-up-the-brain.html) and [improve executive function & WM](http://www.sciencedaily.com/releases/2010/04/100414184220.htm), S2B performance ([local copy of Zeidan 2010](/docs/dnb/2010-zeidan.pdf)), attention^[Specifically, performance on [attentional blink](!Wikipedia); see ["Mental Training Affects Distribution of Limited Brain Resources"](http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.0050138) (Slagter 2007); cf. ["Study Suggests Meditation Can Help Train Attention "](http://www.nytimes.com/2007/05/08/health/psychology/08medi.html) (_New York Times_).], and [cardiovascular health](http://circ.ahajournals.org/cgi/content/meeting_abstract/120/18_MeetingAbstracts/S461-a)^[["Can Meditation Curb Heart Attacks?"](http://well.blogs.nytimes.com/2009/11/20/can-meditation-curb-heart-attacks/) (_New York Times_)] among many other results.
- One study claims that spatial ability is trainable and that there is transfer (from trained tasks to novel ones)[^spatial]. Suggested games include [4-D Rubik's Cube](http://www.superliminal.com/cube/cube.htm), [mental rotation](http://psych.hanover.edu/JavaTest/CLE/Cognition/Cognition/MentalRotation.html), [3-D Tetris](http://www.3dtris.de/), the [VZ-2 paper folding test](http://groups.google.com/group/brain-training/browse_frm/thread/52d802a9df720962), and the [Cambridge Brain Sciences](http://www.cambridgebrainsciences.com/) suite of games/tasks.
- [neurofeedback](!Wikipedia) can be similar to meditation and has been linked to physical changes in the brain;^[["First Direct Evidence of Neuroplastic Changes Following Brainwave Training"](http://www.sciencedaily.com/releases/2010/03/100310114936.htm) & ["Mindfulness Meditation Training Changes Brain Structure in Eight Weeks"](http://www.sciencedaily.com/releases/2011/01/110121144007.htm), _Science Daily_] [binaural beats](!Wikipedia) have also been discussed.
- Crypto [recommends](http://groups.google.com/group/brain-training/browse_thread/thread/8af44f3b20df9904) Win Wenger's ["image streaming"](http://www.winwenger.com/imstream.htm) as another mental exercise
- UOchris1 [reports](http://groups.google.com/group/brain-training/browse_thread/thread/18eeddd23451f1f0) very positive results while working through a mental exercise regimen developed by an American performer from the 1920s who specialized in doing multiple mental tasks simultaneously; he is using a _Stand Magazine_ article on Harry Kahne and Kahne's ["The Multiple Mentality Course"](http://www.rexresearch.com/kahne/kahne.htm) as resources.
- Exercise is right up there with nutrition and sleep! See previous discussion.
- I personally believe that the habit of listening to music - pervasive among students and especially university students - is deeply harmful to any serious thinking or learning. I've noticed my DNB scores seem damaged by my favorite music, although I have not done any experiments to test this theory. The existing research seems to agree with me in finding enjoyable music a distract[^music-distraction-1][^music-distraction-2].

[^spatial]: _Psychonomic Bulletin & Review_ 2008 Aug;15(4):763-71. "Training generalized spatial skills." Wright R, Thompson WL, Ganis G, Newcombe NS, Kosslyn SM.

     > ...The present study investigated whether intensive long-term practice leads to change that transcends stimulus and task parameters. Thirty-one participants (14 male, 17 female) were tested on three cognitive tasks: a computerized version of the Shepard-Metzler (1971) mental rotation task (MRT), a mental paper-folding task (MPFT), and a verbal analogies task (VAT). Each individual then participated in daily practice sessions with the MRT or the MPFT over 21 days. Postpractice comparisons revealed transfer of practice gains to novel stimuli for the practiced task, as well as transfer to the other, nonpracticed spatial task. Thus, practice effects were process based, not instance based. Improvement in the nonpracticed spatial task was greater than that in the VAT; thus, improvement was not merely due to greater ease with computerized testing.
[^music-distraction-1]: [Summary](http://bps-research-digest.blogspot.com/2012/08/music-we-like-is-more-distracting-than.html) of [Perham & Sykora 2012](/docs/dnb/2012-perham.pdf "Disliked Music can be Better for Performance than Liked Music")

    > In lab studies, people who listen to music they like, generally perform better at mental tasks afterwards, an effect that's been attributed to boosts in mood and arousal. But what about the effect of background music that plays on during a task - more akin what we do in real life? This is actually less studied. The traditional mood-arousal literature would predict it to be beneficial too, especially if the music is to the listener's taste.
    >
    > However, there's another line of research, known as the "Irrelevant Sound Effect", that's all about the way background sounds can interfere with our short-term memory for ordered lists, which would be a bad thing for many work-related tasks. These studies show that the distraction is greater when the sound is more acoustically varied - just like your typical pop song. Based on this, Nick Perham and Martinne Sykora made a counter-intuitive prediction - background music that you like will be more detrimental to working memory than unappealing music, so long as the liked music has more acoustical variation than the disliked music.
    >
    > Twenty-five undergrads completed several serial recall tasks. They were presented with strings of eight consonants and had to repeat them back from memory in the correct order. Performance was best in the quiet condition, but the key finding was that participants' performance was worse when they completed the memory task with a song they liked playing over headphones (Infernal's "From Paris to Berlin"), compared with a song they disliked (songs such as "Acid Bath" from the grind core metal band Repulsion). In case you're wondering, participants who liked Repulsion were excluded from the study.

    Particularly relevant, since people like to claim their favorite music "helps them focus":

    > A further intriguing detail from the study is the participants' lack of insight into the degree of distraction associated with each type of music. Asked to judge their own performance, they determined correctly that their memory was more accurate in the quiet condition, but they didn't realise that their performance was poorest whilst listening to the music they liked.
[^music-distraction-2]: ["Background music as a risk factor for distraction among young-novice drivers"](/docs/dnb/2013-brodsky.pdf), Brodsky & Slor 2013:

    > ...The current study explored the effects of driver-preferred music on driver behavior. 85 young-novice drivers completed six trips in an instrumented Learners Vehicle. The study found that all participants committed at-least 3 driver deficiencies; 27 needed a verbal warning/command and 17 required a steering or braking intervention to prevent an accident. While there were elevated positive moods and enjoyment for trips with driver-preferred music, this background also produced the most frequent severe driver miscalculations and inaccuracies, violations, and aggressive driving...

## Supplements

[Nootropics](!Wikipedia) (see [Nootropics]() for the author's own experiences with them), may help boost performance. The relation of caffeine to learning & memory is complicated; for now, see [the thread on it](http://groups.google.com/group/brain-training/browse_thread/thread/74991713608a29a5) or my [Nootropics page](Nootropics#caffeine).

### Piracetam

A useful pharmaceutical is [piracetam](!Wikipedia); [TheQ17](http://groups.google.com/group/brain-training/browse_thread/thread/e9074921a905a7fa) mentions that "Personally, I have found piracetam be quite useful in helping me stay alert and focused during long study hours or doing redundant tasks." [Other members](http://groups.google.com/group/brain-training/browse_thread/thread/4482aa213c8cbdf4/d1456393d7083660) also swear by piracetam+choline.

The author of this FAQ [reports that](Nootropics#piracetam) piracetam and [choline](!Wikipedia) helped reduced mental fatigue and gave a small (~10%) increase in his D4B score.

### Huperzine

[Reece](http://groups.google.com/group/brain-training/browse_thread/thread/3de2bf7a8e70949c) writes

> I've tried [huperzine](!Wikipedia) [a chemical extracted from an herb] (actually been using it for about a year now) and it is quite effective for both lucid dreaming and increasing dream recall if taken shortly before bed, not to mention the other benefits you'd expect from a potent [acetylcholinesterase](!Wikipedia) inhibitor. I haven't had anything in the way of negative side effects when I've stuck to a 5 day/week dosage of 200mcg.
>
> I've never tried piracetam, however [oxiracetam](!Wikipedia) felt like a placebo when compared to the benefits I've received from huperzine A. At larger doses, I've found huperzine A to be far more powerful than any nootropic I've ever tried (haven't tried any prescription meds such as [deprenyl](!Wikipedia "Selegiline")), however the side effects such as blurry vision and light-headedness weren't something I could tolerate.

He further compared their effects:

> I found Oxiracetam to have a somewhat "speedy" effect -- you would certainly know you took something if someone slipped that in your drink! As for effects, Oxiracetam seemed to help most with verbal fluency (auditory working memory?) and creativity. Huperzine helped more with working memory although it didn't have some of the interesting effects Oxiracetam had on creativity, nor the speedy rush that sometimes seemed like a powerful motivator to get work done.

(Reece did not take the oxiracetam with any choline supplements, which is usually recommended.)

### Creatine

In the realm of unusual supplements to n-backing, we can include [creatine possibly increasing intelligence](Creatine), but the evidence is too weak to say much.

# See also

- The author's own Brain Workshop statistics can be found [here](/docs/dnb/gwern-bw-stats.txt)

# Appendix
## Flaws in mainstream science (and psychology)

> Statistical background: [Against null-hypothesis significance testing](http://lesswrong.com/lw/g13/against_nhst/)

Mainstream science is flawed: seriously mistaken statistics combined with poor incentives has led to masses of misleading research. Not that this problem is exclusive to psychology. Medical science in general is often on very shaky ground. The basic nature of ['significance'](!Wikipedia "Statistical significance") being usually defined as _p_<0.05 means we should expect something like >5% of studies or experiments to be bogus (optimistically), but that only considers "false positives"; reducing "false negatives" requires [statistical power](!Wikipedia) (weakened by small samples), and the two combine with the base rate of true underlying effects into a total error rate. [Ioannidis 2005](http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124) points out that considering the usual _p_ values, the underpowered nature of many studies, the rarity of underlying effects, and a little bias, even large randomized trials may wind up with only an [85%](http://www.plosmedicine.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pmed.0020124.t004&representation=PNG_M "'Table 4. PPV of Research Findings for Various Combinations of Power (1 - ß), Ratio of True to Not-True Relationships (R), and Bias (u)', Ioannidis 2005") chance of having yielded the truth. [One survey](http://arxiv.org/pdf/1301.3718v1 "'Empirical estimates suggest most published medical research is true', Lager & Leek 2013") of reported _p_-values in medicine yielding a lower bound of false positives of 17%.

Yet, there are [too](http://www.nature.com/news/replication-studies-bad-copy-1.10634 "Replication studies: Bad copy: In the wake of high-profile controversies, psychologists are facing up to problems with replication") [many positive results](!Wikipedia "Publication bias")^[Publication bias can come in many forms, and seems to be severe. For example, the 2008 version of a Cochrane review (["Full publication of results initially presented in abstracts (Review)"](/docs/dnb/2008-scherer.pdf)) finds "Only 63% of results from abstracts describing randomized or controlled clinical trials are published in full. 'Positive' results were more frequently published than not 'positive' results."] ([psychiatry](http://archpsyc.ama-assn.org/cgi/content/abstract/archgenpsychiatry.2011.28), [cancer](/docs/dnb/2007-kyzas.pdf "'Almost all articles on cancer prognostic markers report statistically significant results', Kyzas et al 2007") or [neurobiology](http://www.psychiatry.freeuk.com/Excessvolume.pdf "'Excess Significance Bias in the Literature on Brain Volume Abnormalities', Ioannidis 2011") [biomedicine](/docs/dnb/2007-kyzas.pdf "'Almost all articles on cancer prognostic markers report statistically significant results', Kyzas et al 2007"), [biology](/docs/dnb/1996-csada.pdf "'The `File Drawer Problem` of Non-Significant Results: Does It Apply to Biological Research?', Csada et al 1996"), [ecology & evolution](/docs/dnb/2002-jennions.pdf "'Publication bias in ecology and evolution: an empirical assessment using the trim-and-fill method', Jennions & Moeller 2002"), [psy](http://www.bdat.nl/Bakker%20Van%20Dijk%20Wicherts%202012.pdf "'The Rules of the Game Called Psychological Science', Bakker et al 2012")[chology](/docs/dnb/1995-sterling.pdf "'Publication Decisions Revisited: The Effect of the Outcome of Statistical Tests on the Decision to Publish and Vice Versa', Sterling et al 1995") [2](/docs/dnb/2012-masicampo.pdf "'A peculiar prevalence of _p_ values just below .05', Masicampo & Lalande 2012") [3](http://www3.nd.edu/~ghaeffel/Theories_Ferguson.pdf "'A Vast Graveyard of Undead Theories: Publication Bias and Psychological Science's Aversion to the Null',  Ferguson & Heene 2012"), [economics](http://ideas.repec.org/a/eee/ecolet/v91y2006i3p395-401.html)' [top journals](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2089580 "'Star Wars: The Empirics Strike Back', Brodeur et al 2012"), [sociology](http://www.stanford.edu/~neilm/smr.pdf "'Publication Bias in Empirical Sociological Research: Do Arbitrary Significance Levels Distort Published Results?', Gerber & Malhotra 2008"), [gene-disease correlations](http://www.genetsim.org/workshop/201106/readings/ioannidis2001.pdf "'Replication validity of genetic association studies', Ioannidis et al 2001")) given [effect sizes](!Wikipedia) (and positive results correlate with per capita publishing rates in [US states](http://commonsenseatheism.com/wp-content/uploads/2011/09/Fanelli-Do-pressures-to-publish-increase-scientists-bias.pdf "'Do Pressures to Publish Increase Scientists` Bias? An Empirical Support from US States Data', Fanelli 2010") & vary by [period & country](https://web.archive.org/web/20130510182827/http://mres.gmu.edu/pmwiki/uploads/Main/Fanelli2011.pdf "'Negative results are disappearing from most disciplines and countries', Fanelli 2011") - apparently random chance is kind to scientists who must publish a lot and recently!); then there come the inadvertent errors which might cause retraction, which is rare, but the true retraction rate may be 0.1-1% (["How many scientific papers should be retracted?"](http://www.nature.com/embor/journal/v8/n5/full/7400970.html)), is increasing & seems to positively [correlate with journal quality](http://iai.asm.org/cgi/reprint/IAI.05661-11v1.pdf) (modulo the confounding factor that famous papers/journals get more scrutiny), not that [anyone pays](http://jama.ama-assn.org/content/298/21/2517.short) [any attention](http://www.esajournals.org/doi/pdf/10.1890/ES10-00142.1) to such things; then there are basic statistical errors in >11% of papers (based on the high-quality papers in _Nature_ and the _British Medical Journal_; ["Incongruence between test statistics and P values in medical papers"](http://www.biomedcentral.com/1471-2288/4/13), García-Berthou 2004) or [50% in neuroscience](http://www.brainvitge.org/papers/23Erroneous%20analyses%20of%20interactions%20in%20neuroscience%20%20a%20problem%20of%20significance.pdf "'Erroneous analyses of interactions in neuroscience: a problem of significance', Nieuwenhuis et al 2011").

And only *then* can we get into replicating at all. See for example _[The Atlantic](!Wikipedia)_ article ["Lies, Damned Lies, and Medical Science"](http://www.theatlantic.com/magazine/archive/2010/11/lies-damned-lies-and-medical-science/8269) on [John P. A. Ioannidis](!Wikipedia)'s research showing 41% of the *most cited* medical research failed to be [replicated](!Wikipedia "Reproducibility") - were wrong. For details, you can see Ioannidis's ["Why Most Published Research Findings Are False"](http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124)[^Pashler-power], or Begley's [failed attempts to replicate 47](http://www.nature.com/nature/journal/v483/n7391/full/483531a.html "'Drug development: Raise standards for preclinical cancer research', Begley & Ellis 2012 ") of 53 articles on top cancer journals (leading to Booth's ["Begley's Six Rules"](http://lifescivc.com/2012/09/scientific-reproducibility-begleys-six-rules/); see also the [_Nature Biotechnology_ editorial](http://www.nature.com/nbt/journal/v30/n9/pdf/nbt.2335.pdf "Further confirmation needed: A new mechanism for independently replicating research findings is one of several changes required to improve the quality of the biomedical literature") & note that full details have not been published because [the researchers of the original studies demanded secrecy from Begley's team](http://www.nature.com/nature/journal/v485/n7396/full/485041e.html "Editorial note")), or Kumar & Nash 2011's ["Health Care Myth Busters: Is There a High Degree of Scientific Certainty in Modern Medicine?"](http://www.scientificamerican.com/article.cfm?id=demand-better-health-care-book&print=true) who write 'We could accurately say, "Half of what physicians do is wrong," or "Less than 20% of what physicians do has solid research to support it."' Nutritional epidemiology is something of a fish in a barrel; after Ioannidis, is anyone surprised that when [Young & Karr 2011](/docs/dnb/2011-young.pdf "Deming, data and observational studies: A process out of control and needing fixing") followed up on 52 correlations tested in 12 RCTs, 0/52 replicated and the RCTs found the opposite of 5?

[^Pashler-power]: For a second, shorter take on the implications of low prior probabilities & low power: ["Is the Replicability Crisis Overblown? Three Arguments Examined"](http://www3.nd.edu/~ghaeffel/Overblown_Pashler.pdf), Pashler & Harris 2012:

    > So what is the truth of the matter? To put it simply, adopting an alpha level of, say, 5% means that about 5% of the time when researchers test a null hypothesis that is true (i.e., when they look for a difference that does not exist), they will end up with a statistically significant difference (a Type 1 error or false positive.)1 Whereas some have argued that 5% would be too many mistakes to tolerate, it certainly would not constitute a flood of error. So what is the problem?
    >
    > Unfortunately, the problem is that the alpha level does not provide even a rough estimate, much less a true upper bound, on the likelihood that any given positive finding appearing in a scientific literature will be erroneous. To estimate what the literature-wide false positive likelihood is, several additional values, which can only be guessed at, need to be specified. We begin by considering some highly simplified scenarios. Although artificial, these have enough plausibility to provide some eye-opening conclusions.
    >
    > For the following example, let us suppose that 10% of the effects that researchers look for actually exist, which will be referred to here as the prior probability of an effect (i.e., the null hypothesis is true 90% of the time). Given an alpha of 5%, Type 1 errors will occur in 4.5% of the studies performed (90% × 5%). If one assumes that studies all have a power of, say, 80% to detect those effects that do exist, correct rejections of the null hypothesis will occur in 8% of the time (80% × 10%). If one further imagines that all positive results are published then this would mean that the probability any given published positive result is erroneous would be equal to the proportion of false positives divided by the sum of the proportion of false positives plus the proportion of correct rejections. Given the proportions specified above, then, we see that more than one third of published positive findings would be false positives [4.5% / (4.5% + 8%) = 36%]. In this example, the errors occur at a rate approximately seven times the nominal alpha level (row 1 of Table 1).
    >
    > Table 1 shows a few more hypothetical examples of how the frequency of false positives in the literature would depend upon the assumed probability of null hypothesis being false and the statistical power. An 80% power likely exceeds any realistic assumptions about psychology studies in general. For example, Bakker, van Dijk, and Wikkerts, (2012, this issue) estimate .35 as a typical power level in the psychological literature. If one modifies the previous example to assume a more plausible power level of 35%, the likelihood of positive results being false rises to 56% (second row of the table). John Ioannidis (2005b) did pioneering work to analyze (much more carefully and realistically than we do here) the proportion of results that are likely to be false, and he concluded that it could very easily be a majority of all reported effects.
    >
    > Prior probability of effect Power Proportion of studies yielding true positives  Proportion of studies yielding false positives Proportion of total positive results (false+positive) which are false
    > --------------------------- ----  ---------------------------------------------  ---------------------------------------------- ---------------------------------------------
    > 10%                         80%   10% x 80% = 8%                                 (100-10%) x 5% = 4.5%                          4.5% / (4.5% + 8%) = 36%
    > 10%                         35%   = 3.5%                                         = 4.5%                                         4.5% / (4.5% + 3.5%) = 56.25%
    > 50%                         35%   = 17.5%                                        (100-50%) x 5% = 2.5%                          2.5% / (2.5% + 17.5%) = 12.5%
    > 75%                         35%   = 26.3%                                        (100-75%) x 5% = 1.6%                          1.6% / (1.6% + 26.3%) = 5.73%
    >
    > Table: Table 1. Proportion of Positive Results That Are False Given Assumptions About Prior Probability of an Effect and Power.

Attempts to use animal models to infer anything about humans suffer from all the methodological problems previously mentioned[^animal-models], and add in interesting new forms of error such as [mice](http://www.slate.com/articles/health_and_science/the_mouse_trap/2011/11/lab_mice_are_they_limiting_our_understanding_of_human_disease_.html "The Mouse Trap: The dangers of using one lab animal to study every disease") simply being irrelevant to humans, leading to cases like <150 [sepsis](!Wikipedia) clinical trials all failing - because the drugs worked in mice but [humans have a completely different](http://www.pnas.org/content/early/2013/02/07/1222878110.short "'Genomic responses in mouse models poorly mimic human inflammatory diseases', Seok et al 2013") set of genetic reactions to inflammation.

[^animal-models]: On the general topic of animal model external validity & translation to humans:

    - ["The evaluation of anticancer drugs in dogs and monkeys for the prediction of qualitative toxicities in man"](/docs/dnb/1970-schein.pdf), Schein et al 1970; systematic review
    - ["Drug safety tests and subsequent clinical experience"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1436259/pdf/jrsocmed00293-0087.pdf), Fletcher 1978; systematic review
    - ["Response of Experimental Animals To human carcinogens: an analysis based upon the IARC Monographs programme"](/docs/dnb/1986-wilbourn.pdf), Wilbourn et al 1986; systematic review
    - ["Predictability of clinical adverse reactions of drugs by general pharmacology studies"](https://www.jstage.jst.go.jp/article/jts1976/20/2/20_2_77/_pdf), Igrashi et al 1992; systematic review
    - ["Genetics of Mouse Behavior: Interactions with Laboratory Environment"](http://psych.colorado.edu/~carey/pdfFiles/MouseLab_Crabbe.pdf), Crabbe et al 1999; experiment
    - ["Evidence-Based Data From Animal and Human Experimental Studies on Pain Relief With Antidepressants: A Structured Review"](/docs/dnb/2000-fishbain.pdf), Fishbain et al 2000; review
    - ["Concordance of the Toxicity of Pharmaceuticals in Humans and in Animals"](/docs/dnb/2000-olson.pdf), Olson et al 2000; survey
    - ["Nimodipine in animal model experiments of focal cerebral ischemia: a systematic review"](http://stroke.ahajournals.org/content/32/10/2433.full), Horn 2001; review
    - ["Wound healing in cell studies and animal model experiments by Low Level Laser Therapy; were clinical studies justified? A systematic review"](http://dare.uva.nl/document/156963#page=103), Lucas et al 2002; meta-analysis
    - ["Does animal experimentation inform human healthcare? Observations from a systematic review of international animal experiments on fluid resuscitation"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1122396/), Roberts et al 2002; meta-analysis
    - ["Systematic reviews of animal experiments"](/docs/dnb/2002-sandercock.pdf), Sandercock & Roberts 2002
    - ["Why did NMDA receptor antagonists fail clinical trials for stroke and traumatic brain injury?"](/docs/dnb/2002-ikonomidou.pdf), Ikonomidou & Turski 2002; essay
    - ["Meta-analysis of the effects of endothelin receptor blockade on survival in experimental heart failure"](/docs/dnb/2003-lee.pdf), Lee et al 2003; meta-analysis
    - ["Emergency medicine animal research: does use of randomization and blinding affect the results?"](https://onlinelibrarystatic.wiley.com/store/10.1111/j.1553-2712.2003.tb00056.x/asset/j.1553-2712.2003.tb00056.x.pdf), Bebarta et al 2003 (review/meta-analysis)
    - ["Where is the evidence that animal research benefits humans?"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC351856/), Pound et al 2004; review
    - ["The use of animal models in the study of complex disease: all else is never equal or why do so many human studies fail to replicate animal findings?"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.138.944&rep=rep1&type=pdf), Williams et al 2004; essay
    - ["First Dose of Potential New Medicines to Humans: How Animals Help"](/docs/dnb/2004-greaves.pdf), Greaves et al 2004; essay
    - ["The future of teratology research is in vitro"](http://www.aknight.info/publications/anim_expts_tox/teratol/JB%20et%20al%20Teratol%20Biog%20Amines%202005%2019%282%29%2097-146.pdf), Bailey et al 2005; review
    - ["How good are rodent models of carcinogenesis in predicting efficacy in humans? A systematic review and meta-analysis of colon chemoprevention in rats, mice and men"](http://hal.inria.fr/docs/00/33/46/99/PDF/2005-Corpet-Pierre-Rodent-models-Metanal-EJC-Author-version.pdf), Corpet & Pierre 2005; meta-analysis
    - ["Surveying the literature from animal experiments"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC557132/), Lemon & Dunnett 2005; essay
    - ["Systematic review and meta-analysis of the efficacy of FK506 in experimental stroke"](http://www.researchgate.net/publication/8028645_Systematic_review_and_metaanalysis_of_the_efficacy_of_FK506_in_experimental_stroke/file/9fcfd50f5d4695ef88.pdf), Macleod et al 2005; meta-analysis
    - ["Systematic review and meta-analysis of the efficacy of melatonin in experimental stroke"](/docs/dnb/2005-macleod.pdf), Macleod et al 2005; meta-analysis
    - ["Methodological quality of animal studies on neuroprotection in focal cerebral ischaemia"](http://www.researchgate.net/publication/7592419_Methodological_quality_of_animal_studies_on_neuroprotection_in_focal_cerebral_ischaemia/file/d912f50d152f934997.pdf), van der Worp et al 2005; review
    - ["Nitric oxide synthase inhibitors in experimental ischemic stroke and their effects on infarct size and cerebral blood flow: a systematic review"](http://eprints.nottingham.ac.uk/439/1/Willmot_NO_synthase_JFRBM.pdf), Willmot et al 2005; meta-analysis
    - ["A systematic review of nitric oxide donors and L-arginine in experimental stroke; effects on infarct size and cerebral blood flow"](http://eprints.nottingham.ac.uk/509/01/NO_paper3_10.doc), Willmot et al 2005; meta-analysis
    - ["Translation of Research Evidence From Animals to Humans"](http://web.archive.org/web/20110420082841/http://jama.ama-assn.org/content/296/14/1731.full), Hackam 2006; review
    - ["1,026 experimental treatments in acute stroke"](http://www.researchgate.net/publication/7321528_1026_experimental_treatments_in_acute_stroke/file/9fcfd50f5d469961a9.pdf), O'Collins et al 2006; review
    - ["A Systematic Review of Systematic Reviews and Meta-Analyses of Animal Experiments with Guidelines for Reporting"](/docs/dnb/2006-peters.pdf), Peters 2006; review
    - ["Translating animal research into clinical benefit"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1782020/), Hackam 2007; essay
    - ["Systematic Reviews of Animal Experiments Demonstrate Poor Human Clinical and Toxicological Utility"](http://www.aknight.info/publications/anim_expts_overall/sys_reviews/AK%20Sys%20rev%20ATLA%202007%2035\(6\)%20641-659.pdf), Knight 2007; review
    - ["Comparison of treatment effects between animal experiments and clinical trials: systematic review"](http://www.bmj.com/content/334/7586/197), Perel et al 2007; review
    - ["How can we improve the pre-clinical development of drugs for stroke?"](http://www.researchgate.net/publication/6075743_How_can_we_improve_the_pre-clinical_development_of_drugs_for_stroke/file/d912f50641e7b79078.pdf), Sena et al 2007; essay
    - ["Healthy animals and animal models of human disease(s) in safety assessment of human pharmaceuticals, including therapeutic antibodies"](/docs/dnb/2007-dixit.pdf), Dixit & Boelsterli 2007; review
    - ["Systematic Reviews of Animal Experiments Demonstrate Poor Contributions to Human Healthcare"](http://www.andrewknight.info/publications/anim_expts_overall/sys_reviews/AK%20Sys%20rev%20RRCT%202008%203(2)%2089-96.pdf), Knight 2008; essay
    - ["Are animal models as good as we think?"](http://naldc.nal.usda.gov/download/13645/PDF), Wall & Shani 2008; essay
    - ["Predictive validity of animal pain models? A comparison of the pharmacokinetic-pharmacodynamic relationship for pain drugs in rats and humans"](/docs/dnb/2008-whiteside.pdf), Whiteside et al 2008
    - ["Design, power, and interpretation of studies in the standard murine model of ALS"](http://www.researchals.org/uploaded_files/ALS%202008%209%204.pdf), Scott et al 2008; review
    - ["Evidence for the efficacy of NXY-059 in experimental focal cerebral ischaemia is confounded by study quality"](http://stroke.ahajournals.org/content/39/10/2824.full), Macleod et al 2008 (meta-analysis)
    - ["Empirical evidence of bias in the design of experimental stroke studies: a metaepidemiologic approach"](http://stroke.ahajournals.org/content/39/3/929.full), Crossley et al 2008
    - ["Publication bias in reports of animal stroke studies leads to major overstatement of efficacy"](http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1000344), Sena et al 2010; meta-analysis
    - ["Can Animal Models of Disease Reliably Inform Human Studies?"](http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.1000245), van der Worp et al 2010; essay
    - ["Improving the translational hit of experimental treatments in multiple sclerosis"](http://www.researchgate.net/publication/45505164_Improving_the_translational_hit_of_experimental_treatments_in_multiple_sclerosis/file/d912f50641e7a74813.pdf), Vesterinen et al 2010 (meta-analysis)
    - ["Human relevance of pre-clinical studies in stem cell therapy: systematic review and meta-analysis of large animal models of ischaemic heart disease"](http://cardiovascres.oxfordjournals.org/content/91/4/649.full), van der Spoel et al 2011; meta-analysis
    - ["When Mice Mislead: Tackling a long-standing disconnect between animal and human studies, some charge that animal researchers need stricter safeguards and better statistics to ensure their science is solid"](/docs/dnb/2013-couzinfrankel.pdf), Couzin-Frankel 2013; popularization
    - ["Evaluation of Excess Significance Bias in Animal Studies of Neurological Diseases"](http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001609), Tsilidis et al 2013 (meta-analysis)
    - ["Two Years Later: Journals Are Not Yet Enforcing the ARRIVE Guidelines on Reporting Standards for Pre-Clinical Animal Studies"](http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001756), Baker et al 2014 (review)

'Hot' fields tend to be new fields, which brings problems of its own, see ["Large-Scale Assessment of the Effect of Popularity on the Reliability of Research"](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0005996) & [discussion](http://scienceblogs.com/insolence/2009/07/06/popularity-versus-reliability-in-medical/ "Popularity versus reliability in medical research"). (Failure to replicate in larger studies seems to be a hallmark of biological/medical research. Ioannidis performs the same trick with [biomarkers](!Wikipedia), [finding](http://jama.ama-assn.org/content/305/21/2200.short) less than half of the most-cited biomarkers were even statistically significant in the larger studies. 12 of the more prominent [SNP](!Wikipedia "Single-nucleotide polymorphism")-IQ correlations [failed to replicate](http://europepmc.org/articles/PMC3498585 "'Most Reported Genetic Associations with General Intelligence Are Probably False Positives', Chabris et al 2011") on a larger data.) On the plus side, the parlous state of affairs means that there are some cheap heuristics for detecting unreliable papers - [simply asking for data & being refused/ignored](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0026828 "'Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results', Wicherts et al 2011") correlates strongly with the original paper having errors in its statistics.

This epidemic of false positives is apparently deliberately and knowing accepted by [epidemiology](!Wikipedia); Young's 2008 ["Everything is Dangerous"](http://nisla05.niss.org/talks/Young_Safety_June_2008.pdf) remarks that 80-90% of epidemiology's claims do not replicate (eg. the NIH ran 20 randomized-controlled-trials of claims, and only 1 replicated) and that lack of '[multiple comparisons](!Wikipedia)' (either [Bonferroni](!Wikipedia "Bonferroni correction") or [Benjamin-Hochberg](http://www.math.tau.ac.il/~ybenja/MyPapers/benjamini_hochberg1995.pdf "Controlling the False Discovery Rate: a Practical and Powerful Approach to Multiple Testing")) is taught: "[Rothman (1990)](/docs/dnb/1990-rothman.pdf "No adjustments are needed for multiple comparisons") says no correction for multiple testing is necessary and [Vandenbroucke, PLoS Med (2008)](http://www.sld.cu/galerias/pdf/sitios/revsalud/observational_research,_randomised.pdf) agrees" (see also [Perneger 1998](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1112991/) who also explicitly understands that no correction increases type 2 errors and reduces type 1 errors). Multiple correction is necessary because its absence does, in fact, result in the overstatement of medical benefit ([Godfrey 1985](/docs/dnb/1985-godfrey.pdf "Comparing the means of several groups"), [Pocock et al 1987](/docs/dnb/1987-pocock.pdf "Statistical problems in reporting of clinical trials"), [Smith 1987](/docs/dnb/1987-smith.pdf "Impact of multiple comparisons in randomized clinical trials")). The average effect size for [findings confirmed meta-analytically](http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Lehre/StatIIKrim/AP_1993_1181-1209.pdf "'The efficacy of psychological, educational, and behavioral treatment: Confirmation from meta-analysis', Lipsey & Wilson 1993") in psychology/education is _d_=0.5[^Jaeggi-priors] (well below several effect sizes from n-back/IQ studies); when moving from laboratory to non-laboratory settings, meta-analyses replicate findings correlate ~0.7 of the time, but for [social psychology](!Wikipedia) the replication correlation falls to ~0.5 with >14% of findings actually turning out to be the opposite (see [Anderson et al 1999](http://bama.ua.edu/~sprentic/607%20Anderson%20et%20al.%201999.pdf "Research in the Psychological Laboratory: Truth or Triviality?") and [Mitchell 2012](/docs/dnb/2012-mitchell.pdf "Revisiting Truth or Triviality: The External Validity of Research in the Psychological Laboratory"); for exaggeration due to non-blinding or poor randomization, [Wood et al 2008](http://www.bmj.com/content/336/7644/601 "Empirical evidence of bias in treatment effect estimates in controlled trials with different interventions and outcomes: meta-epidemiological study")). (Meta-analyses also give us a starting point for understanding how unusual medium or large effects sizes are[^Bond].) Psychology does have many challenges, but practitioners also handicap themselves; an older overview is the entertaining ["What's Wrong With Psychology, Anyway?"](http://cogprints.org/371/3/148.pdf "Lykken 1991"), which mentions the obvious point that statistics & experimental design are [flexible enough](http://people.psych.cornell.edu/~jec7/pcd%20pubs/simmonsetal11.pdf "'False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant', Simmons et al 2011") to reach significance as desired. In an interesting example of how methodological reforms are no panacea in the presence of continued perverse incentives, an earlier methodological improvement in psychology (reporting multiple experiments in a single publication as a check against results not being generalizable) has merely demonstrated the widespread _p_-value hacking or manipulation or publication bias [when one notes that](http://www.ubc-emotionlab.ca/wp-content/uploads/2012/09/Schimmack-2012-Effect-of-Significance-on-Article-Credibility.pdf "'The Ironic Effect of Significant Results on the Credibility of Multiple-Study Articles', Schimmack 2012") given the low statistical power of each experiment, even if the underlying phenomena were real it would still be wildly improbable that all _n_ experiments in a paper would turn up statistically-significant results, since power is usually extremely low in experiments (eg. [in neuroscience, ~30%](/docs/dnb/2013-button.pdf "'Power failure: why small sample size undermines the reliability of neuroscience', Button et al 2013")). These problems are pervasive enough that I believe they entirely explain any ["decline effects"](http://www.newyorker.com/reporting/2010/12/13/101213fa_fact_lehrer?currentPage=all "The Truth Wears Off: Is there something wrong with the scientific method?")^[One might be aware that the writer of that essay, [Jonah Lehrer](!Wikipedia), was fired after making up materials for one of his books, and wonder if this work can be trusted; I believe it can as the _New Yorker_ is famous for rigorous fact-checking (and no one has cast doubt on this article), Lehrer's scandals involved his books, I have not found any questionable claims in the article besides Lehrer's belief that known issues like publication bias are insufficient to explain the decline effect (which reasonable men may differ on), and [Virginia Hughes](http://www.lastwordonnothing.com/2012/11/05/jonah-lehrer-nature-of-truth/ "Jonah Lehrer, Scientists, and the Nature of Truth") ran the finished article against 7 people quoted in it like Ioannidis without any disputing facts/quotes & several somewhat praising it (see also [Andrew Gelman](http://andrewgelman.com/2010/12/13/the_truth_wears/)).].

[^Jaeggi-priors]: So for example, if we imagined that a Jaeggi effect size of 0.8 were completely borne out by a meta-analysis of many studies and turned in a point estimate of _d_=0.8; this data would imply that the strength of the n-back effect was ~1 standard deviation above the average effect (of things which get studied enough to be meta-analyzable & have published meta-analyses etc) or to put it another way, that n-back was stronger than ~84% of all reliable well-substantiated effects that psychology/education had discovered as of 1992.
[^Bond]: We can infer empirical priors from field-wide collections of effect sizes, in particular, highly reliable meta-analytic effect sizes. For example, [Lipsey & Wilson 1993](http://www2.jura.uni-hamburg.de/instkrim/kriminologie/Mitarbeiter/Enzmann/Lehre/StatIIKrim/AP_1993_1181-1209.pdf "The Efficacy of Psychological, Educational, and Behavioral Treatment: Confirmation From Meta-Analysis") which finds for various kinds of therapy a mean effect of _d_=0.5 based on >300 meta-analyses; or better yet, ["One Hundred Years of Social Psychology Quantitatively Described"](http://jenni.uchicago.edu/Spencer_Conference/Representative%20Papers/Richard%20et%20al,%202003.pdf), Bond et al 2003:

    > This article compiles results from a century of social psychological research, more than 25,000 studies of 8 million people. A large number of social psychological conclusions are listed alongside meta-analytic information about the magnitude and variability of the corresponding effects. References to 322 meta-analyses of social psychological phenomena are presented, as well as statistical effect-size summaries. Analyses reveal that social psychological effects typically yield a value of _r_ equal to .21 and that, in the typical research literature, effects vary from study to study in ways that produce a standard deviation in _r_ of .15. Uses, limitations, and implications of this large-scale compilation are noted.

    Only 5% of the [correlations](!Wikipedia "Pearson product-moment correlation coefficient") were greater than .50; only 34% yielded an _r_ of .30 or more; for example, Jaeggi 2008's 15-day group racked up an IQ increase of _d_=1.53 which converts to an _r_ of 0.61 and is 2.6 standard deviations above the overall mean, implying that the DNB effect is greater than ~99% of previous known effects in psychology! ([Schönbrodt & Perugini 2013](http://www.psy.lmu.de/allg2/download/schoenbrodt/pub/stable_correlations.pdf "At what sample size do correlations stabilize?") observe that their sampling simulation imply that, given Bond's mean effect of _r_ = .21, a psychology study would require _n_=238 for reasonable accuracy in estimating effects; most studies are far smaller.)

The failures to replicate "statistically significant" results has led [one blogger](http://lesswrong.com/lw/1gc/frequentist_statistics_are_frequently_subjective/) to caustically remark (see also ["Parapsychology: the control group for science"](http://lesswrong.com/lw/1ib/parapsychology_the_control_group_for_science/), ["Using degrees of freedom to change the past for fun and profit"](http://lesswrong.com/lw/a9f/using_degrees_of_freedom_to_change_the_past_for/), ["The Control Group is Out Of Control"](http://slatestarcodex.com/2014/04/28/the-control-group-is-out-of-control/)):

> Parapsychology, the control group for science, would seem to be a thriving field with "statistically significant" results aplenty...Parapsychologists are constantly protesting that they are playing by all the standard scientific rules, and yet their results are being ignored - that they are unfairly being held to higher standards than everyone else. I'm willing to believe that. It just means that the *standard* statistical methods of science are so weak and flawed as to permit a field of study to sustain itself in the complete absence of any subject matter. With two-thirds of medical studies in prestigious journals failing to replicate, getting rid of the entire actual subject matter would shrink the field by only 33%.

[Cosma Shalizi](http://vserver1.cscs.lsa.umich.edu/~crshalizi/weblog/698.html "The Neutral Model of Inquiry (or, What Is the Scientific Literature, Chopped Liver?; Attention conservation notice: 900 words of wondering what the scientific literature would look like if it were entirely a product of publication bias. Veils the hard-won discoveries of actual empirical scientists in vague, abstract, hyper-theoretical doubts, without alleging any concrete errors. A pile of skeptical nihilism, best refuted by going back to the lab."):

> ...Let me draw the moral [about publication bias]. Even if the community of inquiry is both too clueless to make any contact with reality and too honest to nudge borderline findings into significance, so long as they can keep coming up with new phenomena to look for, the mechanism of the file-drawer problem alone will guarantee a steady stream of new results. There is, so far as I know, no _Journal of Evidence-Based Haruspicy_ filled, issue after issue, with methodologically-faultless papers reporting the ability of sheep's livers to predict the winners of sumo championships, the outcome of speed dates, or real estate trends in selected suburbs of Chicago. But the difficulty can only be that the evidence-based haruspices aren't trying hard enough, and some friendly rivalry with the plastromancers is called for. It's true that none of these findings will last forever, but this constant overturning of old ideas by new discoveries is just part of what makes this such a dynamic time in the field of haruspicy. Many scholars will even tell you that their favorite part of being a haruspex is the frequency with which a new sacrifice over-turns everything they thought they knew about reading the future from a sheep's liver! We are very excited about the renewed interest on the part of policy-makers in the recommendations of the mantic arts...

And this is when there is enough information to replicate; open access to any data for a paper is rare (economics: [<10%](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2224146 "'Open Access to Data: An Ideal Professed but Not Practised', Versbach  & Mueller-Langer 2013")) the economics journal _Journal of Money, Credit and Banking_, which required researchers provide the data & software which could replicate their statistical analyses, discovered that <10% of the submitted materials were adequate for repeating the paper (see ["Lessons from the JMCB Archive"](http://muse.jhu.edu/login?uri=/journals/journal_of_money_credit_and_banking/v038/38.4mccullough.pdf)).
In one cute economics example, replication failed because the dataset had been [heavily edited](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889322) to make participants look better (for more economics-specific critique, see [Ioannidis & Doucouliagos 2013](/docs/dnb/2013-ioannidis.pdf "What's to know about the credibility of empirical economics?")).
Availability of data [decreases with time](http://arxiv.org/pdf/1312.5670 "'The Availability of Research Data Declines Rapidly with Article Age', Vines et al 2014"), and many studies never get published [regardless of whether publication is legally mandated](http://www.bmj.com/content/344/bmj.d7373 "'Compliance with mandatory reporting of clinical trial results on ClinicalTrials.gov: cross sectional study', Prayle et al 2012").
And as software and large datasets becomes more inherent to research, the need and the problem of it being possible to replicate [will](http://www.nature.com/nature/journal/v482/n7386/pdf/nature10836.pdf "The case for open computer programs") [get](http://www.nature.com/news/2010/101013/full/467775a.html "Computational science: Error...why scientific programming does not compute") [worse](http://james.howison.name/pubs/HowisonHerbslebSciSoftCscw2011-all-figures.pdf) because even mature commercial software libraries can [disagree majorly on their computed results to the same mathematical specification](http://web.archive.org/web/20130128115712/http://www.leshatton.org/wp-content/uploads/2012/01/Texp_ICSE297.pdf "'The T-Experiments: Errors in Scientific Software', Hatton 1997") (see also [Anda et al 2009](/docs/dnb/2009-anda.pdf "Variability and Reproducibility in Software Engineering: A Study of Four Companies that Developed the Same System")).
And spreadsheets are especially bad, with error rates in the 88% range (["What we know about spreadsheet errors"](http://panko.shidler.hawaii.edu/My%20Publications/Whatknow.htm), Panko 1998); spreadsheets are used in all areas of science, including biology and medicine (see ["Error! What biomedical computing can learn from its mistakes"](http://biomedicalcomputationreview.org/7/2/9.pdf); famous examples of coding errors include [Donohue-Levitt](!Wikipedia "The Impact of Legalized Abortion on Crime") & [Reinhart-Rogoff](http://www.nextnewdeal.net/rortybomb/researchers-finally-replicated-reinhart-rogoff-and-there-are-serious-problems)), not to mention regular [business](http://www.eusprig.org/horror-stories.htm) (eg the [London Whale](http://baselinescenario.com/2013/02/09/the-importance-of-excel/)).

Psychology is far from being perfect either; look at the examples in _The New Yorker_'s ["The Truth Wears Off"](http://www.newyorker.com/reporting/2010/12/13/101213fa_fact_lehrer?currentPage=all) article (or look at some [excerpts](http://groups.google.com/group/brain-training/browse_frm/thread/c0fe2e1f14b8af06) from that article). Computer scientist [Peter Norvig](!Wikipedia) has written a must-read essay on interpreting statistics, ["Warning Signs in Experimental Design and Interpretation"](http://norvig.com/experiment-design.html); a number of warning signs apply to many psychological studies. There may be incentive problems: a transplant researcher discovered the only way to publish in _Nature_ his inability to replicate his earlier _Nature_ paper was to [officially retract it](http://retractionwatch.wordpress.com/2013/06/19/why-i-retracted-my-nature-paper-a-guest-post-from-david-vaux-about-correcting-the-scientific-record/);  another interesting example is when, after [Daryl Bem](!Wikipedia "Daryl Bem#.22Feeling the Future.22 and the resulting controversy") got a paper published in the top journal [_JPSP_](!Wikipedia "Journal of Personality and Social Psychology") demonstrating *precognition*, the journal refused to publish any replications (failed or successful) because... "'We don't want to be the Journal of Bem Replication', he says, pointing out that other high-profile journals have similar policies of publishing only the best original research." (Quoted in [_New Scientist_](http://www.newscientist.com/article/dn20447-journal-rejects-studies-contradicting-precognition.html)) One doesn't need to be a genius to understand why psychologist Andrew D. Wilson might snarkily remark ["...think about the message _JPSP_ is sending to authors. That message is 'we will publish your crazy story if it's new, but not your sensible story if it's merely a replication'."](http://psychsciencenotes.blogspot.com/2011/05/failing-to-replicate-bems-ability-to.html) (You get what you pay for.) In one large test of the most famous psychology results, [10 of 13 (77%)](https://openscienceframework.org/project/WX7Ck/ "'Investigating variation in replicability: The `Many Labs` Replication Project', Klein et al 2013") replicated. The replication rate is under 1/3 in [one area of psychology](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3222234/ "'A Critical Review of the First 10 Years of Candidate Gene-by-Environment Interaction Research in Psychiatry', Duncan & Keller 2011") touching on genetics. This despite the obvious point that replications reduce the risk of publication bias, and increase statistical power, so that a replicated result is [more likely to be true](http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.0040028 "'Most Published Research Findings Are False - But a Little Replication Goes a Long Way', Moonesinghe et al 2007"). And the small samples of n-back studies and nootropic chemicals are especially problematic. Quoting from [Nick Bostrom](!Wikipedia) & [Anders Sandberg](!Wikipedia)'s 2006 ["Converging Cognitive Enhancements"](http://www.nickbostrom.com/papers/converging.pdf):

> The reliability of research is also an issue. Many of the cognition-enhancing interventions show small effect sizes, which may necessitate very large epidemiological studies possibly exposing large groups to unforeseen risks.

Particularly troubling is the [slowdown](http://innovationandgrowth.wordpress.com/2010/06/13/the-debt-crisis-and-the-human-genome/) in [drug discovery](http://www.nytimes.com/2011/03/07/business/07drug.html) & medical technology during the 2000s, even as genetics in particular was expected to produce earth-shaking new treatments. One biotech [venture capitalist](!Wikipedia) [writes](http://lifescivc.com/2011/03/academic-bias-biotech-failures/):

> The company spent $5M or so trying to validate a platform that didn't exist. When they tried to directly repeat the academic founder's data, it never worked. Upon re-examination of the lab notebooks, it was clear the founder's lab had at the very least massaged the data and shaped it to fit their hypothesis. Essentially, they systematically ignored every piece of negative data. Sadly this "failure to repeat" happens more often than we'd like to believe. It has happened to us at Atlas [Venture] several times in the past decade...The unspoken rule is that at least 50% of the studies published even in top tier academic journals - _Science_, _Nature_, _Cell_, _PNAS_, etc… - can't be repeated with the same conclusions by an industrial lab. In particular, key animal models often don't reproduce.  This 50% failure rate isn't a data free assertion: it's backed up by dozens of experienced R&D professionals who've participated in the (re)testing of academic findings. This is a huge problem for translational research and one that won't go away until we address it head on.

Half the respondents to [a 2012 survey](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0063221 "'A Survey on Data Reproducibility in Cancer Research Provides Insights into Our Limited Ability to Translate Findings from the Laboratory to the Clinic', Mobley et al 2013") at one cancer research center reported 1 or more incidents where they could not reproduce published research; two-thirds of those were unable to "ever able to explain or resolve their discrepant findings", half had trouble publishing results contradicting previous publications, and two-thirds failed to publish contradictory results. An internal [Bayer](!Wikipedia) [survey of 67 projects](http://www.nature.com/nrd/journal/v10/n9/full/nrd3439-c1.html "'Believe it or not: how much can we rely on published data on potential drug targets?', Prinz et al 2011") ([commentary](http://blogs.nature.com/news/2011/09/reliability_of_new_drug_target.html "Reliability of 'new drug target' claims called into question")) found that "only in ~20-25% of the projects were the relevant published data completely in line with our in-house findings", and as far as assessing the projects went:

> ...despite the low numbers, there was no apparent difference between the different research fields. Surprisingly, even publications in prestigious journals or from several independent groups did not ensure reproducibility. Indeed, our analysis revealed that the reproducibility of published data did not significantly correlate with journal impact factors, the number of publications on the respective target or the number of independent groups that authored the publications. Our findings are mirrored by 'gut feelings' expressed in personal communications with scientists from academia or other companies, as well as published observations. [apropos of above] An unspoken rule among early-stage venture capital firms that "at least 50% of published studies, even those in top-tier academic journals, can't be repeated with the same conclusions by an industrial lab" has been recently reported (see Further information) and discussed [4](/docs/dnb/2011-osherovich.pdf "'Hedging against academic risk', Osherovich 2011").

Physics has relatively small sins; ["Assessing uncertainty in physical constants"](http://hanson.gmu.edu/temp/Henrion-Fischhoff-AmJPhysics-86.pdf) (Henrion & Fischoff 1985); [Hanson's](http://www.overcomingbias.com/2006/12/academic_overco.html) summary:

> Looking at 306 estimates for particle properties, 7% were outside of a 98% confidence interval (where only 2% should be). In seven other cases, each with 14 to 40 estimates, the fraction outside the 98% confidence interval ranged from 7% to 57%, with a median of 14%.

Nor is peer review itself robust against even [low levels of collusion](http://arxiv.org/abs/1008.4324v1 "'Peer-review in a world with rational scientists: Toward selection of the average', Thurner & Hanel 2010"). Scientists who win the Nobel Prize find their *other* work suddenly being [heavily cited](http://www.nature.com/news/2011/110506/full/news.2011.270.html "Are scientific reputations boosted artificially?: Reputations emerge in a collective manner. But does this guarantee that fame rests on merit, asks Philip Ball"), suggesting either that the community either badly failed in recognizing the work's true value or that they are now sucking up & attempting to look better [by association](!Wikipedia "Halo effect"). (A mathematician once told me that often, to boost a paper's acceptance chance, they would add citations to papers by the journal's editors - a practice that will surprise none familiar with [Goodhart's law](!Wikipedia) and the use of [citations](!Wikipedia "Citation impact") in tenure & grants.)

The former editor [Richard Smith](http://jrs.sagepub.com/content/99/4/178.long "'Peer review: a flawed process at the heart of science and journals', 2006") amusingly recounts his doubts about the merits of peer review as practiced, and physicist [Michael Nielsen](!Wikipedia) [points out](http://michaelnielsen.org/blog/three-myths-about-scientific-peer-review/) that peer review is historically rare (just one of Einstein's 300 papers was peer reviewed; the famous _Nature_ did not institute peer review until 1967), has been [poorly studied](http://jama.jamanetwork.com/article.aspx?articleid=194989 "'Effects of Editorial Peer Review: A Systematic Review', Jefferson et al 2002") & not shown to be effective, is [nationally biased](http://jama.ama-assn.org/cgi/content/full/295/14/1675 "'Effect of Blinded Peer Review on Abstract Acceptance', Ross et al 2006"), erroneously rejects many historic discoveries (one study lists ["34 Nobel Laureates whose awarded work was rejected by peer review"](/docs/dnb/2008-gonzalezalvarez.pdf "Science in the 21st century: social, political, and economic issues"); [Horrobin 1990](http://www.fqxi.org/data/forum-attachments/jama.pdf "The Philosophical Basis of Peer Review and the Suppression of Innovation") lists other), and catches only a [small fraction](http://jama.jamanetwork.com/article.aspx?articleid=187748 "'Effect on the Quality of Peer Review of Blinding Reviewers and Asking Them to Sign Their Reports: A Randomized Controlled Trial', Godlee et al 1998") of errors. And questionable choices or fraud? [Forget about it](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0005738 "'How Many Scientists Fabricate and Falsify Research? A Systematic Review and Meta-Analysis of Survey Data', Fanelli 2009"):

> A pooled weighted average of 1.97% (N = 7, 95%CI: 0.86-4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once - a serious form of misconduct by any standard - and up to 33.7% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12% (N = 12, 95% CI: 9.91-19.72) for falsification, and up to 72% for other questionable research practices...When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others.

And [psychologists](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1996631 "'Measuring the Prevalence of Questionable Research Practices with Incentives for Truth-Telling', John et al 2012"):

> We surveyed over 2,000 psychologists about their involvement in questionable research practices, using an anonymous elicitation format supplemented by incentives for honest reporting. The impact of incentives on admission rates was positive, and greater for practices that respondents judge to be less defensible. Using three different estimation methods, we find that the proportion of respondents that have engaged in these practices is surprisingly high relative to respondents' own estimates of these proportions. Some questionable practices may constitute the prevailing research norm.

In short, the secret sauce of science is *not* 'peer review'. It is replication!

None of these systematic problems should be considered minor or methodological quibbling or foolish idealism: they are systematic biases and as such, they force an upper bound on how accurate a corpus of studies can be even if there were thousands upon thousands of studies, because the total error in the results is made up of [random error](!Wikipedia) and [systematic error](!Wikipedia), but while random error shrinks as more studies are done, systematic error remains the same. A thousand biased studies merely result in an extremely precise estimate of the wrong number. This is a point appreciated by statisticians and experimental physicists, but it doesn't seem to be frequently discussed. Andrew Gelman has a [fun demonstration of selection bias involving candy](http://andrewgelman.com/2008/05/08/doing_the_candy/), or from pg812-1020 of [Chapter 8 "Sufficiency, Ancillarity, And All That"](http://omega.albany.edu:8008/ETJ-PS/cc8k.ps) of [_Probability Theory: The Logic of Science_](http://omega.albany.edu:8008/JaynesBook.html) by [E.T. Jaynes](!Wikipedia):

> The classical example showing the error of this kind of reasoning is the fable about the height of the Emperor of China. Supposing that each person in China surely knows the height of the Emperor to an accuracy of at least ±1 meter, if there are _N_=1,000,000,000 inhabitants, then it seems that we could determine his height to an accuracy at least as good as
>
> $\frac{1}{\sqrt{1,000,000,000}}m = 0.003cm$ (8-49)
>
> merely by asking each person's opinion and averaging the results.
>
> The absurdity of the conclusion tells us rather forcefully that the $\sqrt{N}$ rule is not always valid, even when the separate data values are causally independent; it requires them to be *logically* independent. In this case, we know that the vast majority of the inhabitants of China have never seen the Emperor; yet they have been discussing the Emperor among themselves and some kind of mental image of him has evolved as folklore. Then knowledge of the answer given by one does tell us something about the answer likely to be given by another, so they are not logically independent. Indeed, folklore has almost surely generated a systematic error, which survives the averaging; thus the above estimate would tell us something about the folklore, but almost nothing about the Emperor.
>
> We could put it roughly as follows:
>
> error in estimate = $S \pm \frac{R}{\sqrt{N}}$ (8-50)
>
> where _S_ is the common systematic error in each datum, _R_ is the [RMS](!Wikipedia "Root-mean-square deviation") 'random' error in the individual data values. Uninformed opinions, even though they may agree well among themselves, are nearly worthless as evidence. Therefore sound scientific inference demands that, when this is a possibility, we use a form of probability theory (i.e. a probabilistic model) which is sophisticated enough to detect this situation and make allowances for it.
>
> As a start on this, equation (8-50) gives us a crude but useful rule of thumb; it shows that, unless we *know* that the systematic error is less than about $\frac{1}{3}$ of the random error, we cannot be sure that the average of a million data values is any more accurate or reliable than the average of ten[^one-third]. As [Henri Poincare](!Wikipedia) put it: "The physicist is persuaded that one good measurement is worth many bad ones." This has been well recognized by experimental physicists for generations; but warnings about it are conspicuously missing in the "soft" sciences whose practitioners are educated from those textbooks.

[^one-third]: If I am understanding this right, Jaynes's point here is that the random error shrinks towards zero as _N_ increases, but this error is added onto the "common systematic error" _S_, so the total error approaches _S_ no matter how many observations you make and this can force the total error up as well as down (variability, in this case, actually being helpful for once). So for example, $\frac{1}{3} + \frac{1}{\sqrt{10}} = 0.66$; with _N_=100, it's 0.43; with _N_=1,000,000 it's 0.334; and with _N_=1,000,000 it equals 0.333365 etc, and never going below the original systematic error of $\frac{1}{3}$ - that is, after 10 observations, the portion of error due to sampling error is less than that due to the systematic error, so one has hit severely diminishing returns in the value of any additional (biased) data, and to meaningfully improve the estimate one must obtain unbiased data. This leads to the unfortunate consequence that the likely error of _N_=10 is 0.017<_x_<0.64956 while for _N_=1,000,000 it is the similar range 0.017<_x_<0.33433 - so it is possible that the estimate could be exactly as good (or bad) for the tiny sample as compared with the enormous sample, since neither can do better than 0.017!

Or pg1019-1020 [Chapter 10 "Physics of 'Random Experiments'"](http://omega.albany.edu:8008/ETJ-PS/cc10i.ps):

> ...Nevertheless, the existence of such a strong connection is clearly only an ideal limiting case unlikely to be realized in any real application. For this reason, the [law of large numbers](!Wikipedia) and limit theorems of probability theory can be grossly misleading to a scientist or engineer who naively supposes them to be experimental facts, and tries to interpret them literally in his problems. Here are two simple examples:
>
> 1. Suppose there is some random experiment in which you assign a probability _p_ for some particular outcome _A_. It is important to estimate accurately the fraction _f_ of times _A_ will be true in the next million trials. If you try to use the laws of large numbers, it will tell you various things about _f_; for example, that it is quite likely to differ from _p_ by less than a tenth of one percent, and enormously unlikely to differ from _p_ by more than one percent. But now, imagine that in the first hundred trials, the observed frequency of _A_ turned out to be entirely different from _p_. Would this lead you to suspect that something was wrong, and revise your probability assignment for the 101'st trial? If it would, then your state of knowledge is different from that required for the validity of the law of large numbers. You are not sure of the independence of  different trials, and/or you are not sure of the correctness of the numerical value of _p_. Your prediction of _f_ for a million trials is probably no more reliable than for a hundred.
> 2. The common sense of a good experimental scientist tells him the same thing without any probability theory. Suppose someone is measuring the velocity of light. After making allowances for the known systematic errors, he could calculate a probability distribution for the various other errors, based on the noise level in his electronics, vibration amplitudes, etc. At this point, a naive application of the law of large numbers might lead him to think that he can add three significant figures to his measurement merely by repeating it a million times and averaging the results. But, of course, what he would actually do is to repeat some unknown systematic error a million times. It is idle to repeat a physical measurement an enormous number of times in the hope that "good statistics" will average out your errors, because we cannot know the full systematic error. This is the old "Emperor of China" fallacy...
>
> Indeed, unless we know that all sources of systematic error - recognized or unrecognized - contribute less than about one-third the total error, we cannot be sure that the average of a million measurements is any more reliable than the average of ten. Our time is much better spent in designing a new experiment which will give a lower probable error per trial. As Poincare put it, "The physicist is persuaded that one good measurement is worth many bad ones."^[Possibly this is what Lord Rutherford meant when he said, "If your experiment needs statistics you ought to have done a better experiment".] In other words, the common sense of a scientist tells him that the probabilities he assigns to various errors do not have a strong connection with frequencies, and that methods of inference which presuppose such a connection could be disastrously misleading in his problems.

What's particularly sad is when people read something like this and decide to rely on anecdotes, personal experiments, and alternative medicine where there are even more systematic errors and no way of reducing random error at all! Science may be [the lens that sees its own flaws](http://lesswrong.com/lw/jm/the_lens_that_sees_its_flaws/), but if other epistemologies do not boast such long detailed self-critiques, it's not because they are flawless...  It's like that old [Jamie Zawinski](!Wikipedia) quote: Some people, when faced with the problem of mainstream medicine & epidemiology having serious methodological weaknesses, say "I know, I'll turn to non-mainstream medicine & epidemiology. After all, if only some medicine is based on real scientific method and outperforms placebos, why bother?" (Now they have *two* problems.) Or perhaps [Isaac Asimov](http://chem.tufts.edu/answersinscience/relativityofwrong.htm "The Relativity of Wrong"): "John, when people thought the earth was flat, they were wrong. When people thought the earth was spherical, they were wrong. But if you think that thinking the earth is spherical is just as wrong as thinking the earth is flat, then your view is wronger than both of them put together."

<!--
"Randomized Controlled Trials Commissioned by the Institute of Education Sciences Since 2002: How Many Found Positive Versus Weak or No Effects", Coalition for Evidence-Based Policy July 2013 http://coalition4evidence.org/wp-content/uploads/2013/06/IES-Commissioned-RCTs-positive-vs-weak-or-null-findings-7-2013.pdf

Since the establishment of the Institute for Education Sciences (IES) within the U.S. Department of Education in 2002, IES has commissioned a sizable number of well-conducted randomized controlled trials (RCTs) evaluating the effectiveness of diverse educational programs, practices, and strategies ("interventions"). These interventions have included, for example, various educational curricula, teacher professional development programs, school choice programs, educational software, and data-driven school reform initiatives.
...A clear pattern of findings in these IES studies is that the large majority of interventions evaluated produced weak or no positive effects compared to usual school practices. This pattern is consistent with findings in other fields where RCTs are frequently carried out, such as medicine and business1
...A total of 90 interventions have been evaluated in IES-commissioned RCTs. Of these:

- 11 interventions (12%) were found to produce positive effects;
- 79 interventions (88%) were found to produce weak or no positive effects.

Focusing on the subset of 77 interventions evaluated in RCTs that our review identified as having no major study limitations (e.g., differential attrition, inadequate statistical power)2:

- 7 interventions (9%) were found to produce positive effects;
- 70 interventions (91%) were found to produce weak or no positive effects.

1. In medicine: reviews have found that 50-80% of positive results in initial ("phase II") clinical studies are overturned in subsequent, more definitive RCTs ("phase III"). John P. A. Ioannidis, ["Contradicted and Initially Stronger Effects in Highly Cited Clinical Research"](http://jama.jamanetwork.com/article.aspx?articleid=201218), Journal of the American Medical Association, vol. 294, no.  July 13, 2005, pp. 218-228. Mohammad I. Zia, Lillian L. Siu, Greg R. Pond, and Eric X. Chen, "Comparison of Outcomes of Phase II Studies and Subsequent Randomized Control Studies Using Identical Chemotherapeutic Regimens," Journal of Clinical Oncology, vol. 23, no. 28, October 1, 2005, pp. 6982-6991. John K. Chan et. al., "Analysis of Phase II Studies on Targeted Agents and Subsequent Phase III Trials: What Are the Predictors for Success," Journal of Clinical Oncology, vol. 26, no. 9, March 20, 2008.
-->
<!-- "Why do Phase III Trials of Promising Heart Failure Drugs Often Fail? The Contribution of 'Regression to the Truth'", Krum & Tonkin 2003 /docs/dnb/2003-krum.pdf

There has been considerable recent disappointment with the failure of a number of major new pharmacological strategies for the treatment of chronic heart failure. In turn, there has been much speculation as to why trials of these therapies have not shown benefit. Among a number of plausible and scientifically valid reasons, consideration should be afforded to the potential contribution of "regression to the truth." Regression to the truth derives from the biological concept of regression to the mean, whereby random fluctuations in a biological variable occur over time, such that the true value of the variable is approached with repeated measurements. This same concept can be applied to clinical trial programs for new drugs for heart failure. Because only strongly positive trials generally go on to phase III testing, and some of these early phase studies are positive by chance alone, on retesting in phase III the results are very likely not be as strongly positive. Numerous examples of regression to the truth apply for trials of heart failure therapies, as well as in other areas.

To understand regression to the truth we must first consider the concept of regression to the mean. This concept derives from the random fluctuations that can occur in a variable over time. As a consequence, a single measurement of that variable more often yields a value removed from the mean, and the "true" value of the variable is approached with repeated measurements. As a corollary, in population studies, a single measurement of the dependent variable - for example, cholesterol - can lead to an underestimate of the strength of its association with an outcome such as coronary heart disease death ("regression distribution bias"). Consider a theoretical drug (drug x) being studied to determine its benefit in heart failure, as assessed by a surrogate measure, lowering of plasma norepinephrine (Fig. 1). The left panel shows that there is really no difference in plasma norepinephrine levels before and after drug x. However, the investigators went on to perform a subgroup analysis of those patients with norepinephrine levels above the mean (middle panel), and that subgroup demonstrated a significant reduction in norepinephrine levels with drug x. The investigators might therefore claim that drug x is effective in lowering plasma norepinephrine in patients with high norepinephrine levels. Furthermore, it is these patients (ie, patients with high levels) who are those that are particularly in need of a drug that will lower such elevated levels. Although it is possible that drug x does indeed lower elevated plasma norepinephrine levels, it is equally plausible (if not more so) that the high plasma levels were "captured" as being falsely or atypically high (for the individual patient) at baseline and then when the same patients were remeasured at a later time point, levels were not as high (ie, classic regression to the mean).
This concept is well-understood for a biologic variable, but how can this concept be applied to that of a clinical trial program for a new drug for heart failure? This is conceptually illustrated in Fig. 2, which depicts early phase trials conducted in the assessment of a variety of potential new drug therapies for heart failure. Each dot represents a trial of a certain drug. As can be seen, some early phase studies will be strongly negative, some strongly positive, but most will cluster around neutrality and, therefore, one can construct a standard bell-shaped curve. We know that many trials of new chemical entities are conducted in the setting of heart failure. Because of the large number of studies conducted, some will be positive by chance and indeed some will be strongly positive by chance. Does this matter? Yes, it does. It is highly likely that only drugs associated with strongly positive trials (ie, those to the right of the vertical dotted line) will go on to phase III testing. Because some of these studies that are positive by chance alone will be among these, then when retested in phase III trials, the results will no longer be strongly positive. This is analogous to the high plasma cholesterol or norepinephrine being retested in the earlier examples. This concept is true, not just of heart failure trials, but of any drug therapy for any specific indication. What exacerbates the problem in the setting of chronic heart failure is the low percentage strike rate in the development of successful pharmacologic therapies for this condition. Only renin-angiotensin and β-adrenoceptor blocking agents have come to the market over the last 30 years or so.
Therefore, very few promising drugs in early phase would be positive in phase III (if tested) and thus registrable for a heart failure indication. This is illustrated by the open circles below the curved line, interposed on the totality of early phase trials in Fig. 2. This line is curved because, of course, a strongly positive early phase study will make it more likely (but possibly still with low probability) of positive findings in phase III studies. Nevertheless, this still leaves a large number of trials strongly positive in early phase by chance alone (circled cluster) "regressing to the truth."
-->
<!-- "Trial unpredictability yields predictable therapy gains" Djulbegovic et al 2013 /docs/dnb/2013-djulbegovic.pdf
Here we provide empirical evidence that the system's success rate is optimal. We analysed hundreds of trials, published and unpublished, public and industry funded, involving hundreds of thousands of people over several decades. We find that just over half the time, RCTs show that new treatments are better than existing ones.
...We conducted an analysis of 860 published and unpublished phase III RCTs performed by academics or pharmaceutical companies in six consecutive series of trials with a total of more than 350,000 patients: four series of 743 publicly sponsored trials over the past 50 years6, and two series of 117 publicly and commercially sponsored clinical trials over the past 30 years7 (see 'The best medicine'). Our results show that the probability of finding that a new treatment is better than a standard treatment is about 50-60%, confirming the theoretical predictions we made more than 15 years ago4,5.
We found that in publicly sponsored RCTs, the likelihood that new treatments would work better than existing ones ranges from 57% to 63% for patient survival and from 55% to 66% for all primary outcomes (such as survival without recurrence of disease, response to treatment, symptom frequency and measures of disability). The only available comparable rates for industry-sponsored RCTs show that, overall, new treatments are superior to existing treatments for measures of morbidity (nausea, for example) in 75% of trials, but similar (53%) for survival7. Over time, the pattern in all trials has converged at around 50% (probably because earlier studies used inferior comparators) and applies across various clinical fields and types of treatment6,7.
-->
<!--
"New Drug Development: Estimating entry from human clinical trials", Adams & Brantner 2003 http://www.ftc.gov/be/workpapers/wp262.pdf :

This paper analyses a detailed data set on drugs in human clinical trials around the world between 1989 and 2002. The data provides information on the probabilities with which drugs successfully complete the different phases of the trials and the durations of successful completions. The paper shows that success rates and durations can vary substantially across observable characteristics of the drugs, including primary indication, originating company, route of administration and chemistry. It suggests that analysis of this type of data can help us to answer questions such as: Do AIDS drugs get to market faster? Do Biotech drugs have higher probabilities of getting to market? This paper provides some general statistics for analyzing these questions...Our basic summary is that approximately 1 in 8 drugs that entered Phase I are launched on the US market.

Note that according to the FDA, only 1 in 1,000 drugs pass the preclinical stage and are proposed for testing in humans (FDA, 2002). However, almost half the R&D expenditures occur in the preclinical stage of development (Levy, 1999)

Pharmaprojects contains information on 27,987 new branded drug entities that have reached the late stage development from 1980 to 2002. For the purposes of this study, we limited the sample size to the 3,328 drugs that have entered either Phase I, or Phase II, or Phase III of the human clinical trials somewhere in the world for the first time since 1989.

On average, it takes just under 8 years for a drug to go from Phase I of human clinical trials to market launch in the US. The same figures for Phase II and Phase III drugs are 6.1 and 3.7 years respectively. More specifically, an average drug spends 1.7 years in Phase I, 2.4 years in Phase II, and 3.7 years in Phase III before launch.
Graph 1 presents a graph showing the estimated duration for the drugs in the data set by their primary indication. While it takes just 5.5 years on average for HIV/AIDS drugs to get from Phase I to the market, it takes drugs for Parkinson's disease almost twice that long to go through the same process. Drugs for arthritis also spend more than 9 years, and asthma drugs spend more than 8 years in clinical trials on average. HIV/AIDS, anti-hypertension, and leukemia cancer drugs are some drugs that spend less than 7 years in clinical development. Again, this result is suggestive, but more sophisticated analysis is necessary to determine whether more important drugs get to market faster, and why.
-->
